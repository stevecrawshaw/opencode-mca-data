DataFrame.to_numpy(order='fortran', writable=False, allow_copy=True, structured=False, use_pyarrow=None) -> np.ndarray: Converts DataFrame to NumPy ndarray. order: 'C' or 'fortran'. writable: force data copy to make array writable. allow_copy: fail if zero-copy not possible. structured: return structured array. df = pl.DataFrame({"a": [1, 2, 3]}); df.to_numpy() DataFrame.to_torch(return_type='tensor', label=None, features=None, dtype=None) -> torch.Tensor | dict[str, torch.Tensor] | PolarsDataset: Convert DataFrame to PyTorch Tensor, Dataset, or dict of Tensors. return_type: 'tensor', 'dataset', 'dict'. label: column(s) for labels. features: column(s) for features. dtype: unify dtype. df = pl.DataFrame({"lbl": [0, 1, 2, 3],"feat1": [1, 0, 0, 1],"feat2": [1.5, -0.5, 0.0, -2.25]}); df.to_torch(dtype=pl.Float32) polars.read_database(query: str | TextClause | Selectable, connection: ConnectionOrCursor | str, *, iter_batches: bool = False, batch_size: int | None = None, schema_overrides: SchemaDict | None = None, infer_schema_length: int | None = 100, execute_options: dict[str, Any] | None = None) -> DataFrame | Iterator[DataFrame]. Read SQL query into DataFrame. Params: query, connection, iter_batches, batch_size, schema_overrides, infer_schema_length, execute_options. LazyFrame.join_asof(other: LazyFrame, *, left_on: str | None | Expr = None, right_on: str | None | Expr = None, on: str | None | Expr = None, by_left: str | Sequence[str] | None = None, by_right: str | Sequence[str] | None = None, by: str | Sequence[str] | None = None, strategy: AsofJoinStrategy = 'backward', suffix: str = '_right', tolerance: str | int | float | timedelta | None = None, allow_parallel: bool = True, force_parallel: bool = False, coalesce: bool = True, allow_exact_matches: bool = True, check_sortedness: bool = True) -> LazyFrame: Performs an asof join (nearest key). Parameters: other - LazyFrame to join, left_on/right_on/on - Join columns, by - Join on these columns first, strategy - {'backward', 'forward', 'nearest'}, suffix - Column name suffix, tolerance - Numeric tolerance, allow_parallel/force_parallel - Parallel execution, coalesce - Coalesce join columns, allow_exact_matches - Allow matching with the same on value, check_sortedness - Check sortedness of asof keys. LazyFrame.rolling(index_column: IntoExpr, *, period: str | timedelta, offset: str | timedelta | None = None, closed: ClosedInterval = 'right', group_by: IntoExpr | Iterable[IntoExpr] | None = None) -> LazyGroupBy. Create rolling groups based on a temporal or integer column. index_column: Column to group by (Date/Datetime, sorted ascending). period: Length of window. offset: Offset of window (default -period). closed: {'right','left','both','none'}. group_by: Also group by this column/these columns. Returns: LazyGroupBy, sorted by index_column (within group_by groups). Series.bitwise_xor() -> PythonLiteral | None: Aggregate bitwise XORs. Series.gt(other: Any) -> Series | Expr: Method equivalent of series > other operator. Series.le(other: Any) -> Series | Expr: Series less than or equal to other. Series.last() -> PythonLiteral | None: Get the last element of the Series. Returns None if Series is empty. Series.lt(other: Any) -> Series | Expr: Method equivalent of series < other. DataFrame.write_delta(target: str | Path | deltalake.DeltaTable, mode: Literal['error', 'append', 'overwrite', 'ignore', 'merge'] = 'error', overwrite_schema: bool | None = None, storage_options: dict[str, str] | None = None, credential_provider: CredentialProviderFunction | Literal['auto'] | None = 'auto', delta_write_options: dict[str, Any] | None = None, delta_merge_options: dict[str, Any] | None = None) -> deltalake.table.TableMerger | None: Writes DataFrame as a delta table. mode: Error, append, overwrite, ignore, or merge existing data. Raises TypeError for unsupported data types, ArrowInvalidError for uncastable types, TableNotFoundError if merge on non-existent table. Expr.rolling_quantile_by(by: IntoExpr, window_size: timedelta | str, *, quantile: float, interpolation: RollingInterpolationMethod = 'nearest', min_samples: int = 1, closed: ClosedInterval = 'right') -> Expr Compute rolling quantile based on another column. Parameters: by (DateTime, Date, UInt64, UInt32, Int64, Int32), window_size (timedelta|str), quantile (float 0.0-1.0), interpolation (str), min_samples (int), closed (str 'left', 'right', 'both', 'none'). Unstable. Expr.rolling_std_by(by: IntoExpr, window_size: timedelta | str, min_samples: int = 1, closed: ClosedInterval = 'right', ddof: int = 1) -> Expr: Computes a rolling standard deviation based on another column. Warning: Unstable functionality. polars.read_excel(source: FileSource, *, sheet_id: int | Sequence[int] | None = None, sheet_name: str | list[str] | tuple[str] | None = None, table_name: str | None = None, engine: ExcelSpreadsheetEngine = 'calamine', engine_options: dict[str, Any] | None = None, read_options: dict[str, Any] | None = None, has_header: bool = True, columns: Sequence[int] | Sequence[str] | None = None, schema_overrides: SchemaDict | None = None, infer_schema_length: int | None = 100, include_file_paths: str | None = None, drop_empty_rows: bool = True, drop_empty_cols: bool = True, raise_if_empty: bool = True) -> DataFrame | dict[str, DataFrame]: Read Excel data into a DataFrame. source: file path. sheet_id, sheet_name, table_name: specify sheet/table. engine: 'calamine', 'openpyxl', 'xlsx2csv'. schema_overrides: override column types. Returns DataFrame or dict of DataFrames. DataFrame.is_empty()->bool:Checks if DataFrame is empty. Series.ne(other: Any) -> Series | Expr: Method equivalent of series != other. Expr.bitwise_count_zeros() -> Expr: Count unset bits. polars.scan_ndjson(source: str | Path | IO[str] | IO[bytes] | bytes | list[str] | list[Path] | list[IO[str]] | list[IO[bytes]], *, schema: SchemaDefinition | None = None, schema_overrides: SchemaDefinition | None = None, infer_schema_length: int | None = 100, batch_size: int | None = 1024, n_rows: int | None = None, low_memory: bool = False, rechunk: bool = False, row_index_name: str | None = None, row_index_offset: int = 0, ignore_errors: bool = False, storage_options: dict[str, Any] | None = None, credential_provider: CredentialProviderFunction | Literal['auto'] | None = 'auto', retries: int = 2, file_cache_ttl: int | None = None, include_file_paths: str | None = None) -> LazyFrame: Lazily read newline delimited JSON files. Parameters: source (path to file), schema (dict of {name:type}), schema_overrides (dict, override dtypes), infer_schema_length (rows to scan for schema), batch_size (rows per batch), n_rows (stop after reading), low_memory, rechunk, row_index_name, row_index_offset, ignore_errors (return Null on schema mismatch), storage_options (cloud provider options), credential_provider (callable for cloud credentials), retries (cloud retries), file_cache_ttl (cloud file cache ttl), include_file_paths (filename column). Allows query optimizer pushdown. Series.mean() -> PythonLiteral | None: Reduce Series to the mean value. Example: s = pl.Series("a", [1, 2, 3]); s.mean() polars.testing.parametric.dataframes(cols: int | column | Sequence[column] | None = None, lazy: bool = False, min_cols: int = 1, max_cols: int = 5, min_size: int = 0, max_size: int = 5, include_cols: Sequence[column] | column | None = None, allow_null: bool | Mapping[str, bool] = True, allow_chunks: bool = True, allow_masked_out: bool = True, allowed_dtypes: Collection[PolarsDataType] | PolarsDataType | None = None, excluded_dtypes: Collection[PolarsDataType] | PolarsDataType | None = None, allow_time_zones: bool = True, **kwargs: Any) -> SearchStrategy[DataFrame | LazyFrame]: Hypothesis strategy for generating Polars DataFrames or LazyFrames. UNSTABLE. Series.median() -> PythonLiteral | None. Get median of Series. Series.min() -> PythonLiteral | None: Get the minimal value in this Series. Series.arr.first() -> Series: Get first value of sub-arrays. DataFrame.shrink_to_fit(*, in_place: bool = False) -> DataFrame: Shrink DataFrame memory usage to fit data. Series.shape: tuple[int]. Returns shape of Series. Series.std(ddof: int = 1) -> float | timedelta | None: Get the standard deviation. Parameters: ddof (int) - Delta Degrees of Freedom (N - ddof). Returns: float, timedelta or None. Example: s.std() DataFrame.flags: dict[str, dict[str, bool]]. Get flags set on DataFrame columns. Returns dict: column name -> column flags. Series.bitwise_leading_ones() -> Self: Evaluate number of most-significant set bits before unset bit. DataFrame.width: int. Get the number of columns. Returns: int. Example: df = pl.DataFrame({"foo": [1, 2, 3], "bar": [4, 5, 6]}); df.width # 2 LazyFrame.set_sorted(column: str, descending: bool = False) -> LazyFrame Mark columns as sorted. Use with caution; incorrect if data is not sorted. LazyFrame.cache(): Cache result. Optimizer likely better. Returns LazyFrame. LazyFrame.collect(*, type_coercion: bool = True, predicate_pushdown: bool = True, projection_pushdown: bool = True, simplify_expression: bool = True, slice_pushdown: bool = True, comm_subplan_elim: bool = True, comm_subexpr_elim: bool = True, cluster_with_columns: bool = True, collapse_joins: bool = True, no_optimization: bool = False, streaming: bool = False, engine: EngineType = 'cpu', background: bool = False) -> DataFrame | InProcessQuery. Materialize LazyFrame to DataFrame. Query optimizations enabled by default. engine: 'cpu' or 'gpu'. streaming/background: unstable. Series.approx_n_unique() -> PythonLiteral | None: Approximate count of unique values using HyperLogLog++ algorithm. StringCache: Context manager for enabling/disabling global string cache. __init__(*args, **kwargs): Creates StringCache object. Usage: with pl.StringCache(): ... class polars.datatypes.Int64: 64-bit signed integer type. Methods: base_type() -> DataType: Returns fundamental type class. from_python(py_type) -> DataType: Returns Polars data type. is_(other) -> bool: Checks if same DataType. is_decimal() -> bool: Checks decimal type. is_float() -> bool: Checks float type. is_integer() -> bool: Checks integer type. is_nested() -> bool: Checks nested type. is_numeric() -> bool: Checks numeric type. is_signed_integer() -> bool: Checks signed integer type. is_temporal() -> bool: Checks temporal type. is_unsigned_integer() -> bool: Checks unsigned integer type. max() -> Expr: Max value. min() -> Expr: Min value. to_python() -> type: Returns Python type. class Binary: Binary type. Methods: base_type(), from_python(py_type), is_(other), is_decimal(), is_float(), is_integer(), is_nested(), is_numeric(), is_signed_integer(), is_temporal(), is_unsigned_integer(), to_python(). DataFrame.count() Return number of non-null elements per column. Returns DataFrame. DataFrame.max() -> DataFrame: Aggregate the columns of this DataFrame to their maximum value. DataFrame.mean() -> DataFrame Aggregate the columns of this DataFrame to their mean value. DataFrame.mean_horizontal(ignore_nulls: bool = True) -> Series: Take mean of all values horizontally. ignore_nulls: if False, any null input yields null output. Returns Series named "mean". Example: df.mean_horizontal() DataFrame.__arrow_c_stream__(requested_schema: object | None = None) -> object: Export DataFrame via Arrow PyCapsule Interface. DataFrame.std(ddof: int = 1) -> DataFrame: Aggregate columns to standard deviation. Params: ddof. Returns: DataFrame. DataFrame.sum() -> DataFrame. Aggregate DataFrame columns to sum value. Returns: DataFrame. DataFrame.sum_horizontal(*, ignore_nulls: bool = True) -> Series: Sums values horizontally across columns. ignore_nulls: Ignore nulls (default). Returns Series named "sum". DataFrame.describe(percentiles: Sequence[float] | float | None = (0.25, 0.5, 0.75), *, interpolation: RollingInterpolationMethod = 'nearest') -> DataFrame: Returns summary statistics for a DataFrame. percentiles: Percentiles to include. interpolation: Interpolation method. Warning: Output is not guaranteed to be stable. DataFrame.columns: list[str]. Get or set column names. DataFrame.fold(operation: Callable[[Series, Series], Series]) -> Series. Apply a horizontal reduction on a DataFrame. DataFrame.hash_rows(seed: int = 0, seed_1: int | None = None, seed_2: int | None = None, seed_3: int | None = None) -> Series: Hash and combine rows. Returns UInt64 Series. Seed params for random seed. Not stable across Polars versions. DataFrame.estimated_size(unit: SizeUnit = 'b') -> int | float. Returns estimated total allocated size of DataFrame in given unit (bytes default). DataFrame.is_duplicated() Get mask of duplicated rows. Returns Series[bool]. DataFrame.n_chunks(strategy:'first'|'all'='first') -> int|list[int]. Get number of chunks in ChunkedArrays. Parameters: strategy ('first', 'all'). Returns: int or list[int]. DataFrame.null_count() -> DataFrame: Show null counts per column. Returns: DataFrame. polars.DataFrame.to_arrow(*, compat_level: CompatLevel | None = None) -> Table: Converts DataFrame to Arrow Table. Categorical types are copied. DataFrame.to_dict(as_series=True) -> dict: Convert DataFrame to dictionary. as_series: True -> Series, False -> list. polars.DataFrame.to_dicts() -> list[dict[str, Any]] Convert every row to a dictionary. Note: ns-precision temporal values are truncated to microseconds. DataFrame.to_pandas(use_pyarrow_extension_array: bool=False, **kwargs) -> pandas.DataFrame: Convert to pandas DataFrame. Requires pandas and pyarrow. use_pyarrow_extension_array for zero-copy. DataFrame.to_struct(name: str = '') -> Series Convert a DataFrame to a Series of type Struct. GroupBy.all() -> DataFrame: Aggregate the groups into Series. Example: df = pl.DataFrame({"a": ["one", "two", "one", "two"], "b": [1, 2, 3, 4]}); df.group_by("a", maintain_order=True).all() # returns lists [1,3] and [2,4] for groups "one" and "two" GroupBy.count() -> DataFrame: Returns the number of rows in each group. Deprecated: Use len() GroupBy.head(n: int = 5) -> DataFrame: Get first n rows of each group. GroupBy.min() -> DataFrame: Reduce groups to minimum value. DataFrame.group_by_dynamic(index_column: IntoExpr, *, every: str | timedelta, period: str | timedelta | None = None, offset: str | timedelta | None = None, include_boundaries: bool = False, closed: ClosedInterval = 'left', label: Label = 'left', group_by: IntoExpr | Iterable[IntoExpr] | None = None, start_by: StartBy = 'window') -> DynamicGroupBy: Group by time windows. index_column: Column to group by. every: Window interval. period: Window length. offset: Window offset. include_boundaries: Include boundaries. closed: Closed interval type. label: Window label. group_by: Additional grouping columns. start_by: Start window strategy. Warning: index_column must be sorted. Series.arr.min() -> Series Compute min values of sub-arrays. GroupBy.sum() -> DataFrame: Reduce the groups to the sum. DataFrame.join(other: DataFrame, on: str | Expr | Sequence[str | Expr] | None = None, how: JoinStrategy = 'inner', *, left_on: str | Expr | Sequence[str | Expr] | None = None, right_on: str | Expr | Sequence[str | Expr] | None = None, suffix: str = '_right', validate: JoinValidation = 'm:m', join_nulls: bool = False, coalesce: bool | None = None, maintain_order: MaintainOrderJoin | None = None) -> DataFrame: SQL-like join. `other`: DataFrame to join. `on`: join column(s). `how`: 'inner', 'left', 'right', 'full', 'semi', 'anti', 'cross'. `left_on`, `right_on`: alternative join columns. `suffix`: for duplicate names. `validate`: 'm:m', 'm:1', '1:m', '1:1' (checks uniqueness). `join_nulls`: join on nulls. `coalesce`: controls column merging (None, True, False). `maintain_order` {'none', 'left', 'right', 'left_right', 'right_left'} sets result order, defaults to none for speed. Example: df.join(other_df, on="ham", how="left") DataFrame.to_series(index: int = 0) -> Series: Select column as Series at index location. index: Location of selection. Example: df.to_series(1) DataFrame.clear(n: int) -> DataFrame Create empty (n=0) or n-row null-filled (n>0) DataFrame copy. DataFrame.drop(*columns: ColumnNameOrSelector, strict: bool = True) Returns DataFrame. Remove columns. Parameters: *columns: column names/selectors, strict: validate column existence. Example: df.drop("col1"). DataFrame.drop_in_place(name: str) -> Series: Drops a single column in-place and returns it. Parameters: name: Column name to drop. Returns: The dropped column. DataFrame.drop_nans(subset=None) Drop rows containing NaN values. subset: Column names to consider for NaNs; if None, use all columns with float type. Returns DataFrame. DataFrame.drop_nulls(subset: ColumnNameOrSelector | Collection[ColumnNameOrSelector] | None = None) -> DataFrame. Drop rows with null values. Subset columns optional. DataFrame.join_asof(other: DataFrame, *, left_on: str | None | Expr = None, right_on: str | None | Expr = None, on: str | None | Expr = None, by_left: str | Sequence[str] | None = None, by_right: str | Sequence[str] | None = None, by: str | Sequence[str] | None = None, strategy: AsofJoinStrategy = 'backward', suffix: str = '_right', tolerance: str | int | float | timedelta | None = None, allow_parallel: bool = True, force_parallel: bool = False, coalesce: bool = True, allow_exact_matches: bool = True, check_sortedness: bool=True) -> DataFrame: Perform an asof join, similar to a left-join but matches on nearest key. DataFrames must be sorted by the 'on' key. Strategies: 'backward' (selects last row in right whose 'on' key <= left's), 'forward' (selects first row in right whose 'on' key >= left's), 'nearest'. Parameters: other, left_on, right_on, on, by, by_left, by_right, strategy, suffix, tolerance, allow_parallel, force_parallel, coalesce, allow_exact_matches, check_sortedness. Tolerance: Sets numeric/temporal tolerance. check_sortedness: Checks if keys are sorted. DataFrame.explode(columns: str | Expr | Sequence[str | Expr], *more_columns: str | Expr) -> DataFrame: Explodes list/array columns to long format. DataFrame.extend(other: DataFrame) -> DataFrame. Extend DataFrame memory with values from other. Modifies DataFrame in-place. DataFrame.fill_null(value=None, strategy=None, limit=None, matches_supertype=True) -> DataFrame: Fill null values. Value: fill value. Strategy: 'forward', 'backward', 'min', 'max', 'mean', 'zero', 'one'. Limit: consecutive nulls to fill (forward/backward). Returns: DataFrame with nulls replaced. DataFrame.filter(*predicates: IntoExprColumn | Iterable[IntoExprColumn] | bool | list[bool] | np.ndarray[Any, Any], **constraints: Any) -> DataFrame[source]. Filter rows based on predicates. Parameters: predicates, constraints. Note: null comparisons filter out rows. Use ne_missing for pandas behavior. Example: df.filter(pl.col("foo") > 1). DataFrame.pivot(on, *, index=None, values=None, aggregate_function=None, maintain_order=True, sort_columns=False, separator='_') -> DataFrame: Create pivot table. Params: on (pivot columns), index (index columns), values (value columns), aggregate_function (aggregation function), maintain_order, sort_columns, separator. Eager mode only. DataFrame.get_columns() -> list[Series]: Get DataFrame as List of Series. DataFrame.group_by(*by: IntoExpr | Iterable[IntoExpr], maintain_order: bool = False, **named_by: IntoExpr) -> GroupBy: Start a group by operation. by: Columns to group by. maintain_order: Ensure group order matches input (slower, blocks streaming). Returns: GroupBy object. Example: df.group_by("a").agg(pl.col("b").sum()) DataFrame.rechunk() -> DataFrame: Rechunk DataFrame to contiguous allocation for optimal performance. InProcessQuery.cancel() -> None. Cancel the query. DataFrame.hstack(columns: list[Series] | DataFrame, *, in_place: bool = False) -> DataFrame: Add Series to DataFrame horizontally. Args: columns (Series list), in_place (bool). Example: df.hstack([x]) DataFrame.insert_column(index: int, column: IntoExprColumn) -> DataFrame. Insert Series 'column' at 'index'. In-place. DataFrame.interpolate() -> DataFrame Interpolate intermediate values (linear method). DataFrame.iter_slices(n_rows: int = 10000) -> Iterator[DataFrame]. Returns iterator of DataFrame slices. Parameters: n_rows. See also: iter_rows, partition_by. DataFrame.limit(n: int = 5) -> DataFrame. Alias for DataFrame.head(). Returns the first n rows. n: Number of rows, if negative returns all rows except last abs(n). Example: df.limit(3). DataFrame.rolling(index_column: IntoExpr, period: str | timedelta, offset: str | timedelta | None = None, closed: ClosedInterval = 'right', group_by: IntoExpr | Iterable[IntoExpr] | None = None) -> RollingGroupBy: Create rolling groups on DataFrame based on temporal or integer column. index_column: column for window grouping (sorted ascending within groups if group_by), period: window length, offset: window offset, closed: interval closure, group_by: optional grouping columns. Returns RollingGroupBy object. See also: group_by_dynamic. Example: df.rolling(index_column='dt', period='2d').agg([...]) DataFrame.pipe(function: Callable, *args, **kwargs) -> T: Apply sequence of UDFs. Args: function (Callable, receives frame as first param), *args, **kwargs. Use LazyFrame for optimization. Example: df.pipe(func, col_name="b") polars.DataFrame.reverse() -> DataFrame Reverse the DataFrame. Series.dtype: DataType: Get the data type of this Series. Example: s.dtype DataFrame.select(*exprs: IntoExpr | Iterable[IntoExpr], **named_exprs: IntoExpr) -> DataFrame: Select columns from DataFrame. Parameters: *exprs: Column(s) to select (str or Expr). **named_exprs: Additional columns, renamed to keyword. Example: df.select("foo") DataFrame.select_seq(*exprs,**named_exprs) -> DataFrame: Selects columns sequentially. exprs: column(s) to select. named_exprs: columns to select, renamed to keyword. DataFrame.slice(offset: int, length: int | None = None) -> DataFrame: Gets slice of DataFrame. offset: Start index, length: Length of slice. Example: df.slice(1, 2) InProcessQuery.fetch_blocking() -> DataFrame. Awaits result synchronously. DataFrame.sql(query:str, table_name:str='self') -> DataFrame. Execute SQL query against DataFrame. Parameters: query (SQL query string); table_name (optional, default 'self'). Returns: DataFrame. Note: Unstable feature. DataFrame.tail(n: int = 5) -> DataFrame: Get the last n rows. If n is negative, return all rows except the first abs(n). See also: head, slice. DataFrame.top_k(k: int, *, by: IntoExpr | Iterable[IntoExpr], reverse: bool | Sequence[bool] = False) -> DataFrame: Return k largest rows based on 'by' columns. Non-null elements preferred. Result not guaranteed to be sorted. See also: bottom_k. Example: df.top_k(4, by="b") DataFrame.unique(subset: ColumnNameOrSelector | Collection[ColumnNameOrSelector] | None = None, *, keep: UniqueKeepStrategy = 'any', maintain_order: bool = False) -> DataFrame: Drop duplicate rows. subset: columns to consider. keep: 'first', 'last', 'any', 'none'. maintain_order: keep original order. Fails with List columns. DataFrame.with_columns(*exprs: IntoExpr | Iterable[IntoExpr], **named_exprs: IntoExpr) -> DataFrame: Add columns to DataFrame. Replaces existing columns. Parameters: *exprs (positional expressions/column names), **named_exprs (keyword expressions, renames column). Returns: new DataFrame. Example: df.with_columns((pl.col("a") ** 2).alias("a^2")) DataFrame.with_columns_seq(*exprs: IntoExpr | Iterable[IntoExpr], **named_exprs: IntoExpr) -> DataFrame. Adds columns sequentially. LazyFrame.width property Get number of columns. Returns int. Warning: resolving schema can be expensive. DataFrame.collect_schema() -> Schema: Get ordered mapping of column names to data type. Alias for schema property. Facilitates generic code for DataFrame/LazyFrame. DataFrame.equals(other: DataFrame, *, null_equal: bool = True) -> bool. Checks if DataFrame equals another DataFrame. null_equal: consider null values as equal. DataFrame.lazy() -> LazyFrame. Start a lazy query. Operations are not executed until collect(), explain(), show_graph(), or collect_schema(). Enables query optimization and parallelism. LazyFrame.describe(percentiles: Sequence[float] | float | None = (0.25, 0.5, 0.75), *, interpolation: RollingInterpolationMethod = 'nearest') -> DataFrame. Creates summary stats for a LazyFrame. percentiles: percentiles to include (range [0, 1]). interpolation: {'nearest','higher','lower','midpoint','linear'}. Returns: DataFrame. Warning: collects the final result. DataFrame.serialize(file: IOBase | str | Path | None = None, *, format: SerializationFormat = 'binary') -> bytes | str | None: Serialize DataFrame to file or string. format: "binary" (default) or "json". file: File path or object. If None, return string. Returns bytes or str depending on format. Not stable across Polars versions. LazyFrame.null_count() -> LazyFrame: Aggregate columns as sum of null value counts. Example: lf.null_count().collect() Series.arg_min() -> int | None: Get index of minimal value. Series.arr.n_unique() -> Series: Count unique values in every sub-arrays. LazyFrame.quantile(quantile: float | Expr, interpolation: RollingInterpolationMethod = 'nearest') -> LazyFrame: Compute quantile aggregation on LazyFrame columns. property LazyFrame.columns: list[str]: Get column names. Returns: list of str. Warning: Resolving schema is potentially expensive; use collect_schema() instead. LazyFrame.schema: Schema: Get an ordered mapping of column names to their data type. LazyFrame.explain(*, format='plain'|'tree', optimized=True, type_coercion=True, predicate_pushdown=True, projection_pushdown=True, simplify_expression=True, slice_pushdown=True, comm_subplan_elim=True, comm_subexpr_elim=True, cluster_with_columns=True, collapse_joins=True, streaming=False, tree_format=None, _check_order=True) -> str. Returns query plan string representation. Parameters: format, optimized, optimization flags, tree_format(deprecated). LazyGroupBy.count() -> LazyFrame:Deprecated. Use LazyGroupBy.len(). Counts rows in each group, including nulls. LazyGroupBy.len(name: str | None = None) -> LazyFrame: Return number of rows in each group. Parameters: name - Assign name to resulting column (defaults to "len"). Example: lf.group_by("a").len().collect() polars.LazyFrame.cast(dtypes: Mapping[ColumnNameOrSelector | PolarsDataType, PolarsDataType | PythonDataType] | PolarsDataType, *, strict: bool = True) -> LazyFrame: Casts LazyFrame columns to specified dtypes. Uses selectors. Throws error if cast fails when strict=True. LazyGroupBy.mean() -> LazyFrame: Computes the mean for each group. LazyGroupBy.min() -> LazyFrame: Reduce groups to the minimal value. LazyFrame.group_by_dynamic(index_column: IntoExpr, *, every: str | timedelta, period: str | timedelta | None = None, offset: str | timedelta | None = None, include_boundaries: bool = False, closed: ClosedInterval = 'left', label: Label = 'left', group_by: IntoExpr | Iterable[IntoExpr] | None = None, start_by: StartBy = 'window') -> LazyGroupBy: Groups based on a time or index value. index_column must be sorted. Parameters: index_column: Column to group by. Must be Date/Datetime or Int32/Int64. every: Interval of the window. period: Length of the window (defaults to 'every'). offset: Offset of the window (defaults to zero). include_boundaries: Add lower/upper bound columns (impacts performance). closed: {'left', 'right', 'both', 'none'} Define which sides of the temporal interval are closed (inclusive). label: {'left', 'right', 'datapoint'} Label for the window. group_by: Also group by this column/these columns. start_by: {'window', 'datapoint', 'monday', ..., 'sunday'} Strategy to determine the start of the first window. Returns: LazyGroupBy object. Result will be sorted by index_column within each group. Note: index_column must be sorted in ascending order. If group_by is passed, then the index column must be sorted in ascending order within each group. LazyGroupBy.quantile(quantile: float, interpolation: RollingInterpolationMethod='nearest') -> LazyFrame: Compute quantile per group. LazyFrame.bottom_k(k: int, *, by: IntoExpr | Iterable[IntoExpr], reverse: bool | Sequence[bool] = False) -> LazyFrame: Return the k smallest rows. by: Column(s) to determine bottom rows. reverse: Consider k largest elements. Not guaranteed to be in order. Example: lf.bottom_k(4, by="b").collect() LazyFrame.clone() -> LazyFrame: Create copy of LazyFrame. Cheap operation, no data copy. LazyFrame.clear(n: int = 0) -> LazyFrame: Returns a copy of the LazyFrame with the same schema but with zero to 'n' empty rows. LazyFrame.drop(*columns:ColumnNameOrSelector|Iterable[ColumnNameOrSelector], strict:bool=True) -> LazyFrame. Remove columns from DataFrame. Parameters: *columns:Columns to drop, strict:Validate column names. Returns: LazyFrame. LazyFrame.filter(*predicates: IntoExprColumn | Iterable[IntoExprColumn] | bool | list[bool] | np.ndarray[Any, Any], **constraints: Any) -> LazyFrame. Filter rows based on predicate. predicates: boolean Series expression. constraints: column filters (name=value). Null comparisons yield null and filter out rows. LazyFrame.first() -> LazyFrame: Get the first row of the DataFrame. Example: lf.first().collect() LazyFrame.join(other: LazyFrame, on: str | Expr | Sequence[str | Expr] | None = None, how: JoinStrategy = 'inner', *, left_on: str | Expr | Sequence[str | Expr] | None = None, right_on: str | Expr | Sequence[str | Expr] | None = None, suffix: str = '_right', validate: JoinValidation = 'm:m', join_nulls: bool = False, coalesce: bool | None = None, maintain_order: MaintainOrderJoin | None = None, allow_parallel: bool = True, force_parallel: bool = False) -> LazyFrame. Join operation. Parameters: other: LazyFrame, on: join columns, how: {'inner', 'left', 'right', 'full', 'semi', 'anti', 'cross'}, left_on, right_on, suffix, validate: {'m:m', 'm:1', '1:m', '1:1'}, join_nulls, coalesce, maintain_order: {'none', 'left', 'right', 'left_right', 'right_left'}, allow_parallel, force_parallel. polars.LazyFrame.join_where(other: LazyFrame, *predicates: Expr | Iterable[Expr], suffix: str = '_right') -> LazyFrame: Join based on inequality predicates. Parameters: other (LazyFrame), predicates (Expr | Iterable[Expr]), suffix (str). Inner join, experimental. LazyFrame.limit(n: int = 5) -> LazyFrame: Returns the first n rows. Alias for LazyFrame.head(). LazyFrame.merge_sorted(other: LazyFrame, key: str) -> LazyFrame: Merges two sorted DataFrames by sorted key. Output will be sorted. Callers responsibility to ensure frames are sorted. Schemas must be equal. LazyFrame.select(*exprs: IntoExpr | Iterable[IntoExpr], **named_exprs: IntoExpr) -> LazyFrame: Selects columns. exprs: Columns to select (positional). named_exprs: Columns to select (keyword), renamed to keyword. polars.LazyFrame.shift(n: int | IntoExprColumn = 1, *, fill_value: IntoExpr | None = None) -> LazyFrame Shift values by n indices. Positive n is similar to SQL LAG, negative to LEAD. Parameters: n, fill_value. Series.count() -> int: Counts non-null elements. Series.bitwise_count_zeros() -> Self: Evaluate the number of unset bits. LazyFrame.top_k(k: int, by: IntoExpr | Iterable[IntoExpr], reverse: bool | Sequence[bool] = False) -> LazyFrame. Return the k largest rows. LazyFrame.unnest(columns: ColumnNameOrSelector | Collection[ColumnNameOrSelector], *more_columns: ColumnNameOrSelector) -> LazyFrame. Decompose struct columns into separate columns for each field. Returns LazyFrame. LazyFrame.unpivot(on: ColumnNameOrSelector | Sequence[ColumnNameOrSelector] | None = None, *, index: ColumnNameOrSelector | Sequence[ColumnNameOrSelector] | None = None, variable_name: str | None = None, value_name: str | None = None) -> LazyFrame. Unpivot DataFrame from wide to long. Similar to pandas.melt. LazyFrame.with_columns(*exprs: IntoExpr | Iterable[IntoExpr], **named_exprs: IntoExpr) -> LazyFrame: Add columns to this LazyFrame, replacing existing columns with the same name. *exprs: Column(s) to add (expressions or column names). **named_exprs: Additional columns (renamed to the keyword). Returns new LazyFrame. Example: lf.with_columns((pl.col("a") ** 2).alias("a^2")) LazyFrame.with_context(other: LazyFrame | list[LazyFrame]) -> LazyFrame. Add external context to computation graph. Deprecated: use concat() instead. Series.bitwise_or() -> PythonLiteral|None. Aggregate bitwise ORs. Returns: PythonLiteral or None. LazyFrame.collect_async(*, gevent: bool = False, type_coercion: bool = True, ..., streaming: bool = False) -> Awaitable[DataFrame] | _GeventDataFrameResult[DataFrame]. Collect DataFrame asynchronously. Params: gevent, type_coercion, ..., streaming. Returns: Awaitable[DataFrame] or GeventDataFrameResult[DataFrame]. Warning: unstable functionality, streaming mode unstable. LazyFrame.collect_schema() -> Schema: Resolves the schema of the LazyFrame. Example: lf.collect_schema() Series.implode() -> Self. Aggregate values into a list. Series.var(ddof: int=1) -> float|timedelta|None: Get variance. ddof:Delta Degrees of Freedom. LazyFrame.pipe(function: Callable, *args, **kwargs) -> T: Apply sequence of UDFs. function: UDF to apply. *args, **kwargs: Arguments to UDF. LazyFrame.deserialize(source: str | Path | IOBase, *, format: SerializationFormat = 'binary') -> LazyFrame: Read logical plan from file to LazyFrame. Format: 'binary', 'json'. Warning: Uses pickle for Python UDFs, security risk. Not stable across Polars versions. polars.Series.arr.all() -> Series: Checks if all boolean values are true in each subarray. Returns Series of booleans. Series.arr.any() -> Series Evaluate whether any boolean value is true for every subarray. polars.datatypes.Unknown: Represents DataType values that could not be determined statically. Methods: __init__, base_type, from_python, is_, is_decimal, is_float, is_integer, is_nested, is_numeric, is_signed_integer, is_temporal, is_unsigned_integer, to_python. polars.Series.arr.arg_min() -> Series Retrieve the index of the minimal value in every sub-array. Returns: Series (UInt32 or UInt64). Series.arr.contains(item: float | str | bool | int | date | datetime | time | IntoExprColumn) -> Series[bool]. Check if sub-arrays contain item. Returns boolean Series. Series.arr.count_matches(element: IntoExpr) -> Series: Count how often the value produced by element occurs. Series.arr.explode() -> Series. Explode array elements to separate rows. Returns Series of array element type. Example: s.arr.explode() Series.arr.get(index: int | IntoExprColumn, *, null_on_oob: bool = False) -> Series: Get value by index in sub-arrays. null_on_oob: True -> None if out of bounds, False -> error. Series.arr.reverse() -> Series: Reverse the arrays in the column. Series.arr.sort(*, descending: bool = False, nulls_last: bool = False, multithreaded: bool = True) -> Series: Sort arrays in column. Params: descending, nulls_last, multithreaded. Returns: Series. Series.arr.sum() -> Series: Compute the sum of sub-arrays. Example: s.arr.sum() Series.arr.unique(maintain_order=False) -> Series: Get unique values in the array. Series.arr.var(ddof: int = 1) -> Series: Compute the variance of the sub-arrays. polars.Expr.dt.quarter() -> Expr Extract quarter from underlying Date/Datetime representation. Returns: Expression of data type Int8 (1 to 4). class InvalidOperationError(Exception): Raised when operation is invalid against object/data structure. Example: is_in with incompatible types. Series.flags -> dict[str, bool]: Get flags set on Series. Series.bin.ends_with(suffix: IntoExpr) -> Series: Checks if binary values end with a substring. suffix: Substring to check. Series.bin.reinterpret(*, dtype: PolarsDataType, endianness: Endianness = 'little') -> Series Interpret a buffer as a numerical polars type. dtype: PolarsDataType. Series.bin.size(unit: SizeUnit = 'b') -> Series: Get size of binary values in Series. unit: size unit ('b', 'kb', 'mb', 'gb'). Returns Series of UInt32. Example: s.bin.size('kb') Series.not_() -> Series. Negate a boolean Series. Returns boolean Series. Series.cat.ends_with(suffix: str) -> Series. Check if string values end with substring. Parameters: suffix: Substring. Series.cat.get_categories() -> Series: Get categories of Categorical type Series. Example: s.cat.get_categories(). Series.cat.is_local() -> bool: Return whether the column is a local categorical (no string cache). Series.cat.len_chars() -> Series Return character length of string representation. Returns: Series[UInt32]. Use len_bytes() for ASCII for better performance. Series.arccos() -> Series. Compute element-wise inverse cosine. Series.arccosh() -> Series: Computes element-wise inverse hyperbolic cosine. Expr.dt.to_string(format: str | None = None) -> Expr Convert Date/Time/Datetime column into a String column with the given format. format: str or None. Series.arcsin() -> Series: Compute element-wise inverse sine. Example: s = pl.Series("a", [1.0, 0.0, -1.0]); s.arcsin() Series.arcsinh() -> Series: Compute element-wise inverse hyperbolic sine. Series.arctan() -> Series: Compute the element-wise inverse tangent. Series.arg_unique() -> Series. Get unique index as Series. exception polars.exceptions.TooManyRowsReturnedError. Raised when more rows returned than expected. Series.bitwise_leading_zeros() -> Self: Counts most-significant unset bits before a set bit. Series.bitwise_trailing_ones() -> Self. Counts least-significant set bits before an unset bit. Series.bitwise_trailing_zeros() -> Self. Returns number of trailing zeros. Series.cbrt() -> Series: Compute the cube root of elements. Series.cos() -> Series: Computes element-wise cosine. Series.cosh() -> Series: Compute element-wise hyperbolic cosine. Example: s.cosh() Series.cot() -> Series. Compute element-wise cotangent. Series.cum_count(*, reverse: bool = False) -> Self Return cumulative count of non-null values. Series.cum_max(reverse: bool = False) -> Series: Computes cumulative max at every element. reverse: Reverses the operation. Series.cum_min(reverse:bool=False) -> Series. Cumulative min at every element. Series.dot(other: Series | ArrayLike) -> int | float | None: Compute dot/inner product between two Series. Example: s.dot(s2) Series.ewm_mean_by(by: IntoExpr, half_life: str|timedelta) -> Series: Time-based exponentially weighted moving average. by must be DateTime, Date, or integer type. half_life supports timedelta or strings (e.g., "4d"). Series.exp() -> Series: Compute element-wise exponential. Series.peak_max() -> Self: Get boolean mask of local maximum peaks. Series.peak_min() -> Self. Get boolean mask of local minimum peaks. polars.Series.hash(seed: int = 0, seed_1: int | None = None, seed_2: int | None = None, seed_3: int | None = None) -> Series. Hash the Series to UInt64. Note: results are not stable across Polars versions. Example: s.hash(seed=42) Series.index_of(element: IntoExpr) -> int | None: Get index of first occurrence of value; None if not found. Series.log(base: float) -> Series Compute logarithm to base. Series.log10() -> Series: Compute base 10 logarithm. Series.log1p() -> Series: Computes natural logarithm of (array + 1) element-wise. polars.Series.pct_change(n: int | IntoExprColumn = 1) -> Series: Computes percentage change between values. Handles nulls. `n` specifies the period. Expr.bitwise_trailing_ones() -> Expr. Evaluate number of least-significant set bits before unset bit. Series.replace(old: IntoExpr | Sequence[Any] | Mapping[Any, Any], new: IntoExpr | Sequence[Any] | NoDefault, default: IntoExpr | NoDefault = , return_dtype: PolarsDataType | None = None) -> Self: Replace values. Requires global string cache for categorical values. Series.replace_strict(old, new, default=None, return_dtype=None) Replace values in Series. old: value(s) to replace. new: value(s) to replace with. default: Value for non-replaced values. return_dtype: Output data type. Raises InvalidOperationError if non-null values are not replaced and no default is set. Global string cache must be enabled for categorical values. polars.Series.rolling_median(window_size: int, weights: list[float] | None = None, *, min_samples: int | None = None, center: bool = False) -> Series Compute a rolling median. Unstable. Includes the row itself and window_size - 1 elements before it. Parameters: window_size, weights (elementwise multiplication), min_samples (default window_size), center. Example: s = pl.Series("a", [1.0, 2.0, 3.0, 4.0, 6.0, 8.0]); s.rolling_median(window_size=3) returns [null, null, 2.0, 3.0, 4.0, 6.0]. Series.rolling_skew(window_size: int, *, bias: bool = True) -> Series: Compute rolling skew. Parameters: window_size, bias (correct for statistical bias). Unstable API. Example: pl.Series([1, 4, 2, 9]).rolling_skew(3) Series.rolling_sum(window_size: int, weights: list[float] | None = None, *, min_samples: int | None = None, center: bool = False) -> Series: Apply rolling sum. window_size: Window length. weights: Optional weights. min_samples: Min non-null samples. center: Center labels. Series.search_sorted(element: IntoExpr | np.ndarray[Any, Any] | None, side: SearchSortedSide = 'any') -> int | Series. Find indices to maintain order. side: 'any', 'left', 'right'. Series.sin() -> Series: Computes element-wise sine values. Series.sinh() -> Series: Computes the element-wise hyperbolic sine. Series.sqrt() -> Series. Compute square root of elements. Series.tan() -> Series: Compute the element-wise value for the tangent. s = pl.Series("a", [0.0, math.pi / 2.0, math.pi]); s.tan() Series.tanh() -> Series: Compute element-wise hyperbolic tangent. Series.chunk_lengths() -> list[int]. Get length of each chunk. Example: pl.concat([s, s2], rechunk=False).chunk_lengths() returns [3, 3]. Series.has_nulls() -> bool: Check if Series contains null values. Series.has_validity() -> bool Check if Series has null values. Deprecated use has_nulls(). Series.is_duplicated() -> Series: Returns a boolean Series indicating duplicated values. Example: pl.Series("a", [1, 2, 2, 3]).is_duplicated() Series.is_finite() -> Series: Returns boolean Series indicating finite values. Example: s = pl.Series("a", [1.0, 2.0, np.inf]); s.is_finite() polars.align_frames(*frames: FrameType, on: str | Expr | Sequence[str | Expr], how: JoinStrategy = 'full', select: str | Expr | Sequence[str | Expr] | None = None, descending: bool | Sequence[bool] = False) Returns list[FrameType]. Align frames by key columns. Parameters: *frames: frames, on: key columns, how: 'full'/'left', select: columns, descending. Returns aligned frames list. Example: pl.align_frames([df1, df2], on="id"). Series.is_infinite() -> Series: Returns a boolean Series indicating which values are infinite. Returns: Series of data type Boolean. Series.is_nan() -> Series: Returns boolean Series indicating NaN values. Returns: Series of Boolean type. Example: s.is_nan() Series.is_not_nan() -> Series Returns a boolean Series indicating which values are not NaN. polars.Series.is_null() -> Series Returns a boolean Series indicating which values are null. Returns: Series (Boolean). Series.is_sorted(descending: bool = False, nulls_last: bool = False) -> bool: Checks if the Series is sorted. descending checks for descending order. nulls_last specifies null placement. Series.is_unique() -> Series[Boolean]. Mask of unique values. Series.lower_bound() -> Self: Returns the lower bound of Series dtype as a unit Series. Series.n_chunks() -> int: Returns number of chunks in Series. Series.n_unique() -> int. Count unique values in this Series. Returns: int (number of unique values). Series.null_count() -> int: Count null values in Series. Example: s.null_count() Series.upper_bound() -> Self. Returns the upper bound of Series dtype as a unit Series. Series.value_counts(*, sort: bool = False, parallel: bool = False, name: str | None = None, normalize: bool = False) -> DataFrame: Count unique value occurrences. Parameters: sort (sort by count descending), parallel (enable parallel computation), name (resulting count column name), normalize (return proportions instead of counts. Returns: DataFrame of unique values and counts. Series.__array__(dtype: npt.DTypeLike | None = None, copy: bool | None = None) -> np.ndarray: Return NumPy ndarray with given dtype. Series.__arrow_c_stream__(requested_schema: object | None = None) -> object: Exports Series via Arrow PyCapsule Interface. Series.to_arrow(compat_level:CompatLevel=None) -> Array. Get underlying Arrow array. Zero copy single chunk. Series.to_init_repr(n: int = 1000) -> str: Convert Series to instantiable string representation. Series.to_list() -> list[Any]: Convert Series to Python list (copies data). Example: s.to_list() Expr.bitwise_trailing_zeros() -> Expr: Number of least-significant unset bits before a set bit. Series.to_numpy(*, writable: bool = False, allow_copy: bool = True, use_pyarrow: bool | None = None, zero_copy_only: bool | None = None) -> ndarray. Converts Series to NumPy ndarray. Parameters: writable (ensure writable array, forces copy), allow_copy (allow memory copy, fails if False). Zero copy if no nulls, numeric type, single chunk, writable=False. Integer Series with nulls cast to float NaN. Example: s.to_numpy(allow_copy=False). Use .to_arrow().to_numpy() for PyArrow conversion. Series.to_torch() -> torch.Tensor. Converts Series to PyTorch Tensor. Warning: unstable. Notes: UInt16, UInt32, UInt64 cast to Int32, Int64, Int64. polars.Series.list.any() -> Series Evaluate whether any boolean value in a list is true. Returns: Series (Boolean). Series.list.arg_min() -> Series: Index of minimal value in each sublist. Returns UInt32 or UInt64 Series. Series.list.contains(item: float | str | bool | int | date | datetime | time | IntoExprColumn) -> Series. Checks if sublists contain item. polars.NoRowsReturnedError: Exception raised when no rows are returned but at least one row is expected. Series.list.drop_nulls()->Series:Drop nulls from list. Preserves order. Series.list.eval(expr: Expr, *, parallel: bool = False) -> Series: Run polars expr on list elements. Params: expr, parallel (use cautiously). Series.list.first() -> Series Get the first value of sublists. Series.list.get(index: int | Series | list[int], null_on_oob: bool = False) -> Series: Get value by index in sublists. null_on_oob=True returns null for out-of-bounds index, False raises error. Series.list.last() -> Series Get the last value of the sublists. Series.list.len() -> Series: Length of each list. Nulls count. Series.list.max() -> Series: Compute the max value of arrays in the list. Series.list.mean(): Compute mean of list arrays. Returns Series. Series.list.median() -> Series. Compute the median of arrays in the list. Series.list.min() -> Series: Compute the min value of the arrays in the list. Series.list.reverse() -> Series: Reverses the arrays in the list. Series.list.sample(n: int, fraction: float, with_replacement: bool, shuffle: bool, seed: int) -> Series Sample from list. Use n or fraction. Series.list.set_symmetric_difference(other: Series) -> Series: Compute set symmetric difference between elements in list and other. other: Right hand side Series. Example: a.list.set_symmetric_difference(b) Series.list.set_union(other: Series) -> Series: Computes set union between elements in list and elements of other Series. Example: a.list.set_union(b) Series.shuffle(seed: int | None = None) -> Series. Shuffle Series contents. seed: Random seed. polars.Series.list.std(ddof: int = 1) -> Series: Computes the standard deviation of arrays in a list. Returns a Series of floats. Series.list.sum() -> Series: Sum all arrays in the list. Example: s = pl.Series("values", [[1], [2, 3]]); s.list.sum() Series.list.tail(n: int | Expr = 5) -> Series: Slice the last n values of every sublist. Parameters: n: Number of values to return. Example: s.list.tail(2) Series.list.unique(*, maintain_order: bool = False) -> Series: Get unique values in the list. Params: maintain_order (bool). Example: s = pl.Series("a", [[1, 1, 2], [2, 3, 3]]); s.list.unique() Series.list.var(ddof: int = 1) -> Series: Compute variance of arrays in the list. ddof: degrees of freedom. Example: s.list.var() Series.__getitem__(key: SingleIndexSelector | MultiIndexSelector) -> Any | Series: Get part of Series as new Series or scalar. Parameters: key (row(s) to select). Returns: Series or scalar. Example: s[0:2] Series.alias(name: str) -> Series: Rename Series. Series.eq(other: Any) -> Series | Expr. Equivalent of series == other. Series.bottom_k(k: int = 5) -> Series: Return the k smallest elements. Non-null elements are preferred. Time complexity: O(n). Parameters: k (Number of elements to return). See also: top_k. Series.ceil() -> Series: Rounds up to the nearest integer value. Only works on floating point Series. Example: s = pl.Series("a", [1.12345, 2.56789, 3.901234]); s.ceil() # returns [2.0, 3.0, 4.0] Series.clear(n: int = 0) -> Series: Creates an empty copy of the Series. polars.Series.clone() -> Self: Creates a shallow copy of the Series. Series.extend_constant(value: IntoExpr, n: int | IntoExprColumn) -> Series: Extend Series with n copies of value. value can be None for nulls. Example: s.extend_constant(99, n=2) Series.fill_nan(value: int | float | Expr | None) -> Series. Fill NaN values. Use fill_null() for missing values. Series.floor() -> Series. Rounds down to the nearest integer value. Only works on floating point Series. Series.gather(indices: int | list[int] | Expr | Series | np.ndarray) -> Series: Takes values by index. Series.gather_every(n: int, offset: int = 0) -> Series. Take every nth value in Series. Start from offset. polars.Series.head(n: int = 10) -> Series: Get first n elements. n: number of elements (positive or negative). Series.limit(n: int = 10) -> Series. Get first n elements. Alias for head(). n: Negative value returns all elements except last abs(n). Series.new_from_index(index: int, length: int) -> Self: Create Series from index. Example: s = pl.Series("a", [1, 2, 3, 4, 5]); s.new_from_index(1, 3) Series.qcut(quantiles: Sequence[float] | int, *, labels: Sequence[str] | None = None, left_closed: bool = False, allow_duplicates: bool = False, include_breaks: bool = False) -> Series: Bin continuous values into discrete categories based on quantiles. quantiles: Quantile probabilities or number of bins. labels: Category names. left_closed: Intervals left-closed. allow_duplicates: Drop duplicates in quantiles. include_breaks: Include breakpoint column; output type changes to Struct. Returns: Categorical Series or Struct Series if include_breaks is True. Series.rechunk(*, in_place: bool = False) -> Self: Creates a single chunk of memory. Series.rename(name: str) -> Series: Rename this Series. Alias for Series.alias(). Parameters: name (new name). Example: s = pl.Series("a", [1,2,3]); s.rename("b") Series.reverse() -> Series Return Series in reverse order. Series.round(decimals: int = 0) -> Series. Round floating point data. decimals: number of decimals. Example: s = pl.Series("a", [1.12345, 2.56789, 3.901234]); s.round(2) # [1.12, 2.57, 3.9] Series.round_sig_figs(digits: int) -> Series: Rounds to a number of significant figures. digits: Number of significant figures. Example: s.round_sig_figs(2) Series.sample(n: int | None = None, *, fraction: float | None = None, with_replacement: bool = False, shuffle: bool = False, seed: int | None = None) -> Series: Sample from Series. Parameters: n (items to return, not with fraction), fraction (fraction of items, not with n), with_replacement, shuffle, seed. Example: s.sample(2, seed=0) Series.set(filter: Series, value: int | float | str | bool | None) -> Series: Set values based on a boolean mask. Consider using `pl.when().then().otherwise()` instead. Series.shrink_to_fit(*, in_place: bool = False): Shrink Series memory usage. Returns Series. Parameters: in_place=False. Series.slice(offset:int, length:int|None=None) -> Series. Get a slice of this Series. Parameters: offset:Start index, length:Length of the slice. Returns: Series. Series.top_k(k: int = 5) -> Series: Return the k largest elements. Non-null elements preferred. Not guaranteed to be sorted. O(n) time complexity. Parameters: k (number of elements). See also: bottom_k. s = pl.Series("a", [2, 5, 1, 4, 3]); s.top_k(3) Series.unique(*, maintain_order: bool = False) -> Series: Get unique elements in series. maintain_order: Maintain order of data. Example: s = pl.Series("a", [1, 2, 2, 3]); s.unique().sort() # returns [1, 2, 3] Series.get_chunks() -> list[Series]: Get chunks of Series as list of Series. Series.ge(other: Any) -> Series | Expr: Method equivalent of operator expression series >= other. class OutOfBoundsError(Exception). Raised when index out of bounds. Series.set_sorted(*, descending: bool = False) -> Self. Flag Series as sorted. Enables fast paths for sorted arrays. Warning: incorrect results if Series is not sorted. Series.to_physical() -> Series Cast Series to physical representation of logical dtype. Physical representation unstable. Series.ne_missing(other: Any) -> Series | Expr: Equivalent of != where None == None. other: comparison value. Series.pow(exponent: int | float | Series) -> Series. Raise Series to the power of exponent. Series.str.concat(delimiter: str | None = None, *, ignore_nulls: bool = True) -> Series: Vertically concatenate string values. Deprecated, use join(). delimiter: Separator. ignore_nulls: Ignore nulls or propagate. Series.str.contains_any(patterns: Series | list[str], *, ascii_case_insensitive: bool = False) -> Series: Uses Aho-Corasick to check if any patterns are contained. ascii_case_insensitive: Enables ASCII case-insensitive matching. Supports literal string matches only. Series.str.count_matches(pattern: str | Series, *, literal: bool = False) -> Series. Count regex matches. pattern: regex or literal string. Returns UInt32 Series, null if original null. Series.str.decode(encoding: TransferEncoding, *, strict: bool = True) -> Series: Decode values using encoding. encoding: 'hex', 'base64'. strict: Raise error or mask with null. Returns Binary Series. Example: s.str.decode("hex") Series.str.encode(encoding: TransferEncoding) Returns Series[String]. Encode string value. Parameters: encoding: 'hex', 'base64'. Example: s.str.encode('hex'). Series.str.escape_regex() -> Series. Escape regex meta characters in string values. Returns String Series. polars.Series.str.extract_groups(pattern: str) -> Series: Extract regex capture groups. Parameters: pattern (str regex with capture groups). Returns Series of Struct[String]. Series.str.normalize(form: UnicodeForm = 'NFC') -> Series Returns Unicode normal form of string values. Parameters: form (str 'NFC', 'NFKC', 'NFD', 'NFKD'). Returns: Series. Series.str.join(delimiter: str = '', *, ignore_nulls: bool = True) -> Series: Concatenate string values to single string. delimiter: Delimiter between values. ignore_nulls: Ignore nulls (default True). Returns: String Series. If ignore_nulls=False and column contains nulls, output is null. Example: s.str.join("-") Series.str.json_decode(dtype: PolarsDataType | None = None, *, infer_schema_length: int | None = 100) Returns Series. Parse string as JSON. Parameters: dtype: cast dtype, infer_schema_length: rows for inference. Error on invalid JSON. See also json_path_match. Series.str.len_chars() -> Series: Return the length of each string as the number of characters (UInt32). See also: len_bytes (faster for ASCII). Example: s = pl.Series(["Caf", "345", "", None]); s.str.len_chars() Series.str.split(by: IntoExpr, inclusive=False) -> Series: Split string by substring. By: substring to split by. Inclusive: include the split character/string in results. Returns: Series of List(String). Series.str.pad_end(length: int, fill_char: str = ' ') -> Series. Pad end of string to length. Parameters: length: target length, fill_char: padding char. Series.str.replace_many(patterns: Series | list[str] | Mapping[str, str], replace_with: Series | list[str] | str | NoDefault = , *, ascii_case_insensitive: bool = False) -> Series: Replace multiple string patterns using Aho-Corasick. patterns: str patterns to search/replace, or mapping of patterns to replacements. replace_with: replacement strings. ascii_case_insensitive: enable ASCII case-insensitive matching. Supports string literals only. Example: s.str.replace_many(["you", "me"], ["me", "you"]) Series.str.reverse() -> Series. Returns string values in reversed order. Series.str.split_exact(by: IntoExpr, n: int, *, inclusive: bool = False) -> Series: Split string by substring using n splits, result in struct of n+1 fields. If cannot make n splits, remaining fields will be null. Parameters: by (substring to split by), n (number of splits), inclusive (include split character in results). Returns: Series of data type Struct. Example: df["x"].str.split_exact("_", 1).alias("fields"). Series.str.splitn(by, n) -> Series: Split string by substring, return at most n items. If splits < n-1, remaining elements will be null. If splits >= n-1, last substring contains remainder. Series.str.strip_prefix(prefix: IntoExpr) -> Series: Remove prefix once from string if found. Example: s.str.strip_prefix("foo") Series.str.strip_suffix(suffix: IntoExpr) -> Series Remove suffix from string once if found. Series.struct.fields = []. Struct fields attribute. Expr.meta.ne(other: ExprMetaNameSpace | Expr) -> bool. Check if expression is not same as other. Series.str.tail(n: int | Expr) -> Series: Return last n characters of each string. n: length of slice (integer or expression). Negative indexing supported. If string length is less than n, full string is returned. Example: s = pl.Series(["pear", None, "papaya", "dragonfruit"]); s.str.tail(5) # returns "pear", null, "apaya", "fruit" Series.str.to_date(format: str | None = None, *, strict: bool = True, exact: bool = True, cache: bool = True) -> Series: Convert a String column into a Date column. format is the format string. strict raises error on failure. exact requires exact format match. Example: s.str.to_date() Series.str.to_decimal(inference_length: int = 100) -> Series: Convert String column to Decimal. inference_length: elements to parse for precision/scale. Series.str.to_time(format: str | None = None, *, strict: bool = True, cache: bool = True) -> Series: Convert a String column into a Time column. Parameters: format (strptime format), strict (raise error on failure), cache (use conversion cache). Example: s.str.to_time("%H:%M") Series.str.to_titlecase() -> Series: Modify strings to titlecase. Capitalizes first letter of each word, lowercase rest. Non-alphanumeric characters define word boundaries. Example: s = pl.Series("quotes", ["'e.t. phone home'", "you talkin' to me?", "to infinity,and BEYOND!"]); s.str.to_titlecase() Series.str.to_uppercase() -> Series. Convert strings to uppercase. Example: s = pl.Series("foo", ["cat", "dog"]); s.str.to_uppercase() # ["CAT", "DOG"] Series.struct.field(name: str) -> Series: Retrieve field of Struct as a new Series. Params: name (str). Example: s = pl.Series([{"a": 1, "b": 2}, {"a": 3, "b": 4}]); s.struct.field("a") Series.struct.json_encode() -> Series Convert struct Series to string Series with json values. Series.struct.rename_fields(names: Sequence[str]) -> Series: Rename the fields of the struct. names: New names in order of struct's fields. Example: s.struct.rename_fields(["c", "d"]) Series.dt.dst_offset() Returns Series[Duration]. Get daylight saving time offset. See also Series.dt.base_utc_offset. Series.dt.base_utc_offset() -> Series: Base offset from UTC as Duration. See also: Series.dt.dst_offset Series.dt.century() -> Series: Extract century from Date/Datetime columns. Returns: Series of Int32. Example: s.dt.century() Series.dt.date() -> Series: Extract (local) date from Date/Datetime columns. Series.dt.day() -> Series: Extract day from Date/Datetime column (1-31). Series.dt.epoch(time_unit: EpochTimeUnit = 'us') -> Series: Get time since Unix epoch. time_unit: {'us', 'ns', 'ms', 's', 'd'} Series.dt.hour() -> Series: Extract hour (0-23) from Datetime Series. Series.dt.is_leap_year() -> Series. Checks if the year of date/datetime is a leap year. Series.dt.iso_year() -> Series: Extracts ISO year from Date/Datetime. Returns: Int32 Series. Series.dt.max() -> dt.date | dt.datetime | dt.timedelta | None: Returns maximum as Python datetime. Series.dt.mean() -> TemporalLiteral | None: Deprecated since 1.0.0, Use Series.mean. Returns the mean as a Python DateTime. Series.dt.median() -> TemporalLiteral | None. Return median as python DateTime. Deprecated: Use Series.median instead. Series.dt.millisecond() -> Series: Extract milliseconds from DateTime column. Returns Int32 Series. Series.dt.min() -> dt.date|dt.datetime|dt.timedelta|None: Returns minimum as Python datetime. polars.Series.dt.month() -> Series Extract month from date/datetime (1-12). Returns: Series (Int8). Example: date.dt.month() returns the month number. Series.dt.offset_by(by: str | Expr) -> Series: Offset date by relative time offset (e.g., "1y", "1mo"). Parameters: by - offset string (1ns, 1us, 1ms, 1s, 1m, 1h, 1d, 1w, 1mo, 1q, 1y). Returns: Series of Date or Datetime type. Only single minus sign allowed. Example: dates.dt.offset_by("1y") polars.Series.dt.round(every: str | dt.timedelta | IntoExprColumn) -> Series: Round date/datetime to buckets. Parameters: every (str | timedelta). Returns Series of Date or Datetime. Series.dt.second(*, fractional: bool = False) -> Series: Extract seconds from DateTime. Parameters: fractional (bool) - include fractional component. Returns: Series (Int8 or Float64). Example: s.dt.second(fractional=True) polars.exceptions.SQLInterfaceError: Exception raised for SQL interface errors. Series.dt.time() -> Series: Extracts local time from Date/Datetime/Time columns. Returns Series of Time. Series.dt.total_days() -> Series: Extract total days from Duration. Returns: Series (Int64). Example: date.diff().dt.total_days() Series.dt.total_microseconds() -> Series. Extract total microseconds from a Duration type. Returns Series of Int64. Series.dt.total_minutes() -> Series[Int64]. Extract total minutes from Duration type. Series.dt.total_nanoseconds() -> Series: Extract total nanoseconds from a Duration type as Int64. Example: date.diff().dt.total_nanoseconds() Series.dt.total_seconds() -> Series: Extracts total seconds from Duration. Returns Series of Int64. Series.dt.week() -> Series. Extracts the week from date/datetime. Returns ISO week (1-53). Series.dt.weekday() -> Series Extract weekday from Date/Datetime column. Returns ISO weekday (monday = 1, sunday = 7). Returns Int8 Series. Series.dt.year() -> Series. Extract year from Date/Datetime columns. Returns: Series[Int32]. Expr.agg_groups() -> Expr. Get group indexes in aggregation context. Expr.any(ignore_nulls: bool = True) -> Expr: Checks if any value is True. Works on Boolean columns. ignore_nulls: Ignores nulls (default True); False uses Kleene logic. polars.Expr.arg_max() -> Expr. Get the index of the maximal value. Example: df.select(pl.col("a").arg_max()) Expr.arg_min() -> Expr. Get index of minimal value. polars.exceptions.PolarsWarning: Base class for all Polars warnings. Expr.first() -> Expr: Get the first value from a group. Expr.implode() -> Expr: Aggregate values into a list. Example: df = pl.DataFrame({"a": [1, 2, 3], "b": [4, 5, 6]}); df.select(pl.all().implode()) # returns lists [1,2,3] and [4,5,6] polars.Expr.last() -> Expr: Get last value. Expr.len() -> Expr: Returns the number of elements in the column (UInt32). Null values are counted. See also: count. Expr.max() -> Expr: Get maximum value. Example: df = pl.DataFrame({"a": [-1.0, float("nan"), 1.0]}); df.select(pl.col("a").max()) Expr.mean() -> Expr Get mean value. Expr.median() -> Expr: Get median value using linear interpolation. Example: df.select(pl.col("a").median()) polars.exceptions.SQLSyntaxError. Exception raised from SQL interface for invalid syntax. Expr.nan_min() -> Expr: Get minimum value, propagate NaN values. Example: df.select(pl.col("a").nan_min()) polars.Expr.null_count() -> Expr. Count null values. Example: df.select(pl.all().null_count()) Expr.quantile(quantile: float | Expr, interpolation: RollingInterpolationMethod = 'nearest') -> Expr: Get quantile value. Parameters: quantile (0.0 to 1.0), interpolation ('nearest', 'higher', 'lower', 'midpoint', 'linear'). Expr.std(ddof: int = 1) -> Expr: Get standard deviation. ddof: Delta Degrees of Freedom (N - ddof). Default ddof is 1. Example: df.select(pl.col("a").std()) Expr.sum() -> Expr. Get sum value. Dtypes Int8, UInt8, Int16, UInt16 cast to Int64 before summing. Expr.arr.sum() -> Expr: Compute the sum values of the sub-arrays. df = pl.DataFrame(data={"a": [[1, 2], [4, 3]]}, schema={"a": pl.Array(pl.Int64, 2)}); df.select(pl.col("a").arr.sum()) SchemaError Exception raised when schema mismatch error occurs. Expr.arr.all() -> Expr: Evaluates if all boolean values are true for every subarray. Expr.arr.any() -> Expr. True if any boolean value is true for subarray. Expr.arr.to_list() -> Expr: Convert Array column to List column. Expr.arr.explode() -> Expr: Returns a column with a separate row for every array element. Returns Expr with the data type of the array elements. Example: df.select(pl.col("a").arr.explode()) Expr.arr.first() -> Expr: Get first value of sub-arrays. Expr.arr.join(separator: IntoExprColumn, *, ignore_nulls: bool=True) -> Expr. Join string items in array with separator. Params: separator, ignore_nulls. Returns: Expr. Errors if array not string type. Nulls propagated if ignore_nulls=False. Expr.arr.max() -> Expr. Compute the max values of sub-arrays. Example: df.select(pl.col("a").arr.max()). Expr.arr.median() -> Expr Compute median of values in sub-arrays. Expr.arr.min() -> Expr: Compute the min values of the sub-arrays. Example: pl.DataFrame({"a": [[1, 2], [4, 3]]}).select(pl.col("a").arr.min()) Expr.arr.sort(*, descending: bool = False, nulls_last: bool = False) -> Expr Sort the arrays in this column. exception polars.exceptions.SchemaFieldNotFoundError. Raised when schema field is not found. polars.Expr.arr.unique(*, maintain_order: bool = False) -> Expr: Get unique array values. Parameters: maintain_order (bool). Expr.arr.var(ddof: int = 1) -> Expr: Compute variance of sub-arrays. Example: df.select(pl.col("a").arr.var()) Expr.bin.contains(literal: IntoExpr) -> Expr: Check if binaries contain substring. Returns boolean expression. polars.exceptions.StringCacheMismatchError Exception for string cache mismatches. polars.Expr.bin.ends_with(suffix: IntoExpr) -> Expr: Checks if string values end with a binary substring. Returns: Boolean Expr. Expr.bin.size(unit: SizeUnit = 'b') -> Expr. Get binary value size in unit. unit: 'b', 'kb', 'mb', 'gb', 'tb'. Returns UInt32 or Float64 Expr. polars.Expr.bin.starts_with(prefix: IntoExpr) -> Expr Check if values start with a binary substring. Parameters: prefix. Returns: Expr (Boolean). See also: ends_with, contains. Expr.is_between(lower_bound: IntoExpr, upper_bound: IntoExpr, closed: ClosedInterval = 'both') -> Expr Check if this expression is between the given lower and upper bounds. If lower_bound > upper_bound, returns False. Expr.is_duplicated() -> Expr: Boolean mask for duplicated values. Expr.is_finite() -> Expr: Returns boolean Series indicating finite values. Expr.is_last_distinct() -> Expr: Returns boolean mask indicating last occurrence of each distinct value. Result is Boolean Expr. Example: df.with_columns(pl.col("a").is_last_distinct().alias("last")) polars.Expr.is_not_nan() -> Expr. Returns a boolean Series indicating which values are not NaN. Do not confuse with Null/None. Example: df.with_columns(pl.col(pl.Float64).is_not_nan().name.suffix("_is_not_nan")) Expr.is_null() -> Expr: Returns a boolean Series indicating which values are null. Expr.is_unique() -> Expr. Get mask of unique values. Expr.cat.len_bytes() -> Expr: Return the byte-length of string representation. Returns: Expr of UInt32. See Also: len_chars. Note: len_bytes() is O(1), len_chars() is O(n). Example: df.with_columns(pl.col("a").cat.len_bytes().alias("n_bytes")). Expr.cat.len_chars() -> Expr[UInt32]. Returns character length of string representation. Notes: use len_bytes for ASCII for better performance. Expr.arctan() -> Expr: Compute the element-wise inverse tangent. Returns Expr of Float64. Expr.cat.starts_with(prefix:str) -> Expr. Check if string representations start with substring. Parameters: prefix (substring). Returns: Expr. Note: prefix must be literal string. Expr.exclude(columns: str | PolarsDataType | Collection[str] | Collection[PolarsDataType], *more_columns: str | PolarsDataType) -> Expr: Exclude columns from multi-column expression. Expr.arccos() -> Expr: Element-wise inverse cosine. Returns Float64. Expr.arccosh() -> Expr: Compute the element-wise inverse hyperbolic cosine. Expr.bitwise_count_ones() -> Expr: Count the number of set bits. Expr.arg_unique() -> Expr: Get index of first unique value. Returns: Expr. polars.Expr.bitwise_leading_ones() -> Expr: Counts leading set bits. Expr.bitwise_leading_zeros() -> Expr: Evaluates number of leading zeros. Expr.cbrt() -> Expr Compute cube root of elements. polars.Expr.cot() -> Expr: Compute cotangent. Returns Expr[Float64]. polars.Expr.cumulative_eval(expr: Expr, *, min_samples: int = 1, parallel: bool = False) -> Expr Run an expression over a sliding window that increases 1 slot every iteration. expr: Expression to evaluate. min_samples: Minimum valid values. parallel: Run in parallel. Warning: Potentially slow, O(n^2) complexity. Expr.dot(other: Expr | str) -> Expr. Compute dot product between two expressions. Params: other expression. Expr.rolling_max_by(by: IntoExpr, window_size: timedelta | str, min_samples: int=1, closed: ClosedInterval='right') -> Expr Rolling max based on another column. window_size can be temporal. Unstable. Expr.sinh() -> Expr: Compute hyperbolic sine. Returns Float64. Expr.ewm_mean(com, span, half_life, alpha, adjust=True, min_samples=1, ignore_nulls=False) -> Expr: Computes exponentially-weighted moving average. Requires one of com, span, half_life, or alpha. adjust divides by a decaying factor. min_samples is minimum non-null values. ignore_nulls affects weight calculation. Expr.rolling_mean_by(by:IntoExpr,window_size:timedelta|str,min_samples:int=1,closed:ClosedInterval='right')->Expr: Rolling mean by column. Unstable API. by column must be DateTime, Date, UInt64, UInt32, Int64, or Int32. window_size: timedelta or string. Expr.log(base: float = 2.718) -> Expr. Compute logarithm to given base (default e). Expr.log10() -> Expr: Compute the base 10 logarithm element-wise. Example: df.select(pl.col("values").log10()) Expr.rolling_median_by(by: IntoExpr, window_size: timedelta | str, *, min_samples: int = 1, closed: ClosedInterval = 'right') -> Expr: Compute a rolling median based on another column. Parameters: by (DateTime, Date, UInt64, UInt32, Int64, Int32), window_size (timedelta or str), min_samples, closed ('left', 'right', 'both', 'none'). polars.exceptions.StructFieldNotFoundError: Exception raised when a specified Struct field is not found. polars.exceptions.MapWithoutReturnDtypeWarning: Warning issued when map_elements is performed without specifying the return dtype. Expr.peak_min() -> Expr: Get boolean mask of local minimum peaks. Example: pl.col("a").peak_min() Expr.radians() -> Expr: Convert degrees to radians. Returns Float64. Expr.rank(method='average', descending=False, seed=None) -> Expr: Assign ranks to data, handling ties. Method options: 'average', 'min', 'max', 'dense', 'ordinal', 'random'. Descending: rank in descending order. Seed: for method="random". Example: df.select(pl.col("a").rank("ordinal")) Expr.rolling_max(window_size: int, weights: list[float] | None = None, *, min_samples: int | None = None, center: bool = False) -> Expr. Moving max over array. window_size: Length of window. weights: Multiply elementwise with values in window. min_samples: Non-null values required, defaults to window_size. center: Center labels. Example: df.with_columns(pl.col("A").rolling_max(window_size=2, weights=[0.25, 0.75])) Expr.rolling_median(window_size: int, weights: list[float] | None = None, *, min_samples: int | None = None, center: bool = False) -> Expr. Compute rolling median. Params: window_size, weights, min_samples, center. Expr.sign() -> Expr: Compute element-wise sign function on numeric types. Returns -1 if x < 0, 1 if x > 0, x otherwise (typically 0 or NaN). Null values preserved. Example: df.select(pl.col.a.sign()) polars.Expr.rolling_std(window_size: int, weights: list[float] | None = None, *, min_samples: int | None = None, center: bool = False, ddof: int = 1) -> Expr Compute a rolling standard deviation. Parameters: window_size, weights (optional), min_samples (defaults to window_size), center (bool), ddof (int). Expr.search_sorted(element: IntoExpr | np.ndarray[Any, Any], side: SearchSortedSide = 'any') -> Expr: Find indices to insert elements to maintain order. element: Expression or scalar. side: 'any', 'left', 'right' (first, leftmost, rightmost suitable location). Example: df.select(pl.col("values").search_sorted(3)) UnsuitableSQLError: Exception raised for unsuitable SQL in database methods. Expr.skew(bias: bool = True) -> Expr: Computes the sample skewness. Expr.tan() -> Expr: Compute element-wise tangent. Returns: Expr of Float64. Example: df.select(pl.col("a").tan().round(2)) Expr.tanh() -> Expr. Compute element-wise hyperbolic tangent. Returns Float64 Expr. Expr.unique_counts() -> Expr: Return count of unique values in order of appearance. Returns counts only (faster than value_counts). Example: df.select(pl.col("id").unique_counts()) polars.groups(column: str) -> Expr: Syntactic sugar for pl.col("foo").agg_groups(). polars.approx_n_unique(*columns: str) -> Expr. Approximate count of unique values using HyperLogLog++ algorithm. polars.arange(start: int | IntoExprColumn = 0, end: int | IntoExprColumn | None = None, step: int = 1, *, dtype: PolarsIntegerType = Int64, eager: bool = False) -> Expr | Series. Generate range of integers. Alias for int_range(). eager=True returns Series. Expr.list.max() -> Expr: Computes the max value of the lists in the array. Example: df.with_columns(max=pl.col("values").list.max()) polars.arctan2(y: str | Expr, x: str | Expr) -> Expr. Compute two-argument arctan in radians. Returns angle between positive x-axis and ray to (x,y). y: Column name or Expression, x: Column name or Expression. polars.arctan2d(y: str | Expr, x: str | Expr) -> Expr: Computes two-argument arctan in degrees. Deprecated: Use arctan2 followed by Expr.degrees(). Returns angle between positive x-axis and ray from origin to (x,y). polars.concat_list(exprs: IntoExpr | Iterable[IntoExpr], *more_exprs: IntoExpr) -> Expr. Horizontally concatenate columns into a single list column. Linear time. exprs: Columns to concatenate. Accepts expression input, strings as names. *more_exprs: Additional columns. Example: df.with_columns(concat_list=pl.concat_list("a", "b")) corr(a: IntoExpr, b: IntoExpr, method: CorrelationMethod='pearson', propagate_nans: bool=False) -> Expr Compute Pearson's or Spearman correlation between two columns. polars.count(*columns: str) -> Expr: Count non-null values in columns. No columns returns row count (deprecated, use len()). Parameters: columns: column names. polars.cov(a: IntoExpr, b: IntoExpr, ddof: int = 1) -> Expr: Compute the covariance between two columns/expressions. polars.cum_sum(*names: str) -> Expr: Cumulatively sum all values. Syntactic sugar for col(names).cum_sum(). See also: cumsum_horizontal. Example: df.select(pl.cum_sum("a")) polars.date(year: Expr | str | int, month: Expr | str | int, day: Expr | str | int) -> Expr. Create Date literal expression. month: 1-12, day: 1-31. polars.date_range(start: date | datetime | IntoExprColumn, end: date | datetime | IntoExprColumn, interval: str | timedelta = '1d', *, closed: ClosedInterval = 'both', eager: bool = False) -> Series | Expr: Generate date range. Parameters: start, end, interval, closed, eager (bool). Returns Expr or Series of Date. polars.datetime(year, month, day, hour=None, minute=None, second=None, microsecond=None, time_unit='us', time_zone=None, ambiguous='raise') Create Datetime literal expression. year, month, day, hour, minute, second, microsecond: Column or literal values. time_unit: 'us', 'ms', 'ns'. time_zone: Time zone string. ambiguous: 'raise', 'earliest', 'latest', 'null'. Returns Expr. polars.ones(n: int | Expr, dtype: PolarsDataType = Float64, *, eager: bool = False) -> Expr | Series: Construct a column of length n filled with ones. n: Length. dtype: Data type. eager: Evaluate immediately. Returns: Expr or Series. polars.duration(*, weeks: Expr | str | int | None = None, days: Expr | str | int | None = None, hours: Expr | str | int | None = None, minutes: Expr | str | int | None = None, seconds: Expr | str | int | None = None, milliseconds: Expr | str | int | None = None, microseconds: Expr | str | int | None = None, nanoseconds: Expr | str | int | None = None, time_unit: TimeUnit | None = None) -> Expr: Create Duration from time components. Returns: Expr of data type Duration. Use Expr.dt.offset_by() for non-fixed durations. Example: pl.duration(days=1) polars.first(*columns: str) -> Expr: Get first column or value. No columns: returns first column expression; otherwise, first value(s) of given column(s). polars.format(f_string: str, *args: Expr|str) -> Expr: Format expressions as a string. f_string: string with placeholders. args: expressions filling placeholders. polars.from_epoch(column: str | Expr | Series | Sequence[int], time_unit: EpochTimeUnit = 's') -> Expr | Series. Parse epoch timestamp to Polars Datetime/Date. Parameters: column: Series/Expr, time_unit: {'d', 's', 'ms', 'us', 'ns'}. Returns: pl.Date or pl.Datetime. polars.head(column: str, n: int = 10) -> Expr: Get first n rows of column. Parameters: column: Column name. n: Number of rows. Example: df.select(pl.head("a", 2)) polars.int_range(start: int|IntoExprColumn=0, end: int|IntoExprColumn|None=None, step: int=1, dtype: PolarsIntegerType=Int64, eager: bool=False) -> Expr|Series: Generate integer range. end omitted defaults to start, start defaults to 0. polars.len() -> Expr: Return the number of rows in the context. Returns: Expr of type UInt32. exception polars.exceptions.PerformanceWarning: Warning issued to indicate potential performance pitfalls. polars.map_batches(exprs: Sequence[str] | Sequence[Expr], function: Callable[[Sequence[Series]], Series], return_dtype: PolarsDataType | None = None) -> Expr Map a custom function over multiple columns/expressions. exprs: Input Series. function: Function to apply. return_dtype: Output Series dtype. polars.max_horizontal(*exprs: IntoExpr | Iterable[IntoExpr]) -> Expr: Get max value horizontally across columns. Parameters: *exprs: Column(s) (str or Expr). Example: df.with_columns(max=pl.max_horizontal("a", "b")) Expr.map_batches(function: Callable[[Series], Series | Any], return_dtype: PolarsDataType | None = None, *, agg_list: bool = False, is_elementwise: bool = False, returns_scalar: bool = False) -> Expr. Apply custom python function to Series. Params: function, return_dtype, agg_list, is_elementwise, returns_scalar. Returns: Expr. Warning: return_dtype may lead to unexpected results if not provided. polars.mean_horizontal(*exprs: IntoExpr | Iterable[IntoExpr], ignore_nulls: bool = True) -> Expr Compute mean horizontally across columns. Parameters: *exprs (columns/expressions), ignore_nulls (ignore nulls, default True; if False, null input leads to null output). Example: df.with_columns(mean=pl.mean_horizontal("a", "b")) computes the horizontal mean of columns "a" and "b". polars.quantile(column: str, quantile: float | Expr, interpolation: RollingInterpolationMethod = 'nearest') -> Expr. Calculate column quantile. Interpolation methods: 'nearest', 'higher', 'lower', 'midpoint', 'linear'. polars.rolling_corr(a: str | Expr, b: str | Expr, *, window_size: int, min_samples: int | None = None, ddof: int = 1) -> Expr. Compute rolling correlation between two columns/expressions. Parameters: a: Column/Expr. b: Column/Expr. window_size: window length. min_samples: min non-null values. ddof: delta degrees of freedom. polars.rolling_cov(a: str | Expr, b: str | Expr, *, window_size: int, min_samples: int | None = None, ddof: int = 1) -> Expr: Computes rolling covariance between two columns/expressions. Expr.list.mean() -> Expr: Compute mean of lists in array. polars.sum_horizontal(*exprs:IntoExpr|Iterable[IntoExpr], ignore_nulls:bool=True) -> Expr. Sum values horizontally across columns. Parameters: *exprs:Columns to sum, ignore_nulls:Ignore nulls (default True). Returns: Expr. polars.time(hour: Expr | str | int | None = None, minute: Expr | str | int | None = None, second: Expr | str | int | None = None, microsecond: Expr | str | int | None = None) -> Expr: Create a Polars literal expression of type Time. Returns: Expr of type Time. Example: df.with_columns(pl.time(pl.col("hour"), pl.col("minute"))) polars.when(*predicates: IntoExprColumn | Iterable[IntoExprColumn] | bool, **constraints: Any) -> When Starts a when-then-otherwise expression (like if-else). Chain with .then(). and optionally .when().then(). .otherwise(). for a final value, else null. All expressions computed in parallel. Predicates combined with &. polars.exceptions.UnstableWarning: Warning for unstable functionality. Expr.list.any() -> Expr: Check if any boolean in list is true. Expr.list.arg_max() -> Expr: Index of max value in every sublist. Returns: UInt32 or UInt64 Expr. Expr.list.first() -> Expr: Get the first value of the sublists. Returns: Expr. polars.Expr.list.drop_nulls() -> Expr Drop all null values in the list, preserving original order. Expr.list.join(separator: IntoExprColumn, ignore_nulls: bool = True) -> Expr: Joins strings within sublists using a separator. Errors if inner type is not String. ignore_nulls: handles nulls. Expr.list.len() -> Expr Returns number of elements in each list. Null values counted. exception polars.exceptions.ChronoFormatWarning: Warning issued when a chrono format string contains dubious patterns. Uses Rust's chrono crate, which differs from Python's datetime. Expr.list.reverse() -> Expr: Reverse arrays in list. Expr.list.shift(n: int | IntoExprColumn = 1) -> Expr Shift list values by n indices. n: shift forward, negative shifts backward. pl.col("a").list.shift() Expr.explode() Explode list expression. Item to new row. Returns Expr. Expr.list.set_difference(other: IntoExpr) -> Expr. Compute the SET DIFFERENCE between the elements in this list and the elements of other. Expr.list.set_symmetric_difference(other: IntoExpr) -> Expr. Compute set symmetric difference between list elements with other list elements. Expr.list.to_struct(n_field_strategy: ListToStructWidthStrategy = 'first_non_null', fields: Sequence[str] | Callable[[int], str] | None = None, upper_bound: int = 0, _eager: bool = False) -> Expr: Convert list to struct. Requires correct 'upper_bound' for LazyFrame. Expr.list.sort(descending: bool = False, nulls_last: bool = False) -> Expr: Sort lists in column. descending=True for descending sort, nulls_last=True to place nulls last. Expr.list.sum() -> Expr: Sum all lists in the array. Expr.list.to_array(width: int) -> Expr. Convert List column to Array. Params: width. Returns: Expr. Expr.append(other: IntoExpr, *, upcast: bool = True) -> Expr Append expressions by adding chunks of other. Expr.backward_fill(limit: int | None = None) -> Expr: Fill missing values with next non-null value. limit: consecutive null values to fill. See also: forward_fill, shift. Example: df = pl.DataFrame({"a": [1, 2, None], "b": [4, None, 6], "c": [None, None, 2]}); df.select(pl.all().backward_fill()) Expr.drop_nans() -> Expr: Drop floating point NaN values, preserving original order. Note: NaN is different from null. Use drop_nulls() for null values. polars.exceptions.CustomUFuncWarning: Warning for custom ufunc handling differences from numpy ufunc. polars.Expr.bottom_k(k: int | IntoExprColumn = 5) -> Expr. Return the k smallest elements. Prefers non-null. O(n) complexity. Not guaranteed to be sorted. k is number of elements. See also: top_k, top_k_by, bottom_k_by. Expr.bottom_k_by(by, k=5, reverse=False) Get k smallest elements from column `by`. Args: by (column to sort by), k (number of elements), reverse (sort reversed). Returns Expr. Non-null elements are preferred over null. Output order not guaranteed; use sort(). Expr.fill_null(value: Any | Expr | None = None, strategy: FillNullStrategy | None = None, limit: int | None = None) -> Expr: Fills null values. Strategies: forward, backward, min, max, mean, zero, one. polars.Expr.flatten() -> Expr: Flatten list/string column. Alias for Expr.list.explode(). Expr.floor() -> Expr: Rounds down to the nearest integer. Works only on floating point Series. polars.build_info() -> dict[str, Any]: Return detailed Polars build information as a dictionary with keys: "compiler", "time", "dependencies", "features", "host", "target", "git", "version". Expr.head(n: int | Expr = 10) -> Expr. Gets first n rows. n: Number of rows to return. Expr.interpolate(method: str = 'linear') -> Expr: Fill null values using interpolation. Args: method ({"linear", "nearest"}). Example: df.select(pl.all().interpolate()) Expr.interpolate_by(by: IntoExpr) -> Expr: Interpolates null values using another column. Expr.lower_bound() -> Expr: Calculate lower bound for dtype. Returns unit Series. Expr.rechunk() -> Expr. Create single memory chunk for Series. Returns: Expr. Expr.shuffle(seed=None) -> Expr: Shuffle expression contents. Note: shuffled independently. Seed: random number generator seed. Example: df.select(pl.col("a").shuffle(seed=1)) Expr.repeat_by(by: Series | Expr | str | int) -> Expr: Repeat elements as specified, expanding into a List. Parameters: by: Numeric column, coerced to UInt32. Returns: Expr of type List. Example: df.select(pl.col("a").repeat_by("n")) Expr.replace_strict(old: IntoExpr | Sequence[Any] | Mapping[Any, Any], new: IntoExpr | Sequence[Any], *, default: IntoExpr | NoDefault = , return_dtype: PolarsDataType | None = None) -> Expr. Replaces values. Parameters: old, new, default (error if no default and value not replaced), return_dtype. Raises: InvalidOperationError. Notes: String cache for categorical values. Expr.rle() -> Expr: Compresses data using run-length encoding. Returns a Struct with 'len' (UInt32) and 'value' (original type) fields. Expr.round(decimals: int = 0) -> Expr: Round floating point data by decimals digits. Expr.round_sig_figs(digits: int) -> Expr: Round to significant figures. digits: Number of significant figures. polars.Expr.meta.is_regex_projection() -> bool Indicate if this expression expands to columns that match a regex pattern. Expr.slice(offset: int | Expr, length: int | Expr | None = None) -> Expr: Gets slice of expression. offset: Start index, length: Length of slice. Example: df.select(pl.all().slice(1, 2)) Expr.sort_by(by: IntoExpr | Iterable[IntoExpr], *more_by: IntoExpr, descending: bool | Sequence[bool] = False, nulls_last: bool | Sequence[bool] = False, multithreaded: bool = True, maintain_order: bool = False) -> Expr: Sort column by other columns' order. Projection/selection: whole column sorted. Group by: groups sorted. Expr.tail(n: int | Expr = 10) -> Expr: Gets the last n rows. n: Number of rows to return. get_index_type() -> DataType: Return index data type (UInt32 or UInt64). Expr.top_k_by(by: IntoExpr | Iterable[IntoExpr], k: int | IntoExprColumn = 5, reverse: bool | Sequence[bool] = False) -> Expr: Return elements of k largest in 'by' column(s). Expr.meta.has_multiple_outputs() -> bool: Indicate if expression expands to multiple expressions. polars.Expr.meta.is_column() -> bool: Checks if expression is a basic, unaliased column (not regex). Expr.meta.is_literal(allow_aliasing: bool = False) -> bool: Checks if the expression is a literal value. allow_aliasing=True also allows aliased literals. Expr.meta.root_names(): Get root column names. Returns list[str]. Expr.meta.show_graph(*, show: bool = True, output_path: str | Path | None = None, raw_output: bool = False, figsize: tuple[float, float] = (16.0, 12.0)) -> str | None. Format expression as Graphviz graph. Requires Graphviz. Params: show, output_path, raw_output, figsize. Expr.meta.undo_aliases() -> Expr: Undoes renaming (alias, name.keep). Expr.meta.write_json(file: IOBase | str | Path | None = None) -> str | None: Write expression to JSON. Deprecated since 0.20.11, use serialize(). Expr.deserialize(source: str | Path | IOBase | bytes, *, format: SerializationFormat = 'binary') -> Expr. Read serialized expression from file. Format: 'binary', 'json'. Warning: Pickle for Python UDFs, security risk. Not stable across Polars versions. Expr.name.map_fields(function: Callable[[str], str]) -> Expr: Renames struct fields using a mapping function. Expr.name.prefix(prefix: str) -> Expr: Add a prefix to the root column name. Parameters: prefix. See also: suffix. Can only be called as the last expression in a chain. Extending the API: register_expr_namespace(name), register_dataframe_namespace(name), register_lazyframe_namespace(name), register_series_namespace(name). Decorators for registering custom functionality with Expr, DataFrame, LazyFrame, Series. Cannot override existing Polars namespaces. Example: @pl.api.register_expr_namespace("greetings") class Greetings: def __init__(self, expr: pl.Expr): ... Expr.name.suffix(suffix: str) -> Expr: Add a suffix to the root column name. Parameters: suffix. Undoes previous renaming. Only one name operation per chain. Use .name.map for advanced renaming. Expr.name.to_lowercase() -> Expr: Make the root column name lowercase. See also: prefix, suffix, to_uppercase. Note: Undoes previous renaming. Must be last in chain. Only one name operation allowed. Example: df.with_columns(pl.all().name.to_lowercase()) Expr.str.replace(pattern: str | Expr, value: str | Expr, *, literal: bool = False, n: int = 1) -> Expr: Replace first matching regex/literal substring. Args: pattern (regex), value (replacement), literal (bool), n (num replacements). Use (?iLmsuxU) for regex flags. Escape $ as $$. Example: df.with_columns(cost_usd=pl.col("cost").str.replace(r"#(\d+)", "$${1}")) Expr.eq(other: Any) -> Expr Equality operator (==). Parameters: other (Any). Returns: Expr. Example: pl.col("x").eq(pl.col("y")) Expr.eq_missing(other: Any) -> Expr. Equality operator where None == None. Differs from default eq. Expr.ge(other: Any) -> Expr: Returns expression expr >= other. other: Literal or expression to compare. Example: pl.col("x").ge(pl.col("y")) Expr.str.replace_all(pattern: str | Expr, value: str | Expr, *, literal: bool = False) -> Expr. Replace all matching regex/literal substrings with a new string value. Returns Expr. Expr.ne_missing(other: Any) -> Expr: Method equivalent of expr != other where None == None. Differs from ne where nulls propagate. Expr.add(other: Any) -> Expr. Add 'other' to the expression. other: numeric or string value, or expression. Example: df.with_columns(pl.col("x").add(2).alias("x+int")) polars.Expr.neg() -> Expr Method equivalent of unary minus operator -expr. Expr.sub(other: Any) -> Expr. Subtraction operator (expr - other). Parameters: other: Numeric literal or expression. Expr.str.strip_chars(characters: IntoExpr = None) -> Expr: Remove leading and trailing characters. Parameters: characters. If characters is None, remove whitespace. Expr.str.contains_any(patterns: IntoExpr, *, ascii_case_insensitive: bool = False) -> Expr: Check if any patterns are contained in the string (Aho-Corasick). patterns: string patterns. ascii_case_insensitive: ASCII case-insensitive matching. Expr.str.count_matches(pattern: str | Expr, *, literal: bool = False) -> Expr: Count regex matches. pattern: regex pattern. literal: treat pattern as literal string. Returns UInt32 Expr, null if original is null. Expr.str.ends_with(suffix: str | Expr) -> Expr: Check if string values end with substring suffix. Returns boolean Expr, null if original value is null. See contains, starts_with. DataFrame.write_ndjson(file: IOBase | str | Path | None = None) -> str | None: Serialize to newline delimited JSON. Parameters: file: File path or writable file-like object. If None, returns string. Example: df = pl.DataFrame({"foo": [1, 2, 3], "bar": [6, 7, 8]}); df.write_ndjson() Expr.str.explode() -> Expr[String]. Returns column with separate row for each character. Deprecated: Use .str.split('').explode(). Empty strings to null. polars.Expr.str.splitn(by: IntoExpr, n: int) -> Expr Split the string by a substring, restricted to returning at most n items. Parameters: by (substring to split by), n (max number of items to return). Returns: Expr (Struct with String fields). Expr.str.json_decode(dtype: PolarsDataType | None = None, *, infer_schema_length: int | None = 100) -> Expr. Parses string values as JSON. dtype: cast to dtype, inferred if None. infer_schema_length: rows to scan for schema inference. Expr.str.starts_with(prefix: str|Expr) -> Expr: Check if string values start with a substring. prefix:prefix substring. polars.read_parquet(source: FileSource, *, columns: list[int] | list[str] | None = None, n_rows: int | None = None, row_index_name: str | None = None, row_index_offset: int = 0, parallel: ParallelStrategy = 'auto', use_statistics: bool = True, hive_partitioning: bool | None = None, glob: bool = True, schema: SchemaDict | None = None, hive_schema: SchemaDict | None = None, try_parse_hive_dates: bool = True, rechunk: bool = False, low_memory: bool = False, storage_options: dict[str, Any] | None = None, credential_provider: CredentialProviderFunction | Literal['auto'] | None = 'auto', retries: int = 2, use_pyarrow: bool = False, pyarrow_options: dict[str, Any] | None = None, memory_map: bool = True, include_file_paths: str | None = None, allow_missing_columns: bool = False) -> DataFrame: Read DataFrame from parquet file. Parameters: source: file path, columns, n_rows, row_index_name, row_index_offset, parallel, use_statistics, hive_partitioning, glob, schema, hive_schema, try_parse_hive_dates, rechunk, low_memory, storage_options, credential_provider, retries, use_pyarrow, pyarrow_options, memory_map, include_file_paths, allow_missing_columns. Expr.str.len_chars(): Return string length in characters. Returns Expr[UInt32]. See also len_bytes. Note: len_bytes faster for ASCII. polars.scan_parquet(source: FileSource, *, n_rows: int | None = None, row_index_name: str | None = None, row_index_offset: int = 0, parallel: ParallelStrategy = 'auto', use_statistics: bool = True, hive_partitioning: bool | None = None, glob: bool = True, schema: SchemaDict | None = None, hive_schema: SchemaDict | None = None, try_parse_hive_dates: bool = True, rechunk: bool = False, low_memory: bool = False, cache: bool = True, storage_options: dict[str, Any] | None = None, credential_provider: CredentialProviderFunction | Literal['auto'] | None = 'auto', retries: int = 2, include_file_paths: str | None = None, allow_missing_columns: bool = False) -> LazyFrame. Lazily read parquet file(s). Optimizer pushdown. source: file path, n_rows, row_index_name, row_index_offset, parallel: parallelism strategy, use_statistics, hive_partitioning, glob, schema, hive_schema, try_parse_hive_dates, rechunk, low_memory, cache, storage_options, credential_provider, retries, include_file_paths, allow_missing_columns. Expr.str.replace_many(patterns: IntoExpr | Mapping[str, str], replace_with: IntoExpr | NoDefault = , *, ascii_case_insensitive: bool = False) -> Expr: Replaces many matches using the Aho-Corasick algorithm. patterns: String patterns to replace, or a mapping of patterns to replacements. replace_with: Strings to replace with. ascii_case_insensitive: Enable ASCII case-insensitive matching. Supports string literals only; no regex. Example: pl.col("lyrics").str.replace_many(["me", "you"], ["you", "me"]) Expr.str.reverse() -> Expr. Returns reversed string values. Expr.str.split(by: IntoExpr, *, inclusive: bool=False) -> Expr. Split string by substring. Params: by, inclusive. Returns: Expr. If inclusive=True, include separator in results. Expr.str.split_exact(by: IntoExpr, n: int, *, inclusive: bool = False) -> Expr: Split string by substring using n splits. Returns struct of n+1 fields. DataFrame.write_parquet(file: str | Path | IO[bytes], compression: ParquetCompression = 'zstd', compression_level: int | None = None, statistics: bool | str | dict[str, bool] = True, row_group_size: int | None = None, data_page_size: int | None = None, use_pyarrow: bool = False, pyarrow_options: dict[str, Any] | None = None, partition_by: str | Sequence[str] | None = None, partition_chunk_size_bytes: int = 4294967296, storage_options: dict[str, Any] | None = None, credential_provider: CredentialProviderFunction | Literal['auto'] | None = 'auto', retries: int = 2) -> None: Write DataFrame to Parquet file. UNSTABLE features: `partition_by`, cloud storage. polars.Expr.str.strip_chars_end(characters: IntoExpr = None) -> Expr: Remove trailing characters. characters: set of chars or None(whitespace). See also strip_suffix, strip_chars_start. Expr.str.strptime(dtype: PolarsTemporalType, format: str | None = None, *, strict: bool = True, exact: bool = True, cache: bool = True, ambiguous: Ambiguous | Expr = 'raise') -> Expr: Convert string column to Date/Datetime/Time. Parameters: dtype (Date, Datetime, Time), format (strftime, inferred if None), strict (error if fails), exact (require exact format), cache, ambiguous ('raise', 'earliest', 'latest', 'null'). Datetime infers unit from format, defaults to "us". Example: s.str.strptime(pl.Datetime, "%Y-%m-%d %H:%M%#z") Expr.str.to_uppercase() Modify strings to uppercase. Returns Expr. Expr.str.tail(n: int | IntoExprColumn) -> Expr. Return last n chars of string. Params: n. Returns: Expr. Supports negative indexing. Expr.str.to_integer(base: int | IntoExprColumn = 10, strict: bool = True) -> Expr: Converts String column to Int64. base: Radix. strict: Raise error on parse/overflow. False silently converts to Null. Expr.str.to_time(format: str | None = None, *, strict: bool = True, cache: bool = True) -> Expr. Converts String column to Time. format: str format, inferred if None. strict: error if conversion fails. cache: use cache for conversion. Expr.str.to_titlecase() -> Expr: Modify strings to titlecase. Expr.struct.field(name: str | list[str], *more_names: str) -> Expr: Retrieves one or multiple Struct field(s). name: Field name. more_names: Additional field names. Expr.struct.unnest() -> Expr. Expand struct into individual fields. Alias for Expr.struct.field("*"). Returns Expr. Expr.struct.rename_fields(names: Sequence[str]) -> Expr: Rename fields of the struct. Parameters: names: New names (same order as struct's fields). Example: df.select(pl.col("struct_col").struct.rename_fields(["www", "xxx", "yyy", "zzz"])).unnest("struct_col") Expr.dt.convert_time_zone(time_zone: str) -> Expr: Convert Datetime to given time_zone. If converting from a time-zone-naive datetime, conversion happens as if converting from UTC. Example: df.select(pl.col("date").dt.convert_time_zone(time_zone="Europe/London")) Expr.dt.datetime() -> Expr: Returns datetime. Deprecated: Use dt.replace_time_zone(None). Applies to Datetime columns. Returns: Datetime Expr. Expr.dt.day() -> Expr[Int8]. Extract day from Date or Datetime. Returns day of month (1-31). Expr.dt.is_leap_year() -> Expr. Determine if the year of the underlying date is a leap year. Expr.dt.month_end() -> Expr: Roll forward to the last day of the month. Returns: Expr of Date or Datetime type. Expr.dt.iso_year() -> Expr. Extract ISO year from Date/Datetime columns. Returns ISO year (Int32), may differ from calendar year. Expr.dt.microsecond() -> Expr: Extract microseconds from underlying DateTime representation. Applies to Datetime columns. Expr.dt.minute() -> Expr: Extract minutes from DateTime representation (0-59). Applies to Datetime columns. Returns: Expr of data type Int8. Example: df.with_columns(pl.col("datetime").dt.minute().alias("minute")) Expr.dt.month() -> Expr: Extract month (1-12) from Date/Datetime column. Returns Int8 Expr. Expr.over(partition_by: IntoExpr | Iterable[IntoExpr], *more_exprs: IntoExpr, order_by: IntoExpr | Iterable[IntoExpr] | None = None, mapping_strategy: WindowMappingStrategy = 'group_to_rows') -> Expr: Compute expressions over groups, similar to SQL window functions. Parameters: partition_by (columns to group by), *more_exprs (additional group by columns), order_by, mapping_strategy ('group_to_rows', 'join', 'explode'). 'group_to_rows' assigns values back to original positions, 'join' joins as List, and 'explode' creates new rows. polars.Expr.dt.nanosecond() -> Expr: Extract nanoseconds from DateTime. Returns Expr[Int32]. Expr.dt.replace_time_zone(time_zone: str | None, *, ambiguous: Ambiguous | Expr = 'raise', non_existent: NonExistent = 'raise') -> Expr. Replace datetime time zone. Modifies timestamp. ambiguous/non_existent: 'raise', 'earliest', 'latest', 'null'. Expr.dt.second(*, fractional: bool = False) -> Expr. Extract seconds from DateTime. Param: fractional: include fractional component. Returns: Expr[Int8 | Float64]. polars.json_normalize(data: dict | Sequence[dict], *, separator: str = '.', max_level: int | None = None, schema: Schema | None = None, strict: bool = True, infer_schema_length: int | None = 100) -> DataFrame: Normalize semi-structured JSON data into a flat table. Args: data, separator, max_level, schema, strict, infer_schema_length. Unstable API. Example: pl.json_normalize(data, max_level=1) Expr.dt.total_days() -> Expr: Extract the total days from a Duration type. Returns: Expr of data type Int64. Expr.dt.total_microseconds() -> Expr[Int64]. Extract total microseconds from Duration type. Expr.dt.total_milliseconds() -> Expr: Extracts total milliseconds from a Duration. Expr.dt.total_nanoseconds() -> Expr[Int64]. Extract total nanoseconds from Duration. Expr.dt.truncate(every: str | dt.timedelta | Expr) -> Expr: Divides the date/datetime range into buckets, mapping each value to the start of its bucket. Weekly buckets start on Monday. Parameters: every: Interval start and period length (e.g., "1h", timedelta(hours=1)). Returns: Expr of type Date or Datetime. Note: every argument uses a string language (1ns, 1us, 1ms, 1s, 1m, 1h, 1d, 1w, 1mo, 1q, 1y). These strings can be combined (e.g., 3d12h4m25s). Expr.dt.weekday() -> Expr: Extract the week day from the underlying Date representation. Returns the ISO weekday number where monday = 1 and sunday = 7. Returns: Expr of data type Int8. polars.from_dict(data: Mapping[str, Sequence[object] | Mapping[str, Sequence[object]] | Series], schema: SchemaDefinition | None = None, *, schema_overrides: SchemaDict | None = None, strict: bool = True) -> DataFrame: Construct DataFrame from dict of sequences. Parameters: data (dict of sequences), schema (list of names, (name, type), or {name:type}; auto-inferred if None), schema_overrides ({str:DataType}), strict (bool, default True, throw error if type mismatch, else cast or set null). Returns: DataFrame. Example: pl.from_dict({"a": [1, 2], "b": [3, 4]}) polars.using_string_cache() -> bool: Checks if the global string cache is enabled. polars.Enum(categories: Series | Iterable[str] | type[enum.Enum]) A fixed categorical encoding of a unique set of strings. Parameters: categories: Series, iterable of strings, or Python enum. base_type(): Returns DataType's root type class. from_python(py_type): Returns Polars data type from Python type. is_(other): Checks if DataType is the same. union(other): Union of two Enums. Attributes: categories. polars.from_pandas(data: pd.DataFrame | pd.Series[Any] | pd.Index[Any] | pd.DatetimeIndex, *, schema_overrides: SchemaDict | None = None, rechunk: bool = True, nan_to_null: bool = True, include_index: bool = False) -> DataFrame | Series: Construct Polars DataFrame/Series from pandas DataFrame/Series/Index. Clones data. Requires pandas and pyarrow. schema_overrides: Override inferred types. rechunk: Ensure contiguous memory. nan_to_null: Convert NaN to None. include_index: Load non-default indexes as columns. Returns: DataFrame. Example: pl.from_pandas(pd_df) polars.from_repr(data: str) -> DataFrame | Series: Construct DataFrame/Series from string representation. Parameters: data: String containing DataFrame/Series repr. Notes: Handles UTF8_FULL and UTF8_FULL_CONDENSED tables. List/Struct/Object dtypes not supported. See also: DataFrame.to_init_repr, Series.to_init_repr. polars.datatypes.Utf8: Alias of String. polars.concat(items: Iterable[PolarsType], *, how: ConcatMethod = 'vertical', rechunk: bool = False, parallel: bool = True) -> PolarsType: Concatenate DataFrames, LazyFrames, or Series. items: Items to concatenate. how: {'vertical', 'vertical_relaxed', 'diagonal', 'diagonal_relaxed', 'horizontal', 'align'}. rechunk: Rechunk result. parallel: For LazyFrames, execute in parallel. Example: pl.concat([df1, df2]) polars.escape_regex(s: str) -> str: Escape regex meta characters in string. Param: s: string. polars.collect_all(lazy_frames: Iterable[LazyFrame], *, type_coercion: bool = True, predicate_pushdown: bool = True, projection_pushdown: bool = True, simplify_expression: bool = True, no_optimization: bool = False, slice_pushdown: bool = True, comm_subplan_elim: bool = True, comm_subexpr_elim: bool = True, cluster_with_columns: bool = True, collapse_joins: bool = True, streaming: bool = False, _check_order: bool = True) -> list[DataFrame]. Collect multiple LazyFrames in parallel. Parameters: lazy_frames: List of LazyFrames. type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, no_optimization, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, collapse_joins: optimizations. streaming: streaming mode. Warning: Streaming mode unstable. Returns: List of DataFrames. polars.enable_string_cache() -> None: Enable the global string cache. `disable_string_cache()` to disable. class Float64: 64-bit floating point type. base_type(): Returns fundamental type class. from_python(py_type): Returns Polars data type from Python type. is_(other): Checks if DataType is same as another. is_float(): Check if floating point. max(): Returns literal expression of max value. min(): Returns literal expression of min value. to_python(): Returns Python type corresponding to Polars data type. Catalog.list_tables(catalog_name: str, schema_name: str) -> list[TableInfo]. List available tables under schema. Unstable API. polars.datatypes.UInt8: 8-bit unsigned integer type. Methods: base_type(), from_python(py_type), is_(other), is_decimal(), is_float(), is_integer(), is_nested(), is_numeric(), is_signed_integer(), is_temporal(), is_unsigned_integer(), max(), min(), to_python() class polars.datatypes.UInt16. 16-bit unsigned integer type. Methods: is_integer(), is_unsigned_integer(), max(), min(). Catalog.get_table_info(catalog_name: str, schema_name: str, table_name: str) -> TableInfo: Retrieve table metadata. Parameters: catalog_name, schema_name, table_name. Unstable API. class polars.datatypes.UInt64: 64-bit unsigned integer type. Methods: base_type() -> DataType: Returns fundamental type class. from_python(py_type) -> DataType: Returns Polars data type. is_(other) -> bool: Checks if same DataType. is_decimal() -> bool: Checks decimal type. is_float() -> bool: Checks float type. is_integer() -> bool: Checks integer type. is_nested() -> bool: Checks nested type. is_numeric() -> bool: Checks numeric type. is_signed_integer() -> bool: Checks signed integer type. is_temporal() -> bool: Checks temporal type. is_unsigned_integer() -> bool: Checks unsigned integer type. max() -> Expr: Max value. min() -> Expr: Min value. to_python() -> type: Returns Python type. class polars.datatypes.Date: Calendar date datatype. 32-bit signed integer (days since Unix epoch). Methods: __init__, base_type(), from_python(py_type), is_(other), is_decimal(), is_float(), is_integer(), is_nested(), is_numeric(), is_signed_integer(), is_temporal(), is_unsigned_integer(), to_python(). class Boolean. Boolean type. Methods: __init__, base_type(), from_python(py_type), is_(other), is_decimal(), is_float(), is_integer(), is_nested(), is_numeric(), is_signed_integer(), is_temporal(), is_unsigned_integer(), to_python(). polars.datatypes.Field(name: str, dtype: PolarsDataType): Defines a field in a Struct DataType. Field.__init__(name, dtype) class polars.datatypes.String. UTF-8 string type. Methods: base_type(), from_python(py_type), is_..., to_python(). polars.read_csv(source: str | Path | IO[str] | IO[bytes] | bytes, *, has_header: bool = True, columns: Sequence[int] | Sequence[str] | None = None, new_columns: Sequence[str] | None = None, separator: str = ',', comment_prefix: str | None = None, quote_char: str | None = '"', skip_rows: int = 0, skip_lines: int = 0, schema: SchemaDict | None = None, schema_overrides: Mapping[str, PolarsDataType] | Sequence[PolarsDataType] | None = None, null_values: str | Sequence[str] | dict[str, str] | None = None, missing_utf8_is_empty_string: bool = False, ignore_errors: bool = False, try_parse_dates: bool = False, n_threads: int | None = None, infer_schema: bool = True, infer_schema_length: int | None = 100, batch_size: int = 8192, n_rows: int | None = None, encoding: CsvEncoding | str = 'utf8', low_memory: bool = False, rechunk: bool = False, use_pyarrow: bool = False, storage_options: dict[str, Any] | None = None, skip_rows_after_header: int = 0, row_index_name: str | None = None, row_index_offset: int = 0, sample_size: int = 1024, eol_char: str = '\n', raise_if_empty: bool = True, truncate_ragged_lines: bool = False, decimal_comma: bool = False, glob: bool = True) -> DataFrame: Read a CSV file into a DataFrame. source: file path or file-like object. has_header: first row is header. columns: Columns to select (indices or names). new_columns: Rename columns after parsing. separator: Separator character. comment_prefix: Prefix for comment lines. quote_char: Quoting character. skip_rows: Skip rows. skip_lines: Skip lines. schema: Provide schema. schema_overrides: Overwrite dtypes during inference. null_values: Values to interpret as null. missing_utf8_is_empty_string: missing utf8 values are treated as empty string. ignore_errors: Try to keep reading lines if errors occur. try_parse_dates: Automatically parse dates. n_threads: Number of threads. infer_schema: Infer schema from data. infer_schema_length: Rows to scan for schema inference. batch_size: Lines to read into buffer at once. n_rows: Stop reading after n_rows. encoding: Encoding type. low_memory: Reduce memory pressure. rechunk: Ensure columns are contiguous in memory. use_pyarrow: Use pyarrow's CSV parser. storage_options: Extra options for fsspec.open(). skip_rows_after_header: Skip rows after header. row_index_name: Insert row index column. row_index_offset: Start row index at offset. eol_char: End of line character. raise_if_empty: Raise error if no data. truncate_ragged_lines: Truncate lines longer than schema. decimal_comma: Use comma as decimal separator. glob: expand path via globbing. Returns DataFrame. See also scan_csv. Example: pl.read_csv("data.csv", separator="|") Extending the API: register_expr_namespace(name), register_dataframe_namespace(name), register_lazyframe_namespace(name), register_series_namespace(name). Decorators for custom functionality. Cannot override existing Polars namespaces (str, dt). Example: @pl.api.register_expr_namespace("greetings") class Greetings: def __init__(self, expr: pl.Expr): self._expr = expr def hello(self) -> pl.Expr: return (pl.lit("Hello ") + self._expr).alias("hi there") class polars.datatypes.Object: Data type for arbitrary Python objects. Methods include: base_type(), from_python(py_type), is_(other), is_decimal(), is_float(), is_integer(), is_nested(), is_numeric(), is_signed_integer(), is_temporal(), is_unsigned_integer(), to_python(). polars.read_clipboard(separator: str = '\\t', **kwargs: Any) -> DataFrame. Reads text from clipboard as CSV. DataFrame.write_clipboard(*, separator: str = '\t', **kwargs: Any) -> None: Write DataFrame to clipboard as CSV. separator: Field separator. kwargs: Arguments for write_csv. See also: read_clipboard, write_csv. BatchedCsvReader.next_batches(n: int) -> list[DataFrame] | None: Read n batches. Batches parallelized over threads. Returns list of DataFrames. DataFrame.write_database(table_name:str, connection:ConnectionOrCursor|str, *, if_table_exists:DbWriteMode='fail', engine:DbWriteEngine|None=None, engine_options:dict[str,Any]|None=None) -> int. Write DataFrame to database. Parameters: table_name:Table name, connection:DB connection URI, if_table_exists:{'append', 'replace', 'fail'}, engine:{'sqlalchemy', 'adbc'}, engine_options:Engine options. Returns: int (rows affected or -1). polars.read_ods(source: FileSource, *, sheet_id: int | Sequence[int] | None = None, sheet_name: str | list[str] | tuple[str] | None = None, has_header: bool = True, columns: Sequence[int] | Sequence[str] | None = None, schema_overrides: SchemaDict | None = None, infer_schema_length: int | None = 100, include_file_paths: str | None = None, drop_empty_rows: bool = True, drop_empty_cols: bool = True, raise_if_empty: bool = True) -> DataFrame | dict[str, DataFrame]: Reads ODS file into DataFrame. Handles multiple sheets. Allows schema overrides. polars.read_ipc(source: str | Path | IO[bytes] | bytes, *, columns: list[int] | list[str] | None = None, n_rows: int | None = None, use_pyarrow: bool = False, memory_map: bool = True, storage_options: dict[str, Any] | None = None, row_index_name: str | None = None, row_index_offset: int = 0, rechunk: bool = True) -> DataFrame: Read Arrow IPC (Feather v2) file. source: File path or object. columns: Columns to select. n_rows: Number of rows to read. use_pyarrow: Use pyarrow reader. memory_map: Memory map file. storage_options: Storage options. row_index_name: Row index name. row_index_offset: Row index offset. rechunk: Rechunk data. Warning: memory_map prevents writing to the same file. polars.read_ipc_schema(source) -> dict[str, DataType]: Get schema of IPC file without reading data. source: file path or file-like object. LazyFrame.sink_ipc(path: str | Path, *, compression: str | None = 'zstd', maintain_order: bool = True, type_coercion: bool = True, predicate_pushdown: bool = True, projection_pushdown: bool = True, simplify_expression: bool = True, slice_pushdown: bool = True, collapse_joins: bool = True, no_optimization: bool = False, storage_options: dict[str, Any] | None = None, credential_provider: CredentialProviderFunction | Literal['auto'] | None = 'auto', retries: int = 2) -> None: Stream query to IPC file. Params: path, compression, maintain_order, type_coercion, optimizations, storage_options, credential_provider, retries. Returns: None. Warning: Streaming unstable. polars.scan_iceberg(source: str | Table, snapshot_id: int | None = None, storage_options: dict | None = None) -> LazyFrame: Lazily read Apache Iceberg table. source: path or table. snapshot_id: specific snapshot. storage_options: cloud storage config. LazyFrame.sink_ndjson(path: str | Path, *, maintain_order: bool = True, type_coercion: bool = True, _type_check: bool = True, predicate_pushdown: bool = True, projection_pushdown: bool = True, simplify_expression: bool = True, slice_pushdown: bool = True, collapse_joins: bool = True, no_optimization: bool = False, storage_options: dict[str, Any] | None = None, credential_provider: CredentialProviderFunction | Literal['auto'] | None = 'auto', retries: int = 2) -> None: Evaluate the query in streaming mode and write to an NDJSON file. Parameters: path (file path), maintain_order, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, collapse_joins, no_optimization, storage_options (cloud options), credential_provider (cloud credentials), retries (cloud retries). Streaming mode is unstable. lf = pl.scan_csv("/path/to/my_larger_than_ram_file.csv"); lf.sink_ndjson("out.ndjson") polars.read_parquet_schema(source: str | Path | IO[bytes] | bytes) -> dict[str, DataType] Get schema of Parquet file. class CredentialProviderAzure(*, scopes: list[str] | None = None, tenant_id: str | None = None, _storage_account: str | None = None, _verbose: bool = False): Azure Credential Provider. Requires azure-identity. __init__(*, scopes: list[str] | None = None, tenant_id: str | None = None, _storage_account: str | None = None, _verbose: bool = False) -> None: Initialize credential provider. scopes: Scopes for get_token. tenant_id: Azure tenant ID. Config.set_float_precision(precision: int | None = None) -> type[Config]: Sets float precision for display. None reverts to default. Affects Float32/64 only. Config.set_fmt_float(fmt: FloatFmt | None = 'mixed') -> type[Config]: Controls floating point value display. fmt: {'mixed', 'full'}. 'mixed' limits decimal places and uses scientific notation; 'full' prints full precision. Config.set_fmt_table_cell_list_len(n: int | None) -> type[Config]: Set number of elements to display for List values. Negative values: all, 0: [...] if not empty, 1: last item. Config.set_streaming_chunk_size(size: int | None) -> type[Config]: Overwrite chunk size in streaming engine. Parameters: size: rows per chunk. polars.Config.set_tbl_cols(n: int | None) -> type[Config]: Set visible table columns. Parameters: n (int | None). Config.set_tbl_formatting(format: Literal['ASCII_FULL', 'ASCII_FULL_CONDENSED', 'ASCII_NO_BORDERS', 'ASCII_BORDERS_ONLY', 'ASCII_BORDERS_ONLY_CONDENSED', 'ASCII_HORIZONTAL_ONLY', 'ASCII_MARKDOWN', 'MARKDOWN', 'UTF8_FULL', 'UTF8_FULL_CONDENSED', 'UTF8_NO_BORDERS', 'UTF8_BORDERS_ONLY', 'UTF8_HORIZONTAL_ONLY', 'NOTHING'] | None, rounded_corners: bool | None) -> Config: Set table formatting style. Raises ValueError if format unrecognised. Python API Introduction SQL interface entry points: SQLContext, polars.sql(), DataFrame.sql(), LazyFrame.sql(), polars.sql_expr(). Querying: Global SQL polars.sql(): execute SQL against DataFrame, LazyFrame, Series, Pandas DataFrame/Series, PyArrow Table/RecordBatch. Frame SQL DataFrame.sql()/LazyFrame.sql(): execute SQL against frame ("self"). Expression SQL polars.sql_expr(): create Polars expressions from SQL fragments. Class SQLContext Run SQL queries against DataFrame/LazyFrame. SQLContext.__init__(frames=None, register_globals=False, eager=False, **named_frames): initialize SQLContext. Parameters: frames, register_globals, eager, named_frames. SQLContext.__enter__(): track registered tables. SQLContext.__exit__(exc_type, exc_val, exc_tb): unregister tables on exit. SQLContext.execute(query, *, eager): execute SQL query. SQLContext.execute_global(query, *, eager): execute SQL, auto-register globals. SQLContext.register(name, frame): register frame as table. SQLContext.register_globals([n, all_compatible]): register global frames. SQLContext.register_many([frames]): register frames as tables. SQLContext.tables(): return table names. SQLContext.unregister(names): unregister tables. Config.set_tbl_rows(n: int | None) -> type[Config] Set max rows to draw in table. If n < 0, display all rows. PolarsError: Base class for all Polars errors. Config.set_verbose(active: bool = True) -> Config: Enables verbose logging. Config.load(cfg: str) -> Config Load Config options from a JSON string. Config.save(*, if_set: bool = False) -> str Save the current Config options as a JSON string. polars.Config.save_to_file(file: Path | str) -> None: Save Config options to JSON file. Parameters: file (Path | str). polars.api.register_expr_namespace(name: str) -> Callable[[type[NS]], type[NS]]. Decorator for registering custom functionality with a Polars Expr. SQLContext.register(name: str, frame: CompatibleFrameType | None) -> Self: Register a DataFrame as a SQL table. NoDataError: Exception raised when an operation cannot be performed on an empty data structure. SQLContext.register_many(frames: Mapping[str, FrameType] | None = None, **named_frames: FrameType | None) Returns Self. Register multiple frames as tables. Parameters: frames: {name:frame}, **named_frames: named frames kwargs. See also register, unregister. SQLContext.tables() -> list[str]. Return list of registered table names. Equivalent to SHOW TABLES SQL. ColumnNotFoundError: Exception raised when a specified column is not found. Example: df = pl.DataFrame({"a": [1, 2, 3]}); df.select("b") # raises ColumnNotFoundError exception polars.exceptions.ModuleUpgradeRequiredError Exception raised when a module is installed but needs to be upgraded. exception polars.exceptions.ShapeError: Raised when operations on data structures have incompatible shapes. polars.testing.assert_frame_not_equal(left: DataFrame | LazyFrame, right: DataFrame | LazyFrame, *, check_row_order: bool = True, check_column_order: bool = True, check_dtypes: bool = True, check_exact: bool = False, rtol: float = 1e-05, atol: float = 1e-08, categorical_as_str: bool = False) -> None: Asserts that two frames are not equal. polars.testing.parametric.dtypes(*, allowed_dtypes: Collection[PolarsDataType] = None, excluded_dtypes: Sequence[PolarsDataType] = None, allow_time_zones: bool = True, nesting_level: int = 3) -> SearchStrategy[DataType]. Create strategy for generating Polars DataType objects. Warning: unstable. polars.testing.parametric.lists(inner_dtype: DataType, *, select_from: Sequence[Any] | None = None, min_size: int = 0, max_size: int | None = None, unique: bool = False, **kwargs: Any) -> SearchStrategy[list[Any]]: Create strategy for generating lists of given data type. Parameters: inner_dtype, select_from, min_size, max_size, unique, kwargs. polars.testing.parametric.set_profile(profile: Literal['fast', 'balanced', 'expensive'] | int) -> None: Set env var POLARS_HYPOTHESIS_PROFILE. Parameters: profile: 'fast', 'balanced', 'expensive' or int (iterations). Unstable. Example: from polars.testing.parametric import set_profile; set_profile("balanced") polars.threadpool_size() -> int: Return the number of threads in the Polars thread pool. Deprecated: renamed to thread_pool_size(). Expr.rolling(index_column: str, period: str | timedelta, offset: str | timedelta | None = None, closed: ClosedInterval = 'right') -> Expr: Create rolling groups based on temporal or integer column. index_column: column for time window grouping (sorted ascending), period: window length, offset: window offset (default -period), closed: interval closure ('right', 'left', 'both', 'none'). Time string language supported (e.g., "1d", "3mo"). Example: pl.sum('a').rolling(index_column='dt', period='2d') Series.list.all() -> Series: Evaluates if all boolean values in a list are true. Returns: Series of boolean type. LazyFrame.mean() -> LazyFrame: Aggregate columns to their mean value. Example: lf.mean().collect() Expr.list.set_union(other: IntoExpr) -> Expr: Compute SET UNION of list elements. Expr.rolling_sum_by(by: IntoExpr, window_size: timedelta | str, *, min_samples: int = 1, closed: ClosedInterval = 'right') -> Expr. Rolling sum based on column 'by'. 'window_size' timedelta or str. 'closed' interval closure. Unstable. Series.cum_sum(*, reverse: bool = False) -> Series: Get cumulative sum. reverse: reverse operation. Int8, UInt8, Int16, UInt16 cast to Int64 before summing. Expr.cosh() -> Expr: Compute element-wise hyperbolic cosine. Returns Expr of data type Float64. DataFrame.map_rows(function: Callable[[tuple[Any, ...]], Any], return_dtype: PolarsDataType|None=None, inference_size: int=256) -> DataFrame: Apply a UDF row-wise. Slower than native expressions. Use native expressions when possible. exception polars.exceptions.PanicException. Exception raised on unexpected Rust library panic. Series.sign() -> Series: Compute element-wise sign (-1 if x < 0, 1 if x > 0, x otherwise). Preserves nulls and input dtype. Expr.min() -> Expr: Get minimum value. Expr.pct_change(n: int | IntoExprColumn = 1) -> Expr: Computes percentage change between values. n: periods to shift. Series.dt.month_end() -> Series: Roll forward to the last day of the month (Date or Datetime). Catalog.list_catalogs() -> list[CatalogInfo]: List available catalogs (unstable). Series.interpolate(method: InterpolationMethod = 'linear') -> Series: Fill null values using interpolation. method: 'linear', 'nearest'. LazyFrame.reverse() -> LazyFrame: Reverse the DataFrame. polars.coalesce(exprs: IntoExpr | Iterable[IntoExpr], *more_exprs: IntoExpr) -> Expr. Folds columns left to right, keeping first non-null value. Expr.diff(n: int = 1, null_behavior: NullBehavior = 'ignore') -> Expr. Calculate first discrete difference between shifted items. Expr.dt.cast_time_unit(time_unit: TimeUnit) -> Expr: Casts the underlying data to another time unit ('ns', 'us', 'ms'). May lose precision. time_unit: Time unit for the Datetime expression. DataFrame.row(index: int | None = None, *, by_predicate: Expr | None = None, named: bool = False) -> tuple[Any, ...] | dict[str, Any]. Get values of a single row by index or predicate. Returns tuple or dict (named=True). DataFrame.with_row_index(name: str = 'index', offset: int = 0) -> DataFrame. Add row index as first column. Index starts at offset. polars.fold(acc: IntoExpr, function: Callable[[Series, Series], Series], exprs: Sequence[Expr | str] | Expr) -> Expr. Accumulate over columns horizontally/row wise with left fold. Expr.dt.total_minutes() -> Expr. Extract total minutes from Duration type. Returns Expr[Int64]. read_ipc_stream(source: str | Path | IO[bytes] | bytes, *, columns: list[int] | list[str] | None = None, n_rows: int | None = None, use_pyarrow: bool = False, storage_options: dict[str, Any] | None = None, row_index_name: str | None = None, row_index_offset: int = 0, rechunk: bool = True) -> DataFrame: Read Arrow IPC stream. Supports files and file-like objects. n_rows only valid when use_pyarrow=False. DataFrame.iter_columns() -> Iterator[Series]. Returns iterator over columns of DataFrame. Series.kurtosis(*, fisher: bool = True, bias: bool = True) -> float | None. Compute kurtosis (Fisher/Pearson). Params: fisher, bias. Series.arr.shift(n: int | IntoExprColumn = 1) -> Series: Shift array values by n indices. Negative n shifts opposite direction. Expr.replace(old: IntoExpr | Sequence[Any] | Mapping[Any, Any], new: IntoExpr | Sequence[Any] | NoDefault = , *, default: IntoExpr | NoDefault = , return_dtype: PolarsDataType | None = None) -> Expr: Replace values. Accepts expressions, sequences, mappings. Deprecated: Use replace_strict(). Expr.str.slice(offset: int | IntoExprColumn, length: int | IntoExprColumn | None = None) -> Expr: Extract substring. Offset and length in characters (UTF8). Expr.pipe(function: Callable[Concatenate[Expr, P], T], *args: P.args, **kwargs: P.kwargs) -> T: Apply UDFs sequentially. Expr.is_first_distinct() -> Expr: Boolean mask for first occurrence of each distinct value. map_groups(exprs: Sequence[str | Expr], function: Callable[[Sequence[Series]], Series | Any], return_dtype: PolarsDataType | None = None, *, returns_scalar: bool = True) -> Expr: Apply UDF in GroupBy context. Warning: Slower than native expressions. Config.set_tbl_width_chars(width: int | None) -> type[Config]: Set max table width in characters. polars.arg_where(condition: Expr | Series, *, eager: bool = False) -> Expr | Series. Return indices where condition is True. eager=True returns Series. DataFrame.write_ipc_stream(file: str | Path | IO[bytes] | None, *, compression: IpcCompression = 'uncompressed', compat_level: CompatLevel | None = None) -> BytesIO | None. Write to Arrow IPC stream. file=None returns BytesIO. compression options: uncompressed, lz4, zstd. DataFrame.sort(by: IntoExpr | Iterable[IntoExpr], *more_by: IntoExpr, descending: bool | Sequence[bool] = False, nulls_last: bool | Sequence[bool] = False, multithreaded: bool = True, maintain_order: bool = False) -> DataFrame. Sort DataFrame by columns 'by', '*more_by'. Options: descending, nulls_last, multithreaded, maintain_order. Expr.mul(other: Any) -> Expr. Multiplication operator (expr * other). Series.dt.to_string(format: str | None = None) -> Series. Convert Date/Time/Datetime to String with format. Default ISO. Datetime: ISO, ISO:strict. Duration: ISO, polars. Expr.dt.hour() -> Expr: Extracts hour (0-23) from Datetime. Returns Int8 Expr. LazyGroupBy.first() -> LazyFrame: Aggregates first values per group. Series.dt.ordinal_day() -> Series: Extracts ordinal day (1-366) from Date/Datetime. Returns Int16 Series. Series.cat.starts_with(prefix: str) -> Series: Checks if string values start with prefix. Returns Boolean Series. Expr.where(predicate: Expr) -> Expr: Filters a column based on predicate. Deprecated; use filter(). Series.dt.replace_time_zone(time_zone: str|None, ambiguous: str='raise', non_existent: str='raise') -> Series: Replaces timezone in Datetime Series. Modifies timestamp; ignores original timezone. Options for ambiguous/non_existent times. LazyFrame.approx_n_unique() -> LazyFrame: Approximate unique count using HyperLogLog++. Deprecated; use select(pl.all().approx_n_unique()). Series.str.find_many(patterns: Expr, ascii_case_insensitive: bool=False, overlapping: bool=False) -> Series: Finds all matches using Aho-Corasick. Returns List<UInt32> Series. Config.set_tbl_hide_column_names(active: bool|None=True) -> Config: Hides/shows column names in table output. Expr.sqrt() -> Expr: Computes the square root of elements. Series.dt.truncate(every) -> Series: Divides date/datetime range into buckets using specified interval. Returns Series of Date or Datetime. every: string (e.g., '1h', '1d', '3d12h'). Series.str.strip_chars_start(characters) -> Series: Removes leading characters. characters: string or None (removes whitespace). Series.is_empty() -> bool Check if Series is empty. s = pl.Series("a", [], dtype=pl.Float32); s.is_empty() LazyGroupBy.tail(n=5) -> LazyFrame: Gets the last n rows of each group. Expr.list.concat(other) -> Expr: Concatenates arrays in a List Series. other: columns to concatenate. Catalog.list_schemas(catalog_name) -> list[SchemaInfo]: Lists schemas in a catalog. catalog_name: catalog name. Unstable functionality. Expr.str.json_path_match(json_path) -> Expr: Extracts first JSONPath match. json_path: JSONPath query string. Returns String Expr; null if no match or null input. Expr.arcsin() -> Expr: Computes element-wise inverse sine. Returns Float64 Expr. Series.list.count_matches(element: IntoExpr) -> Series Count element occurrences in list. s = pl.Series("a", [[0], [1]]); s.list.count_matches(1) LazyFrame.serialize(file=None,format='binary') -> bytes|str|None: Serializes LazyFrame's logical plan. file: file path or None (returns string). format: 'binary' or 'json'. Serialization is not version-stable. Expr.approx_n_unique() -> Expr. Approximate count of unique values using HyperLogLog++ algorithm. DataFrame.partition_by(by: ColumnNameOrSelector | Sequence[ColumnNameOrSelector], *more_by: ColumnNameOrSelector, maintain_order: bool = True, include_key: bool = True, as_dict: bool = False) -> list[DataFrame] | dict[tuple[object, ...], DataFrame]. Group DataFrame by columns, return list of DataFrames or dict of DataFrames. Options for maintain_order, include_key, as_dict. Expr.meta.tree_format(*, return_as_string: bool = False) -> str | None. Format expression as tree, return string if return_as_string=True. LazyFrame.last() -> LazyFrame. Get last row of DataFrame. Series.any(*, ignore_nulls: bool = True) -> bool | None. Return if any value is True. Boolean dtype. ignore_nulls=False for Kleene logic. polars.testing.assert_series_not_equal(left: Series, right: Series, *, check_dtypes: bool = True, check_names: bool = True, check_order: bool = True, check_exact: bool = False, rtol: float = 1e-05, atol: float = 1e-08, categorical_as_str: bool = False) -> None. Assert Series not equal for unit tests. Checks dtypes, names, order, exactness, tolerances, categorical_as_str. LazyFrame.lazy() -> LazyFrame Returns itself. lf = pl.LazyFrame({'a': [None, 2, 3, 4]}); lf.lazy() Expr.str.len_bytes() -> Expr Returns string length in bytes (UInt32). Faster than len_chars(). df = pl.DataFrame({'a': ["Caf", "345"]}); df.with_columns(pl.col("a").str.len_bytes().alias("n_bytes")) polars.testing.parametric.create_list_strategy(inner_dtype, *, select_from=None, size=None, min_size=0, max_size=None, unique=False) -> SearchStrategy[list[Any]] Create strategy for Polars List data. Deprecated: use lists(). lst = create_list_strategy(inner_dtype=pl.Int32); lst.example() Series.fill_null(value=None, strategy=None, limit=None) -> Series Fill null values. Strategies: forward, backward, min, max, mean, zero, one. s = pl.Series("a", [1, 2, 3, None]); s.fill_null(strategy="forward") Expr.dt.strftime(format: str) -> Expr Convert Date/Time/Datetime to String with format. Alias for to_string(). Format: chrono strftime spec. df = pl.DataFrame({'datetime': [datetime(2020, 3, 1)]}); df.with_columns(pl.col("datetime").dt.strftime("%Y/%m/%d").alias("datetime_string")) Series.list.set_difference(other: Series) -> Series Compute set difference between lists. a = pl.Series([[1, 2, 3]]); b = pl.Series([[2, 3, 4]]); a.list.set_difference(b) class polars.datatypes.Int128 128-bit signed integer type (unstable). Methods: __init__, base_type, from_python, is_, is_decimal, is_float, is_integer, is_nested, is_numeric, is_signed_integer, is_temporal, is_unsigned_integer, max, min, to_python Expr.name.suffix_fields(suffix: str) -> Expr Add suffix to struct field names. struct columns only. df = pl.DataFrame({'x': {'a': 1}}); df.select(pl.col("x").name.suffix_fields("_suffix")) Series.struct.unnest() -> DataFrame: Converts a struct Series to a DataFrame with columns for each field. CredentialProviderGCP(*, scopes: Any | None = None, request: Any | None = None, quota_project_id: Any | None = None, default_scopes: Any | None = None) Initializes GCP credential provider using google.auth.default(). Parameters are passed to `google.auth.default()` Requires google-auth package. Series.tail(n: int = 10) -> Series: Gets the last n elements. n: Number of elements. Negative n returns all elements except the first abs(n). Series.to_frame(name: str | None = None) -> DataFrame: Casts the Series to a DataFrame. name: Optionally rename the Series column. Series.interpolate_by(by: IntoExpr) -> Series: Fill null values using interpolation based on another column. Expr.drop_nulls() -> Expr: Drop all null values, preserving order. Note: null != NaN, use drop_nans() for NaN. polars.mean(*columns: str) -> Expr: Get the mean value of columns. Syntactic sugar for pl.col(columns).mean(). Expr.extend_constant(value: IntoExpr, n: int | IntoExprColumn) -> Expr: Extend Series with n copies of value. Fast method. Expr.inspect(fmt: str = '{}') -> Expr: Print the value this expression evaluates to and pass on the value. polars.Expr.list.slice(offset: int|str|Expr, length: int|str|Expr|None=None) -> Expr: Slice sublists. offset: Start index. length: Slice length. None=to end. polars.collect_all_async(lazy_frames: Iterable[LazyFrame], gevent: bool=False, type_coercion: bool=True, ...) -> Awaitable[list[DataFrame]] | _GeventDataFrameResult[list[DataFrame]]: Asynchronously collect multiple LazyFrames. Unstable. DataFrame.shift(n: int=1, fill_value: IntoExpr|None=None) -> DataFrame: Shift values by n indices. n>0: forward, n<0: backward. fill_value fills nulls. DataFrame.unpivot(on: ColumnNameOrSelector|Sequence[ColumnNameOrSelector]|None=None, index: ColumnNameOrSelector|Sequence[ColumnNameOrSelector]|None=None, variable_name: str|None=None, value_name: str|None=None) -> DataFrame: Unpivot DataFrame from wide to long format. Similar to pandas.DataFrame.melt. Expr.sort(descending: bool=False, nulls_last: bool=False) -> Expr: Sort column or groups. descending: Sort descending. nulls_last: Nulls last. polars.repeat(value: IntoExpr|None, n: int|Expr, dtype: PolarsDataType|None=None, eager: bool=False) -> Expr|Series: Repeat value n times. eager=True returns Series, otherwise Expr. Expr.shift(n: int|IntoExprColumn=1, fill_value: IntoExpr|None=None) -> Expr: Shift values by n indices. Similar to SQL LAG/LEAD. fill_value for nulls. Series.shrink_dtype() -> Series: Shrinks numeric columns to minimal datatype to reduce memory pressure. s = pl.Series("a", [1,2,3,4,5,6]); s.shrink_dtype() Series.str.strip_chars_end(characters: IntoExpr = None) -> Series: Removes trailing characters. characters: characters to remove, None for whitespace. s = pl.Series([" hello ", "world\t"]); s.str.strip_chars_end() Series.cum_prod(reverse: bool = False) -> Series: Returns cumulative product. reverse: reverse operation. Int8, UInt8, Int16, UInt16 cast to Int64 before summing. s = pl.Series("a", [1, 2, 3]); s.cum_prod() DataFrame.write_json(file: IOBase | str | Path | None = None) -> str | None: Serialize to JSON. file: File path or file-like object. If None, returns string. df = pl.DataFrame({"foo": [1, 2, 3],"bar": [6, 7, 8]}); df.write_json() DataFrame.min() -> DataFrame. Aggregate columns to minimum value. Expr.ne(other: Any) -> Expr: Inequality operator (expr != other). other: Literal or expression. df.with_columns(pl.col("x").ne(pl.col("y")).alias("x != y")) Series.bin.encode(encoding: TransferEncoding) -> Series: Encode values using encoding. encoding: 'hex', 'base64'. Returns String Series. s = pl.Series("colors", [b"\x00\x00\x00", b"\xff\xff\x00", b"\x00\x00\xff"]); s.bin.encode("hex") DataFrame.__getitem__(key) -> DataFrame | Series | Any: Get part of DataFrame. key: row/column selector. Returns DataFrame, Series, or scalar. df = pl.DataFrame({"a": [1, 2, 3], "d": [4, 5, 6], "c": [1, 3, 2], "b": [7, 8, 9]}); df[0]; df["a"]; df[0:2, "a"] Series.nan_max() -> int | float | date | datetime | timedelta | str: Get max value, propagate NaN values. s = pl.Series("a", [1, 3, 4]); s.nan_max(); s = pl.Series("a", [1.0, float("nan"), 4.0]); s.nan_max() Series.dt.cast_time_unit(time_unit: TimeUnit) -> Series: Cast underlying data to another time unit ('ns', 'us', 'ms'). May lose precision. Example: date.dt.cast_time_unit("ms") polars.n_unique(*columns: str) -> Expr: Count unique values in columns. Sugar for pl.col(columns).n_unique(). Example: df.select(pl.n_unique("a")) polars.int_ranges(start: int | Expr = 0, end: int | Expr | None = None, step: int | Expr = 1, dtype: PolarsIntegerType = Int64, eager: bool = False) -> Expr | Series: Generate range of integers for each row. start: inclusive, end: exclusive (defaults to start, start defaults to 0), step. dtype: integer type. eager: return Series if True. Example: pl.int_ranges("start", "end") LazyFrame.std(ddof: int = 1) -> LazyFrame: Aggregate columns to their standard deviation. ddof: Delta Degrees of Freedom (N - ddof). Example: lf.std().collect() polars.from_dicts(data: Iterable[dict[str, Any]], schema: SchemaDefinition | None = None, schema_overrides: SchemaDict | None = None, strict: bool = True, infer_schema_length: int | None = 100) -> DataFrame: Construct DataFrame from sequence of dictionaries (clones data). schema: dict, list of names, or list of (name, type). schema_overrides: override inferred types. strict: throw error if data doesn't match type (else cast or set to null). infer_schema_length: rows to scan for inference. Example: pl.from_dicts(data) Expr.str.strip_prefix(prefix: IntoExpr) -> Expr. Remove prefix from string once if found. prefix: The prefix to be removed. See also: strip_chars_start, strip_suffix. Example: df.with_columns(pl.col("a").str.strip_prefix("foo").alias("stripped")) polars.arg_sort_by(exprs: IntoExpr | Iterable[IntoExpr], *more_exprs: IntoExpr, descending: bool | Sequence[bool] = False, nulls_last: bool | Sequence[bool] = False, multithreaded: bool = True, maintain_order: bool = False) -> Expr. Return row indices that would sort the column(s). exprs: Column(s) to sort by. descending: Sort descending. nulls_last: Place nulls last. multithreaded: Use multiple threads. maintain_order: maintain order if elements are equal. See also: Expr.gather, Expr.rank. Example: df.select(pl.arg_sort_by("a")) Series.rolling_map(function: Callable[[Series], Any], window_size: int, weights: list[float] | None = None, *, min_samples: int | None = None, center: bool = False) -> Series. Compute custom rolling window function. Parameters: function: Custom aggregation function. window_size: window length. weights: optional window weights. min_samples: min non-null values. center: center labels. Warning: Slow, use specialized rolling functions if possible. Expr.count() -> Expr. Return number of non-null elements in column. Returns: Expr of type UInt32. Expr.arr.n_unique() -> Expr. Count unique values in every sub-array. LazyFrame.sink_parquet(path: str | Path, *, compression: str = 'zstd', compression_level: int | None = None, statistics: bool | str | dict[str, bool] = True, row_group_size: int | None = None, data_page_size: int | None = None, maintain_order: bool = True, type_coercion: bool = True, predicate_pushdown: bool = True, projection_pushdown: bool = True, simplify_expression: bool = True, slice_pushdown: bool = True, collapse_joins: bool = True, no_optimization: bool = False, storage_options: dict[str, Any] | None = None, credential_provider: CredentialProviderFunction | Literal['auto'] | None = 'auto', retries: int = 2) -> None. Evaluate query in streaming mode and write to Parquet file. Parameters: path: File path. compression: compression algorithm. compression_level: compression level. statistics: write statistics. row_group_size: row group size. data_page_size: data page size. maintain_order: maintain order. type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, collapse_joins, no_optimization: optimizations. storage_options: cloud storage options. credential_provider: credential provider function. retries: retries on cloud failure. Warning: Streaming mode unstable. Expr.str.zfill(length: int | IntoExprColumn) -> Expr. Pad start of string with zeros to given length. Parameters: length: Pad length. Notes: Intended for numeric strings. Use pad_start for non-ASCII. Series.drop_nulls() -> Series. Drop all null values, preserving order. See also: drop_nans. Notes: Null != NaN. Use drop_nans() for NaN. Expr.log1p() -> Expr[source]. Compute natural logarithm of each element plus one. More numerically stable for x close to zero than log(1 + x). Example: df.select(pl.col("a").log1p()). Expr.rolling_mean(window_size: int, weights: list[float] | None = None, *, min_samples: int | None = None, center: bool = False) -> Expr[source]. Apply rolling mean. Parameters: window_size, weights, min_samples, center. Example: df.with_columns(rolling_mean=pl.col("A").rolling_mean(window_size=2)). Expr.struct.with_fields(*exprs: IntoExpr | Iterable[IntoExpr], **named_exprs: IntoExpr) -> Expr[source]. Add or overwrite fields of struct. Parameters: *exprs, **named_exprs. Example: df.with_columns(pl.col("coords").struct.with_fields(pl.field("x").sqrt(), y_mul=pl.field("y") * pl.col("multiply"))). Expr.str.to_date(format: str | None = None, *, strict: bool = True, exact: bool = True, cache: bool = True) -> Expr[source]. Convert String column to Date. Parameters: format, strict, exact, cache. Example: s.str.to_date(). polars.testing.parametric.series(*, name: str | SearchStrategy[str] | None = None, dtype: PolarsDataType | None = None, min_size: int = 0, max_size: int = 5, strategy: SearchStrategy[Any] | None = None, allow_null: bool = True, allow_chunks: bool = True, allow_masked_out: bool = True, unique: bool = False, allowed_dtypes: Collection[PolarsDataType] | PolarsDataType | None = None, excluded_dtypes: Collection[PolarsDataType] | PolarsDataType | None = None, allow_time_zones: bool = True, **kwargs: Any) -> SearchStrategy[Series]. Hypothesis strategy for Polars Series in unit tests. Parameters: name, dtype, size, strategy, allow_null, allow_chunks, allow_masked_out, unique, allowed_dtypes, excluded_dtypes, allow_time_zones, **kwargs. Unstable API. Example: @given(s=series(min_size=3, max_size=5)) def test_series_len(s: pl.Series) -> None:. exception polars.exceptions.PolarsInefficientMapWarning: Warning for potentially slow map_* operation. polars.scan_csv(source: str | Path | IO[str] | IO[bytes] | bytes | list[str] | list[Path] | list[IO[str]] | list[IO[bytes]] | list[bytes], *, has_header: bool = True, separator: str = ',', comment_prefix: str | None = None, quote_char: str | None = '"', skip_rows: int = 0, skip_lines: int = 0, schema: SchemaDict | None = None, schema_overrides: SchemaDict | Sequence[PolarsDataType] | None = None, null_values: str | Sequence[str] | dict[str, str] | None = None, missing_utf8_is_empty_string: bool = False, ignore_errors: bool = False, cache: bool = True, with_column_names: Callable[[list[str]], list[str]] | None = None, infer_schema: bool = True, infer_schema_length: int | None = 100, n_rows: int | None = None, encoding: CsvEncoding = 'utf8', low_memory: bool = False, rechunk: bool = False, skip_rows_after_header: int = 0, row_index_name: str | None = None, row_index_offset: int = 0, try_parse_dates: bool = False, eol_char: str = '\n', new_columns: Sequence[str] | None = None, raise_if_empty: bool = True, truncate_ragged_lines: bool = False, decimal_comma: bool = False, glob: bool = True, storage_options: dict[str, Any] | None = None, credential_provider: CredentialProviderFunction | Literal['auto'] | None = 'auto', retries: int = 2, file_cache_ttl: int | None = None, include_file_paths: str | None = None) -> LazyFrame: Lazily read from CSV file(s). Parameters: source: Path(s), has_header: bool, separator: str, schema: SchemaDict, null_values: str/list[str]/dict[str, str], infer_schema: bool, n_rows: int, encoding: str, ignore_errors: bool. Returns: LazyFrame. Example: pl.scan_csv("my_long_file.csv").select(["a", "c"]).filter(pl.col("a") > 10).head(100) Series.dt.timestamp(time_unit: TimeUnit = 'us') -> Series: Return timestamp in given time unit. Parameters: time_unit: 'us', 'ns', or 'ms'. classmethod Expr.from_json(value: str) -> Expr: Read expression from JSON string. Deprecated: Use deserialize() with file-like input instead. Parameters: value: JSON encoded string. DataFrame.product() -> DataFrame: Aggregate columns to product values. Expr.limit(n: int | Expr = 10) -> Expr: Get first n rows (alias for head()). DataFrame.get_column(name: str, *, default: Any | NoDefault = <no_default>) -> Series | Any: Get column by name. Raises ColumnNotFoundError if no default. Series.is_first_distinct() -> Series: Boolean mask of first distinct value occurrence. Series.ewm_mean(*, com: float | None = None, span: float | None = None, half_life: float | None = None, alpha: float | None = None, adjust: bool = True, min_samples: int = 1, ignore_nulls: bool = False) -> Series: Compute exponentially-weighted moving average. polars.implode(*columns: str) -> Expr: Aggregate column values into a list. LazyFrame.show_graph(*, optimized: bool = True, show: bool = True, output_path: str | Path | None = None, raw_output: bool = False, figsize: tuple[float, float] = (16.0, 12.0), type_coercion: bool = True, predicate_pushdown: bool = True, projection_pushdown: bool = True, simplify_expression: bool = True, slice_pushdown: bool = True, comm_subplan_elim: bool = True, comm_subexpr_elim: bool = True, cluster_with_columns: bool = True, collapse_joins: bool = True, streaming: bool = False) -> str | None: Show plot of query plan. Requires Graphviz. Parameters: optimized, show, output_path, raw_output, figsize, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, collapse_joins, streaming. class polars.datatypes.Int16: 16-bit signed integer type. Methods: __init__(*args, **kwargs), base_type() -> DataType: Return base type class, from_python(py_type) -> DataType: From Python type to Polars type, is_(other) -> bool: Check if same DataType, is_decimal() -> bool: Check decimal type, is_float() -> bool: Check float type, is_integer() -> bool: Check integer type, is_nested() -> bool: Check nested type, is_numeric() -> bool: Check numeric type, is_signed_integer() -> bool: Check signed integer type, is_temporal() -> bool: Check temporal type, is_unsigned_integer() -> bool: Check unsigned integer type, max() -> Expr: Max value literal, min() -> Expr: Min value literal, to_python() -> type: To Python type. DataFrame.var(ddof: int = 1) -> DataFrame: Aggregate columns to variance. Parameters: ddof: Delta Degrees of Freedom. Series.hist(bins: list[float] | None = None, *, bin_count: int | None = None, include_category: bool = True, include_breakpoint: bool = True) -> DataFrame: Bin values into buckets and count occurrences. Unstable. Parameters: bins, bin_count, include_category, include_breakpoint. Expr.bin.encode(encoding) -> Expr: Encode binary value. Params: encoding ('hex', 'base64'). Returns String Expr. DataFrame.to_init_repr(n=1000) -> str: Convert DataFrame to instantiable string representation. Params: n (rows to use). Series.dt.replace(*, year=None, month=None, day=None, hour=None, minute=None, second=None, microsecond=None, ambiguous='raise') -> Series: Replace datetime components (year, month, day, hour, minute, second, microsecond). Params: year, month, day, hour, minute, second, microsecond, ambiguous ('raise', 'earliest', 'latest', 'null'). Expr.dt.total_seconds() -> Expr: Extract total seconds from Duration. Returns Int64 Expr. Config.set_tbl_dataframe_shape_below(active=True) -> type[Config]: Set config to print DataFrame shape below tables. Params: active (bool). Float32: 32-bit floating point type. Methods: base_type(), from_python(py_type), is_(other), is_decimal(), is_float(), is_integer(), is_nested(), is_numeric(), is_signed_integer(), is_temporal(), is_unsigned_integer(), max(), min(), to_python(). Expr.arcsinh() -> Expr: Compute element-wise inverse hyperbolic sine. Returns: Expr of type Float64. Example: df.select(pl.col("a").arcsinh()) Series.reinterpret(*, signed: bool = True) -> Series: Reinterpret underlying bits as signed/unsigned integer (64bit integers only). Parameters: signed (bool) - If True, reinterpret as pl.Int64, else pl.UInt64. Returns: Series. Example: s.reinterpret(signed=False) Expr.ewm_mean_by(by: str | IntoExpr, *, half_life: str | timedelta) -> Expr: Compute time-based exponentially weighted moving average. Parameters: by (str|Expr) - Times to calculate average by (DateTime, Date, UInt64, UInt32, Int64, or Int32). half_life (str|timedelta) - Unit over which observation decays to half its value. Returns: Expr (Float32 if input is Float32, else Float64). Example: df.with_columns(result=pl.col("values").ewm_mean_by("times", half_life="4d")) polars.read_ndjson(source: str | Path | list[str] | list[Path] | IOBase | bytes, *, schema: SchemaDefinition | None = None, schema_overrides: SchemaDefinition | None = None, infer_schema_length: int | None = 100, batch_size: int | None = 1024, n_rows: int | None = None, low_memory: bool = False, rechunk: bool = False, row_index_name: str | None = None, row_index_offset: int = 0, ignore_errors: bool = False, storage_options: dict[str, Any] | None = None, credential_provider: CredentialProviderFunction | Literal['auto'] | None = 'auto', retries: int = 2, file_cache_ttl: int | None = None, include_file_paths: str | None = None,) -> DataFrame: Read DataFrame from newline delimited JSON file. Parameters: source (str|Path|...); schema (SchemaDefinition); schema_overrides (dict); infer_schema_length (int); batch_size (int); n_rows (int); low_memory (bool); rechunk (bool); row_index_name (str); row_index_offset (int); ignore_errors (bool); storage_options (dict); credential_provider (function); retries (int); file_cache_ttl (int); include_file_paths(str). Example: pl.read_ndjson(StringIO(json_str)) Series.arg_max() -> int | None: Get index of maximal value. Expr.bitwise_and() -> Expr: Perform aggregation of bitwise ANDs. Example: df.select(pl.col("n").bitwise_and()) or df.group_by("grouper").agg(pl.col("n").bitwise_and()) GroupBy.len(name: str | None = None) -> DataFrame: Returns number of rows per group. name: Result column name (default: len) Series.str.extract_all(pattern: str | Series) -> Series: Extracts all regex matches as list. Returns Series of List(String). Null input returns Null. Expr.n_unique() -> Expr: Counts unique values. Null is considered unique. Expr.mode() -> Expr: Computes the most frequent value(s). DataFrame.is_unique() -> Series[bool]: Returns a boolean Series masking rows that are unique. df.filter(df.is_unique()) shows unique rows. Expr.gt(other: Any) -> Expr: Evaluates if expr > other. Example: df.with_columns(pl.col("x").gt(pl.col("y")).alias("x > y")). Series.rolling_max(window_size: int, weights: list[float] | None = None, *, min_samples: int | None = None, center: bool = False) -> Series: Applies rolling max. window_size: window length. weights: optional weights. min_samples: min non-null values (default window_size). center: center labels. LazyFrame.min() -> LazyFrame: Aggregate columns to their minimum value. Expr.str.extract(pattern: IntoExprColumn, group_index: int = 1) -> Expr: Extracts capture group from regex pattern. pattern: regex. group_index: capture group index (1 is first). Returns: String Expr, null if no capture. LazyFrame.sql(query: str, *, table_name: str = 'self') -> LazyFrame: Executes SQL query against LazyFrame. query: SQL query. table_name: name for the LazyFrame in SQL context (default 'self'). Series.rolling_min(window_size: int, weights: list[float] | None = None, *, min_samples: int | None = None, center: bool = False) -> Series: Applies rolling min. window_size: window length. weights: optional weights. min_samples: min non-null values (default window_size). center: center labels. polars.Series.str.ends_with(suffix: str | Expr | None) -> Series: Checks if strings end with a substring. polars.Series.str.to_lowercase() -> Series: Converts strings to lowercase. polars.read_json(source: str | Path | IOBase | bytes, *, schema: SchemaDefinition | None = None, schema_overrides: SchemaDefinition | None = None, infer_schema_length: int | None = 100) -> DataFrame: Reads JSON file into DataFrame. Supports schema definition and overrides. polars.Config.restore_defaults() -> type[Config]: Resets Polars Config to defaults. polars.Expr.cum_max(*, reverse: bool = False) -> Expr: Computes cumulative max. Ignores nulls. `reverse` argument flips order. Expr.has_nulls() -> Expr: Check if expression contains null values. Example: df.select(pl.all().has_nulls()) Expr.dt.round(every: str | dt.timedelta | IntoExprColumn) -> Expr. Divide date/datetime range into buckets. Rounds values to bucket start/end. Returns Expr of Date or Datetime. polars.zeros(n: int | Expr, dtype: PolarsDataType = Float64, *, eager: bool = False) -> Expr | Series. Construct column of length n filled with zeros. Returns Expr or Series. Expr.is_nan() -> Expr. Returns boolean Series indicating which values are NaN. Returns Expr. Expr.str.to_decimal(*, inference_length: int = 100) -> Expr. Convert String column into a Decimal column. Infers precision and scale. Returns Expr. Series.list.explode() -> Series. Returns column with separate row for every list element. Returns Series with datatype of list elements. Expr.cat.get_categories() -> Expr: Get categories from categorical data. Returns string. Series.list.slice(offset:int|Expr, length:int|Expr|None=None) -> Series: Slice sublists. Negative indexing supported. Series.scatter(indices:Series|Iterable[int]|int|np.ndarray,values:Series|Iterable|PythonLiteral|None) -> Series: Set values at indices. Anti-pattern; use pl.when().then().otherwise() instead. Expr.meta.output_name(raise_if_undetermined:bool=True)->str|None: Get output column name. Raises ComputeError if undetermined and raise_if_undetermined is True. Series.dt.convert_time_zone(time_zone: str) -> Series: Convert Datetime Series to given time zone. For naive datetimes, assumes UTC conversion. class polars.datatypes.Duration(time_unit: TimeUnit = 'us'): Data type representing time duration. Underlying type is 64-bit signed integer. Methods: __init__(time_unit: TimeUnit = 'us'), base_type(), from_python(py_type), is_(other), is_decimal(), is_float(), is_integer(), is_nested(), is_numeric(), is_signed_integer(), is_temporal(), is_unsigned_integer(), to_python(). Attributes: time_unit LazyFrame.with_row_index(name: str = 'index', offset: int = 0) -> LazyFrame: Add row index as first column. name: index column name, offset: start offset. Warning: Negative performance impact. Example: lf.with_row_index().collect() Expr.or_(*others: Any) -> Expr: Method equivalent of bitwise "or" operator (expr | other | ....). others: one or more expressions. Example: (pl.col('x') == pl.col('y')).or_(pl.col('y') == pl.col('z')) Expr.arr.arg_min() -> Expr: Retrieve index of minimal value in every sub-array. Returns Expr of UInt32 or UInt64. Example: df.with_columns(arg_min=pl.col('a').arr.arg_min()) SQLContext.register_globals(n: int | None, *, all_compatible: bool) -> Self: Register frames from globals scope as tables. Params: n, all_compatible. Returns: Self. DataFrame.n_unique(subset: str | Expr | Sequence[str | Expr] | None = None) -> int: Return number of unique rows or subsets. Params: subset. Returns: int. Expr.top_k(k: int | IntoExprColumn = 5) -> Expr: Return k largest elements. Params: k. Returns: Expr. Note: Unsorted output. polars.concat_str(exprs: IntoExpr | Iterable[IntoExpr], *more_exprs: IntoExpr, separator: str = '', ignore_nulls: bool = False) -> Expr: Concatenate columns to string. Params: exprs, more_exprs, separator, ignore_nulls. Returns: Expr. Note: Linear time. LazyFrame.sort(by: IntoExpr | Iterable[IntoExpr], *more_by: IntoExpr, descending: bool | Sequence[bool] = False, nulls_last: bool | Sequence[bool] = False, maintain_order: bool = False, multithreaded: bool = True) -> LazyFrame: Sort LazyFrame by columns. Params: by, more_by, descending, nulls_last, maintain_order, multithreaded. Returns: LazyFrame. Series.to_pandas(*, use_pyarrow_extension_array: bool = False, **kwargs: Any) -> pd.Series. Converts Series to pandas Series. use_pyarrow_extension_array: Use PyArrow-backed extension array for zero copy, preserves nulls. Expr.str.join(delimiter: str = '', *, ignore_nulls: bool = True) -> Expr. Concatenates string values to single string. delimiter: string to insert. ignore_nulls: ignore null values (default), if False output is null if any nulls. Expr.list.diff(n:int=1, null_behavior:'ignore'|'drop'='ignore') -> Expr. Calculate discrete difference between shifted sublist items. Parameters: n (shift slots); null_behavior ('ignore', 'drop'). Returns: Expr. Expr.sample(n: int | IntoExprColumn | None = None, *, fraction: float | IntoExprColumn | None = None, with_replacement: bool = False, shuffle: bool = False, seed: int | None = None) -> Expr. Samples from expression. n: Number of items. fraction: Fraction of items. with_replacement: Allow sampling more than once. shuffle: Shuffle order. seed: Random seed. polars.any_horizontal(*exprs: IntoExpr | Iterable[IntoExpr]) -> Expr. Compute bitwise OR horizontally across columns. Kleene logic for nulls. Expr.pow(exponent: IntoExprColumn | int | float) -> Expr. Exponentiation operator. Result dtype depends on exponent type. Series.arr.to_list() -> Series. Convert Array column to List column. Returns List Series. LazyFrame.with_columns_seq(*exprs: IntoExpr | Iterable[IntoExpr], **named_exprs: IntoExpr) -> LazyFrame. Add columns to LazyFrame sequentially. Replaces existing columns. For cheap expressions. Expr.list.gather_every(n: int | IntoExprColumn, offset: int | IntoExprColumn = 0) -> Expr. Take every n-th value from offset in sublists. Series.cat.uses_lexical_ordering() -> bool. Indicate if Series uses lexical ordering. Unstable. DataFrame.transpose(*, include_header: bool = False, header_name: str = 'column', column_names: str | Iterable[str] | None = None) -> DataFrame. Transpose DataFrame over diagonal. Expensive. Options for header and column names. Series.str.to_integer(base: int = 10, strict: bool = True) -> Series Convert String column to Int64. base: radix; strict: raise error or return Null on failure. polars.var(column: str, ddof: int = 1) -> Expr Get variance. column: column name; ddof: delta degrees of freedom. polars.read_avro(source: str | Path | IO[bytes] | bytes, columns: list[int] | list[str] | None = None, n_rows: int | None = None) -> DataFrame Read Apache Avro data. source: file path or object; columns: columns to select; n_rows: max rows to read. polars.thread_pool_size() -> int Return Polars thread pool size. Can be overridden via environment variable POLARS_MAX_THREADS. Expr.dt.offset_by(by:str|Expr) -> Expr. Offset date by relative time offset (months, leap years considered). Parameters: by (offset str '1ns', '1d', '1mo', '1y' etc or Expr). Returns: Expr (Date or Datetime). Series.str.extract(pattern: IntoExprColumn, group_index: int = 1) -> Series Extract capture group from regex pattern. pattern: regex; group_index: capture group index. DataFrame.vstack(other: DataFrame, in_place: bool = False) -> DataFrame Vertically stack DataFrames. other: DataFrame to stack; in_place: modify in place. Series.str.pad_start(length: int, fill_char: str = ' ') -> Series Pad string start to length. length: target length; fill_char: padding character. Series.dt.millennium() -> Series Extract millennium from Date/Datetime. Returns millennium as Int32. Expr.ewm_var(com:float=None, span:float=None, half_life:float=None, alpha:float=None, adjust:bool=True, bias:bool=False, min_samples:int=1, ignore_nulls:bool=False) -> Expr. Compute exponentially-weighted moving variance. Parameters: com, span, half_life, alpha (decay specifiers); adjust (adjust for imbalance); bias (unbiased estimate); min_samples; ignore_nulls. Returns: Expr. polars.tail(column:str, n:int=10) -> Expr. Get last n rows of column. Parameters: column (column name); n (rows to return). Returns: Expr. Series.item(index:int=None) -> Any. Return Series as scalar or element at index. Parameters: index (optional, defaults to 0). Returns: Any. If no index, shape must be (1,). Expr.list.gather(indices: Expr | Series | list[int] | list[list[int]], null_on_oob: bool = False) -> Expr: Take sublists by indices. indices: Indices to return per sublist. null_on_oob: True -> set out of bounds index as null, False -> raise error. polars.datetime_range(start: datetime | date | IntoExprColumn, end: datetime | date | IntoExprColumn, interval: str | timedelta = '1d', closed: ClosedInterval = 'both', time_unit: TimeUnit | None = None, time_zone: str | None = None, eager: bool = False) -> Series | Expr: Generate datetime range. start: Lower bound. end: Upper bound. interval: timedelta or duration string (eg "1ns", "1d"). closed: {'both', 'left', 'right', 'none'}. time_unit: {'ns', 'us', 'ms'}. time_zone: Time zone. eager: Evaluate immediately. Returns: Expr or Series of Datetime. pl.datetime_range(datetime(2022, 1, 1), datetime(2022, 3, 1), "1mo", eager=True) Config.set_fmt_str_lengths(n: int | None) -> type[Config]: Set number of characters to display for string values. n: Number of characters. with pl.Config(fmt_str_lengths=50): print(df) class Struct(fields: Sequence[Field] | SchemaDict): Struct composite type. fields: Sequence of Field objects or dict of column names to data types. dtype = pl.Struct({"a": pl.Int8, "b": pl.List(pl.String)}) Methods: base_type(), from_python(py_type), is_(other), is_decimal(), is_float(), is_integer(), is_nested(), is_numeric(), is_signed_integer(), is_temporal(), is_unsigned_integer(), to_python(), to_schema() Attributes: fields Series.append(other: Series) -> Self: Append a Series to this one in-place. Returns series for convenience. Resulting series will consist of multiple chunks. See also: extend. a = pl.Series("a", [1, 2, 3]); b = pl.Series("b", [4, 5]); a.append(b) Expr.str.find(pattern: str | Expr, literal: bool = False, strict: bool = True) -> Expr: Return the bytes offset of first substring matching pattern, returns None if not found. pattern: regex pattern. literal: Treat pattern as literal string. strict: Raise error for invalid regex, else mask with null. df.select(pl.col("txt").str.find("a|e").alias("a|e (regex)")) Expr.dt.epoch(time_unit: EpochTimeUnit = 'us') -> Expr: Get time passed since Unix EPOCH. time_unit: {'ns', 'us', 'ms', 's', 'd'}. df.with_columns(pl.col("date").dt.epoch().alias("epoch_ns")) Expr.arr.reverse() -> Expr: Reverse the arrays in this column. df.with_columns(reverse=pl.col("a").arr.reverse()) polars.Expr.str.encode(encoding: TransferEncoding) -> Expr: Encode values using encoding (hex, base64). Returns Expr[String]. polars.Expr.truediv(other: Any) -> Expr: Float division (expr / other). other: numeric literal or expression. Follows IEEE-754 for zero division. polars.Config.set_tbl_cell_numeric_alignment(format: Literal['LEFT', 'CENTER', 'RIGHT'] | None) -> type[Config]: Set numeric column alignment (LEFT, CENTER, RIGHT). Raises KeyError if format is invalid. polars.DataFrame.melt(id_vars: ColumnNameOrSelector | Sequence[ColumnNameOrSelector] | None = None, value_vars: ColumnNameOrSelector | Sequence[ColumnNameOrSelector] | None = None, variable_name: str | None = None, value_name: str | None = None) -> DataFrame: Unpivot DataFrame (wide to long). id_vars: identifier columns. value_vars: measured variables. variable_name, value_name: new column names. Deprecated: use unpivot(). Expr.floordiv(other: Any) -> Expr: Integer division (//) of expression with other (numeric literal or Expr). See also: truediv. Example: df.with_columns(pl.col("x").floordiv(2).alias("x//2")). Note: Differs subtly from Python's floor division with floating-point numbers. polars.lazyframe.engine_config.GPUEngine(*, device: int | None = None, memory_resource: Any | None = None, raise_on_fail: bool = False, **kwargs: Any): Configures GPU execution engine. device: GPU to use. memory_resource: Memory resource for allocations. raise_on_fail: Raise error if GPU engine fails, otherwise fallback to CPU. Series.all(*, ignore_nulls: bool = True) -> bool | None: Returns True if all values are True. Works on Boolean Series. ignore_nulls: If True (default), ignore nulls. If False, use Kleene logic: returns None if any nulls and no False values. Example: pl.Series([True,True]).all() LazyFrame.inspect(fmt: str = '{}') -> LazyFrame: Inspect a node in the computation graph. Prints the value and passes it on. Series.zip_with(mask: Series, other: Series) -> Series. Take values from self or other based on mask. True mask: self, False mask: other. Series.entropy(base: float = 2.71828, normalize: bool = True) -> float | None. Computes entropy. Parameters: base (default e), normalize (if pk sums to 1). Series.eq_missing(other: Any) -> Series | Expr. Equality operator where None == None. Differs from eq where nulls are propagated. DataFrame.write_avro(file: str | Path | IO[bytes], compression: AvroCompression = 'uncompressed', name: str = '') -> None: Writes to Apache Avro file. file: File path or object. compression: Compression method. name: Schema name. class Categorical(ordering: CategoricalOrdering = 'physical'). Categorical encoding of strings. Parameters: ordering ('lexical', 'physical'). Methods: __init__([ordering]), base_type(), from_python(py_type), is_(other), is_decimal(), is_float(), is_integer(), is_nested(), is_numeric(), is_signed_integer(), is_temporal(), is_unsigned_integer(), to_python(). Attributes: ordering. LazyFrame.var(ddof: int = 1) -> LazyFrame. Aggregate columns to variance. Parameters: ddof (delta degrees of freedom, default 1). LazyFrame.slice(offset: int, length: int = None) -> LazyFrame. Get a slice of DataFrame. Parameters: offset (start index, negative indexing supported), length (slice length, None for all rows from offset). GroupBy.median() -> DataFrame: Returns the median per group. Example: df.group_by("d", maintain_order=True).median() Expr.le(other: Any) -> Expr: Less than or equal comparison. Series.to_dummies(*, separator: str = '_', drop_first: bool = False) -> DataFrame: Get dummy/indicator variables. Params: separator (str), drop_first (bool). Example: s = pl.Series("a", [1, 2, 3]); s.to_dummies() DataFrame.gather_every(n: int, offset: int = 0) -> DataFrame: Take every nth row as new DataFrame. Params: n (int), offset (int). Example: df = pl.DataFrame({"a": [1, 2, 3, 4], "b": [5, 6, 7, 8]}); df.gather_every(2) Expr.list.var(ddof: int = 1) -> Expr: Compute variance of lists in array. Params: ddof (int, default 1). Example: df.with_columns(pl.col("values").list.var().alias("var")) Series.skew(*, bias: bool = True) -> float | None: Compute sample skewness of data set. Params: bias (bool, default True). Returns None if computation not possible. Example: s = pl.Series([1, 2, 2, 4, 5]); s.skew() DataFrame.item(row: int | None = None, column: int | str | None = None) -> Any: Return DataFrame as scalar or element at row/column. If no row/col, equivalent to df[0,0], shape (1,1) check. Example: df.item(1, 1) returns element at row 1, column 1. Expr.rolling_map(function: Callable[[Series], Any], window_size: int, weights: list[float]|None=None, min_samples: int|None=None, center: bool=False) -> Expr: Apply custom rolling window function. Warning: Unstable, slow. Expr.cum_sum(reverse: bool=False) -> Expr: Cumulative sum. Small integer types cast to Int64 to prevent overflow. Nulls excluded; use forward_fill for inclusion. Series.list.join(separator: IntoExprColumn, ignore_nulls: bool=True) -> Series: Join strings in sublist with separator. Errors if inner type != String. ignore_nulls=False propagates nulls. Expr.rolling_var(window_size: int, weights: list[float]|None=None, min_samples: int|None=None, center: bool=False, ddof: int=1) -> Expr: Rolling variance. Use rolling for multiple stats on same window. polars.reduce(function: Callable[[Series, Series], Series], exprs: Sequence[Expr|str]|Expr) -> Expr: Row-wise accumulation with left fold. Expr.str.extract_all(pattern: str | Expr) -> Expr Extract all regex pattern matches as list. Parameters: pattern (str|Expr). Returns: Expr[List(String)]. Example: df.select(pl.col("foo").str.extract_all(r"\\d+")) Series.is_last_distinct() -> Series Boolean mask for last occurrence of distinct values. Returns: Series[Boolean]. Example: Series([1, 1, 2]).is_last_distinct() -> Series([false, true, true]) Series.estimated_size(unit: SizeUnit = 'b') -> int | float Estimate total allocated size of Series. Parameters: unit (str 'b', 'kb', 'mb', 'gb', 'tb'). Returns: int|float. Sliced arrays keep allocated size. LazyGroupBy.n_unique() -> LazyFrame Count unique values per group. Returns: LazyFrame. Example: ldf.group_by("d").n_unique() DataFrame.replace_column(index: int, column: Series) -> DataFrame Replace column at index location. In place. Parameters: index (int), column (Series). Returns: DataFrame. In place operation. Expr.ceil() -> Expr. Rounds to nearest integer value. Works on float Series. Series.str.slice(offset: int | IntoExprColumn, length: int | IntoExprColumn | None = None) -> Series: Extract substring from each string. Parameters: offset (start index, negative indexing), length (slice length, None=end). Returns: Series of type String. Example: s.str.slice(-3) polars.std(column: str, ddof: int = 1) -> Expr: Get standard deviation. Parameters: column (column name), ddof (delta degrees of freedom, N-ddof, default 1). polars.nth(*indices: int | Sequence[int]) -> Expr: Get nth column(s) of context. polars.sum(*names: str) -> Expr: Sum all values. Syntactic sugar for col(name).sum(). LazyFrame.head(n: int = 5) -> LazyFrame: Get first n rows. Expr.dt.date() -> Expr: Extracts date from date(time). Applies to Date and Datetime columns. Returns: Date Expr. Series.list.to_struct(n_field_strategy: ListToStructWidthStrategy = 'first_non_null', fields: Callable[[int], str] | Sequence[str] | None = None) -> Series: Convert List Series to Struct Series. Parameters: n_field_strategy: 'first_non_null'|'max_width', fields: field names or function. DataFrame.get_column_index(name: str) -> int: Find column index by name. Parameters: name: column name. Returns: column index. Raises ColumnNotFoundError. DataOrientationWarning: Warning for inferred row orientation in DataFrame construction. Use orient='row' to avoid. Series.cumulative_eval(expr: Expr, *, min_samples: int = 1, parallel: bool = False) -> Series: Run expression over increasing sliding window. Parameters: expr: expression, min_samples, parallel. Warning: potentially slow O(n^2). Config.load_from_file(file: Path | str) -> Config: Load Config options from JSON file. Parameters: file: file path. DataFrame.cast(dtypes: Mapping[ColumnNameOrSelector|PolarsDataType, PolarsDataType|PythonDataType]|PolarsDataType, *, strict:bool=True) -> DataFrame: Cast DataFrame column(s). dtypes: column names/selector to dtypes or single dtype. strict: raise if invalid cast. LazyFrame.explode(columns: str|Expr|Sequence[str|Expr], *more_columns: str|Expr) -> LazyFrame: Explode DataFrame to long format. columns: column names/expressions to explode. more_columns: additional columns to explode. Config.set_decimal_separator(separator: str|None=None) -> type[Config]: Set the decimal separator character. separator: character to use, None reverts to default. Expr.arctanh() -> Expr: Compute element-wise inverse hyperbolic tangent. Returns Expr of Float64. Series.explode() -> Series: Explode a list Series into new rows. Returns Series with data type of list elements. See also Series.list.explode. Example: s = pl.Series("a", [[1, 2, 3], [4, 5, 6]]); s.explode() Config.set_auto_structify(active: bool | None = False) -> type[Config]: Allow multi-output expressions to be automatically turned into Structs. Example: with pl.Config(set_auto_structify=True): df.select(pl.all()) Expr.list.unique(*, maintain_order: bool = False) -> Expr: Get unique/distinct values in the list. maintain_order: Maintain order (slower). Example: df.with_columns(unique=pl.col("a").list.unique()) Series.ewm_var(*, com: float | None = None, span: float | None = None, half_life: float | None = None, alpha: float | None = None, adjust: bool = True, bias: bool = False, min_samples: int = 1, ignore_nulls: bool = False) -> Series: Compute exponentially-weighted moving variance. Parameters: com, span, half_life, alpha (specify decay). adjust: Divide by decaying adjustment factor. bias: Apply correction for unbiased estimate. min_samples: Minimum observations in window. ignore_nulls: Ignore missing values when calculating weights. Example: s = pl.Series("a", [1, 2, 3]); s.ewm_var(com=1, ignore_nulls=False) LazyFrame.fill_null(value: Any | Expr | None = None, strategy: FillNullStrategy | None = None, limit: int | None = None, *, matches_supertype: bool = True) -> LazyFrame: Fill null values. Parameters: value (fill value), strategy (None, 'forward', 'backward', 'min', 'max', 'mean', 'zero', 'one'), limit (consecutive nulls to fill), matches_supertype. See also: fill_nan. Example: lf.fill_null(99) Series.str.zfill(length: int | IntoExprColumn) -> Series: Pad start of string with zeros to length. Handles sign prefix. Parameters: length. See also: pad_start(). Note: Intended for numeric strings; for non-ASCII, use pad_start(). Example: s.cast(pl.String).str.zfill(4) polars.api.register_dataframe_namespace(name: str) -> Callable[[type[NS]], type[NS]]: Register custom DataFrame functionality. name: Access name. Expr.is_in(other: Expr | Collection[Any] | Series) -> Expr: Check if elements are in other Series. other: Series or sequence. GroupBy.agg(*aggs: IntoExpr | Iterable[IntoExpr], **named_aggs: IntoExpr) -> DataFrame: Compute aggregations for each group. aggs: Positional aggregations. named_aggs: Keyword aggregations. GroupBy.n_unique() -> DataFrame: Count unique values per group. Expr.all(*, ignore_nulls: bool = True) -> Expr: Check if all values are True. Works on Boolean columns only. ignore_nulls: Ignore nulls (default: True). If False, uses Kleene logic. Expr.filter(*predicates: IntoExprColumn | Iterable[IntoExprColumn], **constraints: Any) -> Expr: Filter expression based on predicate expressions. Discards nulls. Use LazyFrame.filter to filter on DataFrame level. Parameters: predicates: Expression(s) that evaluates to a boolean Series. constraints: Column filters; name = value to filter columns by the supplied value. Example: df.group_by("group_col").agg(lt=pl.col("b").filter(pl.col("b") < 2).sum()) classmethod SQLContext.execute_global(query: str, *, eager: bool = False) -> DataFrame | LazyFrame. Execute SQL query with global frame registration. Parameters: query: SQL string, eager: return DataFrame if True. Expr.value_counts(*, sort: bool = False, parallel: bool = False, name: str | None = None, normalize: bool = False) -> Expr. Count unique value occurrences. Parameters: sort: sort by count, parallel, name: count column name, normalize: relative frequencies. Returns: Struct Expr (value, count). GroupBy.last() -> DataFrame: Aggregate last values in each group. Series.rolling_mean(window_size: int, weights=None, min_samples=None, center=False) -> Series: Apply rolling mean. window_size: window length. weights: optional weights. min_samples: min non-null values. center: center labels. Expr.get(index: int | Expr) -> Expr: Get single value by index. Returns expression of same data type. LazyFrame.map_batches(function: Callable[[DataFrame], DataFrame], predicate_pushdown=True, projection_pushdown=True, slice_pushdown=True, no_optimizations=False, schema=None, validate_output_schema=True, streamable=False) -> LazyFrame: Apply custom function. Function must return Polars DataFrame. Optimization flags control pushdown. Streamable: function is streamable. polars.Expr.name.keep() -> Expr: Keeps original expression root name. Undoes renaming. Last expression in chain only. One name operation per expression. Use .name.map for advanced renaming. polars.any(*names: str, ignore_nulls: bool = True) -> Expr | bool | None: Performs a bitwise OR operation on columns. ignore_nulls: handles nulls. LazyGroupBy.map_groups(function: Callable[[DataFrame], DataFrame], schema: SchemaDict | None) -> LazyFrame: Applies a function to each group. Slow; use expressions when possible. schema: output schema. LazyGroupBy.median() -> LazyFrame: Computes the median for each group. Expr.meta.is_column_selection(allow_aliasing: bool = False) -> bool: Check if expression selects columns (optionally with aliasing). Config.set_tbl_hide_dtype_separator(active: bool = True) -> Config: Hides data type separators in table output. polars.datatypes.List(inner: PolarsDataType | PythonDataType): Variable length list type. List.__init__(inner) LazyFrame.fill_nan(value: int | float | Expr | None) -> LazyFrame: Fill floating point NaN values. Args: value (replacement). Warning: NaN != null, use fill_null for nulls. Example: lf.fill_nan(99).collect() LazyFrame.median() -> LazyFrame: Aggregate columns to their median value. Example: lf.median().collect() Series.cat.to_local() -> Series: Convert a categorical column to its local representation. Changes underlying physical representation. Expr.clip(lower_bound: Numeric | Temporal | Expr | None = None, upper_bound: Numeric | Temporal | Expr | None = None) -> Expr: Clip values outside boundaries. Args: lower_bound, upper_bound. Works for numeric/temporal. Example: df.with_columns(clip=pl.col("a").clip(1, 10)) DataFrame.sample(n: int | Series | None = None, fraction: float | Series | None = None, with_replacement: bool = False, shuffle: bool = False, seed: int | None = None) -> DataFrame: Sample DataFrame rows. `n` and `fraction` are mutually exclusive. Expr.dt.year() -> Expr: Extract year from Date or Datetime Expr. Series.str.to_datetime(format: str | None = None, time_unit: TimeUnit | None = None, time_zone: str | None = None, strict: bool = True, exact: bool = True, cache: bool = True, ambiguous: Ambiguous | Series = 'raise') -> Series: Convert String Series to Datetime Series. Series.clip(lower_bound: NumericLiteral | TemporalLiteral | IntoExprColumn | None = None, upper_bound: NumericLiteral | TemporalLiteral | IntoExprColumn | None = None) -> Series: Set values outside the given boundaries to the boundary value. Parameters: lower_bound, upper_bound (accepts expression input or literal). Works for numeric and temporal columns. s.clip(1, 10) Series.reshape(dimensions: tuple[int, ...]) -> Series: Reshape this Series to a flat Series or an Array Series. Parameters: dimensions (tuple of dimension sizes, -1 infers). Returns: Series (original type if single dimension, Array if multiple). See also: Series.list.explode. s = pl.Series("foo", [1, 2, 3, 4, 5, 6, 7, 8, 9]); s.reshape((3, 3)) Expr.dt.replace(*, year: int | Expr | None = None, month: int | Expr | None = None, day: int | Expr | None = None, hour: int | Expr | None = None, minute: int | Expr | None = None, second: int | Expr | None = None, microsecond: int | Expr | None = None, ambiguous: Ambiguous | Expr = 'raise') -> Expr: Replace time unit (year, month, day, hour, minute, second, microsecond). Ambiguous: 'raise', 'earliest', 'latest', 'null'. Returns Expr of Date/Datetime. Example: df.with_columns(pl.col("date").dt.replace(day="new_day").alias("replaced")) Expr.name.to_uppercase() -> Expr: Make root column name uppercase. Only one name operation per expression will work. Consider using .name.map for advanced renaming. Example: df.with_columns(pl.all().name.to_uppercase()) polars.median(*columns: str): Get median value. Sugar for pl.col(columns).median(). Returns Expr. Parameters: columns. polars.testing.parametric.columns(cols=None, *, dtype=None, min_cols=0, max_cols=5, unique=False): Define columns for @dataframes strategy. Deprecated: use column. Unstable. Returns list[column]. Parameters: cols, dtype, min_cols=0, max_cols=5, unique=False. Expr.dt.timestamp(time_unit: TimeUnit = 'us'): Return timestamp in time unit. Returns Expr. Parameters: time_unit='us', 'ns', 'ms'. Expr.entropy(base=2.718281828459045, *, normalize=True): Compute entropy. Formula: -sum(pk * log(pk)). Returns Expr. Parameters: base=e, normalize=True. Expr.arr.get(index: int | IntoExprColumn, null_on_oob: bool = False) -> Expr: Gets value by index in sub-arrays. null_on_oob: True=null, False=error for out-of-bounds index. Expr.list.eval(expr: Expr, parallel: bool = False) -> Expr: Runs a polars expression against the lists' elements. Parameters: expr - Expression to run, parallel - Run expression parallel. Example: df.with_columns(rank=pl.concat_list("a", "b").list.eval(pl.element().rank())) Series.str.json_path_match(json_path: IntoExprColumn) -> Series: Extracts the first match of JSON string with provided JSONPath expression. Returns: Series of type String. Null values if the original value is null or the json_path returns nothing. SQLContext.execute(query: str, *, eager: bool | None = None) -> LazyFrame | DataFrame: Parses and executes a SQL query against registered frame data. Parameters: query - SQL query string, eager - Apply query eagerly (DataFrame return). Returns: LazyFrame or DataFrame. Config.set_tbl_hide_column_data_types(active: bool | None = True) -> type[Config]: Hides table column data types. Example: pl.Config.set_tbl_hide_column_data_types(True) Series.dt.nanosecond() -> Series: Extracts nanoseconds from DateTime representation. Returns: Series of Int32. Example: date.dt.nanosecond() Expr.nan_max() -> Expr: Gets maximum value, propagates NaN values. Differs from numpy by propagating NaNs by default. Example: df.select(pl.col("a").nan_max()) Expr.list.median() -> Expr. Compute median of lists in array. Expr.list.get(index: int | Expr | str, *, null_on_oob: bool = False) -> Expr. Get value by index in sublists. null_on_oob: if index out of bounds, return null or raise error. Expr.arr.contains(item: float | str | bool | int | date | datetime | time | IntoExprColumn) -> Expr. Check if sub-arrays contain item. DataFrame.max_horizontal() -> Series. Get the maximum value horizontally across columns. Series.first() -> PythonLiteral|None. Get first Series element. None if empty. Expr.map_elements(function, return_dtype=None, skip_nulls=True, pass_name=False, strategy='thread_local', returns_scalar=False) -> Expr: Apply custom function to each element of a column. DataFrame.rows_by_key(key, named=False, include_key=False, nunique=False) -> dict: Group rows by key column(s) and return a dictionary. Expr.qcut(quantiles: Sequence[float] | int, labels: Sequence[str] | None = None, left_closed: bool = False, allow_duplicates: bool = False, include_breaks: bool = False) -> Expr: Bin values into categories based on quantiles. Parameters: quantiles (probabilities or number of bins), labels, left_closed, allow_duplicates, include_breaks (changes output to Struct). Returns: Expr (Categorical or Struct). Unstable. Series.arr.max() -> Series: Compute the max values of the sub-arrays. DataFrame.shape -> tuple[int, int]: Get the shape of the DataFrame (rows, cols). Example: df.shape Series.bin.starts_with(prefix: IntoExpr) -> Series: Check if values start with binary substring. Parameter: prefix: Prefix substring. polars.exceptions.CategoricalRemappingWarning: Warning when categorical needs remapping for compatibility. polars.exceptions.RowsError: Exception when number of returned rows does not match expectation. Series.list.arg_max() -> Series: Returns the index of the maximum value in every sublist. Returns: Series of UInt32 or UInt64. Example: s.list.arg_max() Series.bitwise_and() -> PythonLiteral | None. Perform bitwise AND aggregation. InProcessQuery.fetch() -> DataFrame | None. Fetch result, returns DataFrame if ready, else None. Expr.cos() -> Expr. Compute element-wise cosine. Returns Float64 Expr. Series.drop_nans() -> Series: Drop NaN float values, preserve order. Use drop_nulls() for null values. polars.Series.is_not_null() -> Series[bool] Returns a boolean Series indicating which values are not null. Example: s = pl.Series("a", [1.0, 2.0, 3.0, None]) s.is_not_null() polars.Series.arctanh() -> Series Compute the element-wise inverse hyperbolic tangent. polars.Series.max() -> PythonLiteral | None Get the maximum value in this Series. Series.mode() -> Series: Compute most occurring value(s). Can return multiple values. Example: s.mode() Series.arr.std(ddof: int = 1) -> Series: Compute the std of the values of the sub-arrays. ddof: Delta Degrees of Freedom. Returns: Series. polars.scan_delta(source: str | DeltaTable, *, version: int | str | datetime | None = None, storage_options: dict[str, Any] | None = None, ...) Returns LazyFrame. Lazily read Delta lake table. Parameters: source: path/DeltaTable, version, storage_options, etc. Example: pl.scan_delta(table_path).collect(). Series.product() -> int|float. Reduce this Series to the product value. Returns: int or float. property Series.name: str: Gets the name of the Series. DataFrame.set_sorted(column: str, *, descending: bool = False) -> DataFrame. Indicate columns are sorted for faster ops. Warning: incorrect if data unsorted. DataFrame.min_horizontal() -> Series. Get minimum value horizontally across columns. Returns Series named "min". DataFrame.height -> int. Get number of rows. Series.struct.schema = {}: Access the schema of the struct. DataFrame.to_jax(return_type='array', device=None, label=None, features=None, dtype=None, order='fortran') -> jax.Array | dict[str, jax.Array]. Convert DataFrame to a Jax Array or dict of Jax Arrays. return_type: 'array' or 'dict'. device: jax.Device or str. label, features: str or Expr or Sequence. dtype: PolarsDataType. order: 'c' or 'fortran'. Expr.product() -> Expr. Compute product of expression. Example: pl.col("a").product() Series.bitwise_count_ones() -> Self: Evaluate the number of set bits. polars.from_records(data: Sequence[Any], schema: SchemaDefinition | None = None, *, schema_overrides: SchemaDict | None = None, strict: bool = True, orient: Orientation | None = None, infer_schema_length: int | None = 100) -> DataFrame: Construct a DataFrame from a sequence of sequences (clones data). 'data' is a sequence of sequences. 'schema' defines column names/types. 'schema_overrides' allows dtype overrides. 'strict' controls error throwing on type mismatches. 'orient' specifies row/column orientation ('row', 'col', or None for inference). 'infer_schema_length' limits row scan for inference. Example: pl.from_records([[1, 2, 3], [4, 5, 6]], schema=["a", "b"]) polars.read_delta(source: str | DeltaTable, *, version: int | str | datetime | None = None, columns: list[str] | None = None, rechunk: bool | None = None, storage_options: dict[str, Any] | None = None, credential_provider: CredentialProviderFunction | Literal['auto'] | None = 'auto', delta_table_options: dict[str, Any] | None = None, use_pyarrow: bool = False, pyarrow_options: dict[str, Any] | None = None) -> DataFrame: Reads DataFrame from Delta lake table. Parameters: source: DeltaTable or path/URI. version: Table version. columns: Columns to select. storage_options: Storage backend options. Returns: DataFrame. Series.len() -> int: Return the number of elements in the Series, including null values. polars.set_random_seed(seed: int) -> None Sets the global random seed for Polars. seed: A non-negative integer < 2^64. polars.Series.arr.last() -> Series Get the last value of the sub-arrays. polars.exceptions.DuplicateError Exception raised when a column name is duplicated. Series.unique_counts() -> Series. Returns counts of unique values in order of appearance. Expr.rolling_min_by(by: IntoExpr, window_size: timedelta | str, *, min_samples: int = 1, closed: ClosedInterval = 'right') -> Expr. Apply rolling min based on another column. Parameters: by (DateTime, Date, UInt64, UInt32, Int64, Int32), window_size (timedelta or duration string "1ns", "1us", "1ms", "1s", "1m", "1h", "1d", "1w", "1mo", "1q", "1y", "1i"), min_samples (int), closed ('left', 'right', 'both', 'none'). Series.list.n_unique() -> Series Count the number of unique values in every sub-lists. Series.sum() -> int | float. Reduces Series to sum. Dtypes {Int8, UInt8, Int16, UInt16} are cast to Int64 before summing. Series.arg_true() -> Series. Get index values where Boolean Series evaluate True. Returns: Series (UInt32). Example: (s == 2).arg_true() ParameterCollisionError: Exception raised when the same parameter occurs multiple times. polars.read_csv_batched(source, *, has_header=True, columns=None, separator=',', schema_overrides=None, null_values=None, n_threads=None, batch_size=50000, n_rows=None, encoding='utf8', low_memory=False) Read CSV file in batches. Returns BatchedCsvReader. API Extensions: Allow registering custom functionality in dedicated namespaces (Expr, DataFrame, LazyFrame, Series) without subclassing. register_expr_namespace(name), register_dataframe_namespace(name), register_lazyframe_namespace(name), register_series_namespace(name): Decorators to register. Cannot override existing namespaces (like .str or .dt). Example: @pl.api.register_expr_namespace("greetings") class Greetings: ... Series.abs() -> Series: Compute absolute values. Same as abs(series). Returns: Series. Expr.meta.eq(other: ExprMetaNameSpace | Expr) -> bool. Indicates if this expression is the same as another. ComputeError: Exception raised when Polars computation fails. CredentialProvider: Base class for credential providers. LazyFrame.profile(*, type_coercion: bool = True, _type_check: bool = True, predicate_pushdown: bool = True, projection_pushdown: bool = True, simplify_expression: bool = True, no_optimization: bool = False, slice_pushdown: bool = True, comm_subplan_elim: bool = True, comm_subexpr_elim: bool = True, cluster_with_columns: bool = True, collapse_joins: bool = True, show_plot: bool = False, truncate_nodes: int = 0, figsize: tuple[int, int] = (18, 8), streaming: bool = False, _check_order: bool = True,) -> tuple[DataFrame, DataFrame]: Profile a LazyFrame. Returns: (materialized DataFrame, profiling DataFrame). Parameters: optimization flags, show_plot, truncate_nodes, figsize, streaming. Example: lf.profile() Expr.rolling_var_by(by: IntoExpr, window_size: timedelta | str, *, min_samples: int = 1, closed: ClosedInterval = 'right', ddof: int = 1) -> Expr: Compute rolling variance based on another column. by: DateTime, Date, UInt64, UInt32, Int64, or Int32 data type. window_size: Length of the window (timedelta or string like '1ns', '1d', '1mo'). min_samples: Minimum non-null values. closed: {'left', 'right', 'both', 'none'}. ddof: Delta Degrees of Freedom. Unstable API. Series.arr.median() -> Series: Compute the median of sub-arrays. DataFrame.schema: Schema. Get ordered mapping of column names to data type. LazyFrame.max() -> LazyFrame: Aggregate columns to their maximum value. Returns LazyFrame. Series.nan_min() -> int | float | date | datetime | timedelta | str: Get minimum value, propagating NaN values. Differs from numpy's nanmax by propagating NaNs by default. Series.dt.minute() -> Series: Extract minutes from DateTime representation. Applies to Datetime columns. Returns Series of data type Int8. Range: 0 to 59. Series.dt.datetime() -> Series[Datetime]: Extract local datetime. Deprecated: use dt.replace_time_zone(None). polars.Series.rle_id() -> Series Get distinct integer ID for each run of identical values. Starts at 0, increments when value changes. Returns: Series (UInt32). See Also: rle. Notes: Useful to group by value changes. Example: s.rle_id() returns run-length encoding IDs. polars.Expr.str.to_lowercase() -> Expr Modify strings to lowercase. Example: df.with_columns(foo_lower=pl.col("foo").str.to_lowercase()) converts "foo" column to lowercase. polars.Expr.dt.dst_offset() -> Expr Additional offset in effect (typically due to daylight saving time). Returns: Expr (Duration). See Also: Expr.dt.base_utc_offset. Example: pl.col("ts").dt.dst_offset() returns the DST offset. DataFrame.clone() -> DataFrame Create a copy of this DataFrame. (cheap operation, no data copied) DataFrame.approx_n_unique() -> DataFrame: Approximate count of unique values per column using HyperLogLog++. Deprecated: Use select(pl.all().approx_n_unique()) instead. Expr.list.tail(n: int | str | Expr = 5) -> Expr Slice the last n values of every sublist. Expr.exp() -> Expr: Compute the exponential element-wise. Example: pl.col("values").exp() Expr.list.n_unique() -> Expr: Count the number of unique values in every sub-list. Example: df = pl.DataFrame({"a": [[1, 1, 2], [2, 3, 4]]}); df.with_columns(pl.col("a").list.n_unique()) Series.str.starts_with(prefix: str | Expr) -> Series Checks if string values start with a substring. Series.str.strptime(dtype: PolarsTemporalType, format: str | None = None, *, strict: bool = True, exact: bool = True, cache: bool = True, ambiguous: Ambiguous | Series = 'raise') -> Series Converts a String column into a Date/Datetime/Time column. format defaults to inferred. Expr.degrees() -> Expr Convert from radians to degrees. Expr.str.normalize(form: UnicodeForm = 'NFC') -> Expr: Returns Unicode normal form. form: 'NFC', 'NFKC', 'NFD', 'NFKD'. Expr.dt.week() -> Expr Extract the week from the underlying Date representation. Applies to Date and Datetime columns. Returns Int8. Expr.list.min() -> Expr Compute the min value of the lists in the array. Series.arg_sort(*, descending: bool = False, nulls_last: bool = False) -> Series Get the index values that would sort this Series. polars.all_horizontal(*exprs: IntoExpr | Iterable[IntoExpr]) -> Expr Compute bitwise AND horizontally across columns. Parameters: *exprs: Column expressions. Note: Kleene logic for nulls. Series.is_between(lower_bound: IntoExpr, upper_bound: IntoExpr, closed: ClosedInterval = 'both') -> Series Get boolean mask between bounds. Parameters: lower_bound, upper_bound: Boundary values. closed: Interval closure type ('both', 'left', 'right', 'none'). Note: False if lower > upper. DataFrame.head(n: int = 5) -> DataFrame: Get the first 'n' rows. If 'n' is negative, return all rows except the last abs(n). Example: df.head(3) DataFrame.write_excel(workbook: str | Workbook | IO[bytes] | Path | None = None, worksheet: str | Worksheet | None = None, *, position: tuple[int, int] | str = 'A1', table_style: str | dict[str, Any] | None = None, table_name: str | None = None, column_formats: ColumnFormatDict | None = None, dtype_formats: dict[OneOrMoreDataTypes, str] | None = None, conditional_formats: ConditionalFormatDict | None = None, header_format: dict[str, Any] | None = None, column_totals: ColumnTotalsDefinition | None = None, column_widths: ColumnWidthsDefinition | None = None, row_totals: RowTotalsDefinition | None = None, row_heights: dict[int | tuple[int, ...], int] | int | None = None, sparklines: dict[str, Sequence[str] | dict[str, Any]] | None = None, formulas: dict[str, str | dict[str, str]] | None = None, float_precision: int = 3, include_header: bool = True, autofilter: bool = True, autofit: bool = False, hidden_columns: Sequence[str] | SelectorType | None = None, hide_gridlines: bool = False, sheet_zoom: int | None = None, freeze_panes: str | tuple[int, int] | tuple[str, int, int] | tuple[int, int, int, int] | None = None) -> Workbook Write DataFrame to Excel. Parameters: workbook, worksheet: File/sheet names. position: Table position. table_style, table_name, column_formats, dtype_formats, conditional_formats, header_format, column_totals, column_widths, row_totals, row_heights, sparklines, formulas, float_precision, include_header, autofilter, autofit, hidden_columns, hide_gridlines, sheet_zoom, freeze_panes: Various formatting options. DataFrame.to_dummies(columns: ColumnNameOrSelector | Sequence[ColumnNameOrSelector] | None = None, *, separator: str = '_', drop_first: bool = False) -> DataFrame: Convert categorical variables to dummy/indicator variables. columns: Columns to convert, if None, convert all. separator: Separator for new column names. drop_first: Remove the first category. Example: df = pl.DataFrame({"foo": [1, 2], "bar": ["a", "b"]}); df.to_dummies() DataFrame.__dataframe__(nan_as_null: bool = False, allow_copy: bool = True) -> PolarsDataFrame: Convert to a dataframe object implementing the dataframe interchange protocol. nan_as_null: Overwrite nulls with NaN (NotImplementedError). allow_copy: Allow memory copying. If False, zero-copy conversions fail. Series.list.to_array(width: int) -> Series: Convert a List column to an Array column. width: Width of resulting Array. Returns Series of Array type. Example: s = pl.Series([[1, 2], [3, 4]], dtype=pl.List(pl.Int8)); s.list.to_array(2) Expr.str.pad_start(length: int, fill_char: str = ' ') -> Expr: Pad the start of string to given length. length: Target length, strings longer are unchanged. fill_char: Character to pad with. See also: pad_end, zfill. polars.datatypes.Int32: 32-bit signed integer type. Methods: base_type(), from_python(py_type), is_(other), is_decimal(), is_float(), is_integer(), is_nested(), is_numeric(), is_signed_integer(), is_temporal(), is_unsigned_integer(), max(), min(), to_python(). GroupBy.tail(n: int = 5) -> DataFrame: Get the last n rows of each group. n: Number of rows. Example: df.group_by("letters").tail(2) Expr.arr.std(ddof: int = 1) -> Expr: Compute the standard deviation of sub-arrays. Example: pl.DataFrame({"a": [[1, 2], [4, 3]]}).select(pl.col("a").arr.std()) Expr.cut(breaks: Sequence[float], *, labels: Sequence[str] | None = None, left_closed: bool = False, include_breaks: bool = False) -> Expr: Bin continuous values into categories. 'breaks' are cut points. 'labels' are category names (len(labels) == len(breaks) + 1). 'left_closed' makes intervals left-closed. 'include_breaks' adds a column with the right endpoint. Returns Categorical or Struct. Example: pl.col("foo").cut([-1, 1], labels=["a", "b", "c"]) Expr.cast(dtype: PolarsDataType | type[Any], *, strict: bool = True, wrap_numerical: bool = False) -> Expr: Cast between data types. 'dtype' is the target type. 'strict' controls error raising. 'wrap_numerical' enables wrapping on overflow. Example: pl.col("a").cast(pl.Float64) Series.rank(method: RankMethod = 'average', *, descending: bool = False, seed: int | None = None) -> Series: Assign ranks to data. 'method' can be 'average', 'min', 'max', 'dense', 'ordinal', 'random'. 'descending' for descending order. 'seed' for random seed. Example: s.rank("ordinal") DataFrame.glimpse(*, max_items_per_column: int = 10, max_colname_length: int = 50, return_as_string: bool = False) -> str | None: Returns a dense preview of the DataFrame (one line per column: name, type, and first few values). max_items_per_column controls the number of items shown per column. max_colname_length controls the max column name length. return_as_string returns string if set to True. Series.str.find(pattern: str | Expr, *, literal: bool = False, strict: bool = True) -> Series. Returns byte offset of first substring matching pattern, or None if not found. Expr.hist(bins: IntoExpr | None = None, *, bin_count: int | None = None, include_category: bool = False, include_breakpoint: bool = False) -> Expr. Bins values into buckets and counts occurrences. Expr.list.arg_min() -> Expr. Returns index of minimal value in each sublist. LazyFrame.gather_every(n: int, offset: int = 0) -> LazyFrame. Takes every nth row. DataFrame.corr(**kwargs: Any) -> DataFrame. Returns pairwise Pearson correlation coefficients. Requires numpy. Expr.dt.base_utc_offset() -> Expr. Returns base offset from UTC. Expr.cum_count(reverse: bool = False) -> Expr: Cumulative count of non-null values. Parameter: reverse (bool): Reverse operation. Series.str.replace(pattern: str, value: str, *, literal: bool = False, n: int = 1) -> Series: Replace first matching regex/literal substring. Parameters: pattern (str): Regex pattern; value (str): Replacement string; literal (bool): Treat pattern as literal; n (int): Number of matches to replace. $ must be escaped as $$. polars.from_dataframe(df: SupportsInterchange, *, allow_copy: bool = True) -> DataFrame: Create DataFrame from interchange protocol. Parameters: df: Object supporting dataframe interchange; allow_copy (bool): Allow memory copy. Returns: DataFrame. Expr.list.set_intersection(other: IntoExpr) -> Expr: Compute set intersection between lists. Parameter: other: Right hand side expression. Series.quantile(quantile: float, interpolation: str = 'nearest') -> float | None: Get quantile value of Series. Parameters: quantile (float): Quantile between 0.0 and 1.0; interpolation (str): Interpolation method ('nearest', 'higher', 'lower', 'midpoint', 'linear'). Series.dt.quarter() -> Series: Extract quarter (1-4) from Date/Datetime column. Returns: Series (Int8). DataFrame.iter_rows(*, named: bool = False, buffer_size: int = 512) -> Iterator[tuple[Any, ...]] | Iterator[dict[str, Any]]: Returns iterator over DataFrame rows. named=True returns dicts instead of tuples. buffer_size: modify only in specific cases. Warning: Row iteration is not optimal. See also: rows, rows_by_key. Notes: ns-precision temporal values truncated to microseconds. DataFrame.unnest(columns: ColumnNameOrSelector | Collection[ColumnNameOrSelector], *more_columns: ColumnNameOrSelector) -> DataFrame: Decompose struct columns into separate columns. columns: Name of struct column(s) to unnest. *more_columns: Additional columns. Example: df.unnest("t_struct") Expr.arr.count_matches(element: IntoExpr) -> Expr: Count occurrences of element's value in array. element: Expression producing a single value. Returns Expr. Expr.str.strip_chars_start(characters: IntoExpr = None) -> Expr: Remove leading characters. characters: Set of chars to remove. None (default) removes whitespace. See also: strip_prefix, strip_chars_end. Note: strips any chars present in 'characters' from start. Use strip_prefix() to strip a specific "word". Example: df.with_columns(foo_strip_start=pl.col("foo").str.strip_chars_start("wod\\t")) Config.set_thousands_separator(separator: str | bool | None = None) -> type[Config]: Set thousands grouping separator. separator: True for default ("," thousands, "." decimal), custom char, or None to omit. See also: set_decimal_separator. Expr.hash(seed: int = 0, seed_1: int | None = None, seed_2: int | None = None, seed_3: int | None = None) -> Expr: Hash elements. Hash value is UInt64. seed: Random seed (defaults to 0). seed_1, seed_2, seed_3: Random seed parameters (defaults to seed). Note: Unstable across Polars versions. GroupBy.__iter__() -> Self: Iterate over groups. Each group is (name, data). Name is tuple of distinct group values. Series.str.contains(pattern: str | Expr, *, literal: bool = False, strict: bool = True) -> Series[Boolean]: Check if string contains pattern. literal=True for literal match. strict=True error on invalid regex. Series.bin.contains(literal: IntoExpr) -> Series[Boolean]: Check if binaries contain binary substring. polars.last(*columns: str) -> Expr: Get last column or value. Returns last column if no columns given, else last value of columns. Expr.cum_prod(*, reverse: bool = False) -> Expr: Cumulative product at every element. reverse=True reverses operation. Series.arr.to_struct(fields: Callable[[int], str] | Sequence[str] | None = None) -> Series[Struct]: Convert Array Series to Struct Series. fields to name struct fields. Series.str.strip_chars(characters: IntoExpr = None) -> Series: Remove leading and trailing characters. `characters`: set of characters to remove, or None (default, removes whitespace). Example: s.str.strip_chars("o ") Series.str.replace_all(pattern: str, value: str, *, literal: bool = False) -> Series: Replace all matching regex/literal substrings. `pattern`: regex pattern. `value`: replacement string. `literal`: treat pattern as literal. Notes: use inline flags like `(?i)` for case-insensitivity. Use $$ for literal $, or set literal=True. Supports capture groups ($1, ${1}, named groups). Example: s.str.replace_all(r"h(.)t", "b${1}d") Config.set_tbl_hide_dataframe_shape(active: bool | None = True) -> type[Config]: Hide DataFrame shape information when displaying tables. Example: pl.Config.set_tbl_hide_dataframe_shape(True) Expr.arr.to_struct(fields: Sequence[str] | Callable[[int], str] | None = None) -> Expr: Convert Array to Struct Series. `fields`: list of field names, a function to generate names, or None (defaults to field_0, field_1...). Example: df.with_columns(struct=pl.col("n").arr.to_struct(fields=lambda idx: f"n{idx}")) polars.scan_ipc(source: str | Path | IO[bytes] | bytes | list[str] | list[Path] | list[IO[bytes]] | list[bytes], *, n_rows: int | None = None, cache: bool = True, rechunk: bool = False, row_index_name: str | None = None, row_index_offset: int = 0, storage_options: dict[str, Any] | None = None, credential_provider: CredentialProviderFunction | Literal['auto'] | None = 'auto', memory_map: bool = True, retries: int = 2, file_cache_ttl: int | None = None, hive_partitioning: bool | None = None, hive_schema: SchemaDict | None = None, try_parse_hive_dates: bool = True, include_file_paths: str | None = None) -> LazyFrame: Lazily read Arrow IPC file(s). source: Path(s), n_rows: Stop after n rows, cache: Cache result, rechunk: Reallocate contiguous memory, row_index_name: Row index column name, row_index_offset: Row index offset, storage_options: Cloud provider options (AWS, GCP, Azure), credential_provider: Function for cloud credentials, memory_map: Memory map file, retries: Retries for cloud access, file_cache_ttl: Cloud file cache TTL (seconds), hive_partitioning: Infer statistics from Hive partitioned URL, hive_schema: Schema for Hive partitions, try_parse_hive_dates: Parse Hive values as dates, include_file_paths: Column name for source file paths. polars.max(*names: str) -> Expr: Get the maximum value (syntactic sugar for col(names).max()). `*names`: column name(s). Example: df.select(pl.max("a", "b")) Expr.is_not_null() -> Expr: Returns a boolean Series indicating which values are not null. Example: df.with_columns(pl.all().is_not_null().name.suffix("_not_null")) Expr.gather_every(n: int, offset: int = 0) -> Expr. Take every nth value in the Series. Series.filter(predicate: Series | Iterable[bool]) -> Self. Filter elements by boolean mask. Expr.reverse() -> Expr. Reverse the selection. polars.Expr.name.prefix_fields(prefix: str) -> Expr Add prefix to all field names of a struct. Parameters: prefix. See Also: map_fields, suffix_fields. Notes: Only affects struct columns. Example: df.select(pl.col("x").name.prefix_fields("prefix_")).schema shows prefixed field names inside the struct. polars.api.register_lazyframe_namespace(name: str) -> Callable[[type[NS]], type[NS]]. Decorator for registering custom functionality with a Polars LazyFrame. Expr.cat.ends_with(suffix: str) -> Expr. Check if string representations of values end with a substring. Series.dt.strftime(format: str) -> Series. Convert Date/Time/Datetime column into a String column with the given format. DataFrame.bottom_k(k: int, *, by: IntoExpr | Iterable[IntoExpr], reverse: bool | Sequence[bool] = False) -> DataFrame. Return the k smallest rows. Expr.meta.pop() -> list[Expr]: Pop the latest expression. Returns list of Expr (usually unit length). Multiple inputs exist in fold expressions. Example: e = pl.col("foo").alias("bar"); first = e.meta.pop()[0] Expr.mod(other: Any) -> Expr: Modulus operator (expr % other). other: Numeric literal or expression. Example: df.with_columns(pl.col("x").mod(2).alias("x%2")) DataFrame.upsample(time_column: str, *, every: str | timedelta, group_by: str | Sequence[str] | None = None, maintain_order: bool = False) -> DataFrame: Upsample DataFrame at regular frequency. time_column: Time column for date range. every: Interval (e.g., "1mo"). group_by: Group by these columns first. maintain_order: Keep order. Returns sorted DataFrame. Example: df.upsample(time_column="time", every="1mo", group_by="groups", maintain_order=True) DataFrame.__array__(dtype: npt.DTypeLike | None = None, copy: bool | None = None) -> np.ndarray[Any, Any]: Return NumPy ndarray. Enables np.asarray and NumPy universal functions. Expr.rolling_quantile(quantile: float, interpolation: RollingInterpolationMethod = 'nearest', window_size: int = 2, weights: list[float] | None = None, *, min_samples: int | None = None, center: bool = False) -> Expr: Compute rolling quantile. quantile: Quantile (0.0 to 1.0). interpolation: Interpolation method. window_size: Window length. weights: Optional weights. min_samples: Min non-null samples. center: Center labels. Example: pl.col("A").rolling_quantile(quantile=0.25, window_size=4) Series.dt.add_business_days(n: int | IntoExpr, week_mask: Iterable[bool] = (True, True, True, True, True, False, False), holidays: Iterable[dt.date] = (), roll: Roll = 'raise') -> Series: Offset by n business days. n: Number of days. week_mask: Which days to count (default Mon-Fri). holidays: Holidays to exclude. roll: 'raise', 'forward', 'backward' if start date is non-business day. Example: s.dt.add_business_days(5) polars.Series.list.set_intersection(other: Series) -> Series Compute SET INTERSECTION between elements in this list and elements of other. Parameters: other (Series). Example: a.list.set_intersection(b) returns a Series with the intersection of lists in a and b. Series.ewm_std(*, com: float | None = None, span: float | None = None, half_life: float | None = None, alpha: float | None = None, adjust: bool = True, bias: bool = False, min_samples: int = 1, ignore_nulls: bool = False) -> Series: Compute exponentially-weighted moving standard deviation. com: Center of mass. span: Span. half_life: Half-life. alpha: Smoothing factor. adjust: Adjust for imbalance. bias: Unbiased estimate correction. min_samples: Min observations. ignore_nulls: Ignore missing values. Example: s.ewm_std(com=1, ignore_nulls=False) Series.arr.arg_max() -> Series: Retrieve index of maximum value in every sub-array. Returns UInt32/UInt64 Series. Example: s = pl.Series("a", [[0, 9, 3], [9, 1, 2]], dtype=pl.Array(pl.Int64, 3)); s.arr.arg_max() polars.testing.parametric.column(name: str | None = None, dtype: PolarsDataType | None = None, strategy: SearchStrategy[Any] | None = None, allow_null: bool | None = None, unique: bool = False, null_probability: float | None = None) Define a column for hypothesis dataframes strategy. Parameters: name (column name), dtype, strategy (override default strategy), allow_null, unique (all values unique), null_probability (0.0 => 1.0 chance of None, deprecated, use allow_null). __init__(name, dtype, strategy, allow_null, unique, null_probability): Initializes a column definition. Attributes: allow_null, dtype, name, null_probability, strategy, unique. polars.min(*names: str) -> Expr Syntactic sugar for col(names).min(). Get minimum value of column(s). Parameters: *names (column names). See Also: min_horizontal. Example: df.select(pl.min("a")) returns the minimum of column "a". polars.LazyFrame.sum() -> LazyFrame Aggregate columns to their sum. Example: lf.sum().collect() returns a new LazyFrame with the sum of each column. polars.sql_expr(sql: str | Sequence[str]) -> Expr | list[Expr]. Parse SQL expressions to Polars Exprs. Example: pl.sql_expr("MAX(a)") Series.map_elements(function: Callable[[Any], Any], return_dtype: PolarsDataType | None = None, *, skip_nulls: bool = True) -> Self. Map UDF over Series elements. Slower than native API. Example: s.map_elements(lambda x: x + 10, return_dtype=pl.Int64) GroupBy.quantile(quantile: float, interpolation: RollingInterpolationMethod = 'nearest') -> DataFrame. Compute quantile per group. Example: df.group_by("d").quantile(1) LazyFrame.update(other: LazyFrame, on: str | Sequence[str] | None = None, how: Literal['left', 'inner', 'full'] = 'left', *, left_on, right_on, include_nulls: bool = False) -> LazyFrame. Update LazyFrame values with other LazyFrame. How: 'left', 'inner', 'full'. Unstable. Example: lf.update(new_lf).collect() Series.is_in(other: Series | Collection[Any]) -> Series. Check if Series elements are in other Series/Collection. Returns Boolean Series. Example: s2.is_in(s) Series.dt.with_time_unit(time_unit: TimeUnit) -> Series. Set time unit for Datetime/Duration Series. Deprecated: cast to Int64 then cast back. Example: s.dt.with_time_unit("us") polars.disable_string_cache() -> bool: Disables and clears the global string cache. See also: enable_string_cache, StringCache. Use StringCache context manager for reliability. Expr.arr.last() -> Expr: Get the last value of the sub-arrays. Example: df.with_columns(last=pl.col("a").arr.last()) polars.from_numpy(data: np.ndarray, schema: SchemaDefinition | None = None, *, schema_overrides: SchemaDict | None = None, orient: Orientation | None = None) -> DataFrame: Constructs DataFrame from NumPy ndarray (clones data). Parameters: data: NumPy ndarray. schema: column names or {name: type} dict. schema_overrides: dict of type overrides. orient: 'col' or 'row', None infers. Returns: DataFrame. Series.list.concat(other: list[Series] | Series | list[Any]) -> Series Concat arrays in a List Series. other: Columns to concat. s1.list.concat(s2) Expr.bitwise_xor() -> Expr: Perform an aggregation of bitwise XORs. Example: df.select(pl.col("n").bitwise_xor()) Expr.kurtosis(*, fisher: bool = True, bias: bool = True) -> Expr Compute kurtosis of dataset. fisher: True normal==>0.0, False normal==>3.0. bias: False corrects for statistical bias. LazyFrame.rename(mapping: dict[str, str] | Callable[[str], str], *, strict: bool = True) -> LazyFrame Rename columns. mapping: dict or function to map old names to new. strict: validate column names exist. lf.rename({"foo": "apple"}).collect() Python API reference: DataFrame, LazyFrame, Series, Expressions, Selectors, Importing, Set operations, Functions, Data types, Schema, Input/output, Catalog, Config, Extending the API, Plugins, SQL Interface, Exceptions, Testing, Metadata polars.testing.assert_frame_equal(left: DataFrame | LazyFrame, right: DataFrame | LazyFrame, *, check_row_order: bool = True, check_column_order: bool = True, check_dtypes: bool = True, check_exact: bool = False, rtol: float = 1e-05, atol: float = 1e-08, categorical_as_str: bool = False) -> None Assert left and right frames are equal. Raises AssertionError if different. Expr.and_(*others: Any) -> Expr Bitwise "and" operator. others: Integer or boolean expressions. (pl.col("x") >= pl.col("z")).and_(pl.col("y") >= pl.col("z")) Series.rolling_std(window_size: int, weights: list[float] | None = None, *, min_samples: int | None = None, center: bool = False, ddof: int = 1) -> Series Compute rolling std dev. window_size: window length. weights: optional weights. min_samples: min non-null samples. center: center labels. ddof: Delta Degrees of Freedom. polars.Series.diff(n: int = 1, null_behavior: NullBehavior = 'ignore') -> Series Calculates the first discrete difference between shifted items. n: Number of slots to shift. null_behavior: How to handle null values ('ignore', 'drop'). polars.Config.state(if_set: bool = False, env_only: bool = False) -> dict[str, str | None] Shows the current state of all Config variables. if_set: Restrict to variables set to a specific value. env_only: Include only Config environment variables. polars.Series.str.len_bytes() -> Series Return the length of each string as the number of bytes. Returns: Series of data type UInt32. Note: Inefficient for non-ASCII text, use len_chars() instead. Expr.dt.combine(time: dt.time | Expr, time_unit: TimeUnit) -> Expr Create naive Datetime from Date/Datetime and Time. Replaces time in Datetime or combines Date and Time. polars.lazyframe.group_by.LazyGroupBy.head(n: int = 5) -> LazyFrame Get the first n rows of each group. n: Number of rows to return. polars.DataFrame.median() -> DataFrame Aggregate the columns of this DataFrame to their median value. Expr.str.escape_regex() -> Expr Escape regex meta characters in string values. class Catalog(workspace_url: str, bearer_token: str) Unity catalog client. Unstable. Methods: __init__(workspace_url, bearer_token) get_table_info(catalog_name, schema_name, table_name) -> TableInfo list_catalogs() -> list[str] list_schemas(catalog_name) -> list[str] list_tables(catalog_name, schema_name) -> list[str] scan_table(catalog_name, schema_name, table_name, storage_options, row_count, n_rows, projection, push_down_predicate, **kwargs) -> LazyFrame Series.equals(other: Series, check_dtypes: bool, check_names: bool, null_equal: bool) -> bool Check if Series equals another Series. Series.dt.microsecond() -> Series[Int32] Extract microseconds from DateTime. polars.time_ranges(start: time | IntoExprColumn | None = None, end: time | IntoExprColumn | None = None, interval: str | timedelta = '1h', *, closed: ClosedInterval = 'both', eager: bool = False) -> Series | Expr. Create a column of time ranges. Parameters: start (lower bound, defaults to time(0, 0, 0, 0)), end (upper bound, defaults to time(23, 59, 59, 999999)), interval (timedelta or duration string like "1ns", "1us", "1ms", "1s", "1m", "1h", "1d", "1w", "1mo", "1q", "1y"), closed ('both', 'left', 'right', 'none'), eager (bool). Returns: Expr or Series (List(Time)). LazyFrame.drop_nulls(subset: ColumnNameOrSelector | Collection[ColumnNameOrSelector] | None = None) -> LazyFrame. Drops rows containing null values. Parameters: subset (column names to consider, default None uses all). Example: lf.drop_nulls(subset=["foo", "bar"]).collect() polars.date_ranges(start: date | datetime | IntoExprColumn, end: date | datetime | IntoExprColumn, interval: str | timedelta = '1d', *, closed: ClosedInterval = 'both', eager: bool = False) -> Series | Expr. Create a column of date ranges. Parameters: start (lower bound), end (upper bound), interval (timedelta or duration string like "1d", "1w", "1mo", "1q", "1y"), closed ('both', 'left', 'right', 'none'), eager (bool). Returns: Expr or Series (List(Date)). class polars.datatypes.Null. Data type representing null values. Methods: base_type(), from_python(py_type), is_(other), is_decimal(), is_float(), is_integer(), is_nested(), is_numeric(), is_signed_integer(), is_temporal(), is_unsigned_integer(), to_python(). Expr.arg_true() -> Expr: Return indices where expression evaluates True. Modifies number of rows returned. See also: Series.arg_true, polars.arg_where. Example: df.select((pl.col("a") == 1).arg_true()) Expr.str.contains(pattern: str | Expr, *, literal: bool = False, strict: bool = True) -> Expr. Check if string contains substring matching pattern. Parameters: pattern (regex), literal (treat pattern as literal), strict (raise error on invalid regex). See also: starts_with, ends_with, find. Use inline (?iLmsuxU) regex flags. Example: pl.col("s").str.contains("(?i)AA"). SQLContext.unregister(names: str | Collection[str]) -> Self: Unregister eager/lazy frames by name. names: Names of tables to unregister. See also: register, register_globals, register_many. DataFrame.quantile(quantile: float, interpolation: RollingInterpolationMethod = 'nearest') -> DataFrame: Aggregate columns to quantile value. quantile: Quantile between 0.0 and 1.0. interpolation: {'nearest', 'higher', 'lower', 'midpoint', 'linear'}. DataFrame.merge_sorted(other: DataFrame, key: str) -> DataFrame: Merge two sorted DataFrames by sorted key. other: DataFrame to merge. key: Sorted key. Schemas must be equal. Frames must be sorted by key. Example: df0.merge_sorted(df1, key="age") polars.testing.parametric.load_profile(profile: Literal['fast', 'balanced', 'expensive'] | int = 'fast', *, set_environment: bool = False) -> None: Load hypothesis profile for parametric tests. profile: 'fast', 'balanced', 'expensive', or iterations count. set_environment: Set POLARS_HYPOTHESIS_PROFILE env var. Expr.list.explode() -> Expr: Returns a column with a separate row for every list element. Returns: Expr with the data type of the list elements. See also: Expr.reshape. Example: df.select(pl.col("a").list.explode()) Expr.set_sorted(*, descending: bool = False) -> Expr: Flags the expression as 'sorted'. descending: Whether the Series order is descending. Warning: Can lead to incorrect results if data is NOT sorted. Example: df.select(pl.col("values").set_sorted().max()) Expr.dt.millennium() Extract millennium from Date/Datetime columns. Returns millennium number. Returns Expr[Int32]. Config.set_tbl_cell_alignment(format) Set table cell alignment. Params: format: Literal['LEFT', 'CENTER', 'RIGHT'] | None. Returns type[Config]. Raises: ValueError if format invalid. Expr.dt.total_hours() Extract total hours from Duration type. Returns Expr[Int64]. DataFrame.dtypes property Get column data types. Returns list[DataType]. Series.to_jax(device=None) Convert Series to Jax Array. Params: device. Returns jax.Array. Warning: unstable. Expr.lt(other: Any) -> Expr: Method equivalent of "less than" operator expr < other. Returns a boolean Expr. Example: pl.col("x").lt(pl.col("y")) Expr.list.last() -> Expr: Get the last value of the sublists. Returns: Expr. If list is empty returns null. Expr.bin.reinterpret(*, dtype: PolarsDataType, endianness: Endianness = 'little') -> Expr: Interpret a buffer as a numerical Polars type. Params: dtype: PolarsDataType, endianness: "big", "little". Returns: Expr of data type dtype. If binary array is too short value will be null. If binary array is too long, remainder will be ignored. Expr.peak_max() -> Expr: Get a boolean mask of the local maximum peaks. Returns: Expr. polars.datetime_ranges(start: datetime | date | IntoExprColumn, end: datetime | date | IntoExprColumn, interval: str | timedelta = '1d', *, closed: ClosedInterval = 'both', time_unit: TimeUnit | None = None, time_zone: str | None = None, eager: bool = False) -> Series | Expr: Create a column of datetime ranges. Returns: Expr or Series of type List(Datetime). Series.str.head(n: int | IntoExprColumn) -> Series: Returns first n characters of each string. n: Length of slice. Returns: Series of type String. If n is negative, returns chars up to nth from end. If string length < n, returns full string. Example: s.str.head(5) DataFrame.update(other: DataFrame, on: str | Sequence[str] | None = None, how: Literal['left', 'inner', 'full'] = 'left', *, left_on: str | Sequence[str] | None = None, right_on: str | Sequence[str] | None = None, include_nulls: bool = False) -> DataFrame: Update values with values from other DataFrame. on: Join columns. how: 'left', 'inner', 'full'. left_on: Left join columns. right_on: Right join columns. include_nulls: Overwrite with nulls from right. Unstable. Example: df.update(new_df) Series.dt.combine(time: dt.time | Series, time_unit: TimeUnit = 'us') -> Series: Create Datetime from Date/Datetime and Time. time: Time literal or Series. time_unit: 'ns', 'us', 'ms'. Returns: Datetime Series. If input is Datetime, replaces time component. If Date, creates new Datetime. Example: s.dt.combine(time(1, 2, 3)) Expr.meta.serialize(file: IOBase | str | Path | None = None, *, format: SerializationFormat = 'binary') -> bytes | str | None: Serialize expression to file/string in JSON. file: File path (None returns string). format: 'binary', 'json'. Returns: bytes or str. Not stable across Polars versions. See also: Expr.deserialize. Example: expr.meta.serialize() Series.list.gather_every(n: int | IntoExprColumn, offset: int | IntoExprColumn = 0) -> Series: Take every nth value from offset in sublists. n: Gather every nth element. offset: Starting index. Example: s.list.gather_every(2, offset=1) Series.str.explode() -> Series: Returns column with separate row for every string character. Returns: String Series. Deprecated: Use .str.split("").explode(). Empty strings result in null. Example: s.str.explode() Series.dt.month_start() -> Series: Roll backward to first day of month. Returns: Date or Datetime Series. Analogous to pandas.tseries.offsets.MonthBegin().rollback(datetime). Example: s.dt.month_start() Expr.str.strip_suffix(suffix: Expr) -> Expr. Remove suffix once if found. Strips exact suffix. See also: strip_chars_end, strip_prefix. Expr.bitwise_or() -> Expr. Aggregate bitwise ORs. Series.describe(percentiles: float | Sequence[float] = (0.25, 0.5, 0.75), interpolation: str = 'nearest') -> DataFrame. Summary statistics of Series. Percentiles [0, 1]. Interpolation: 'nearest', 'higher', 'lower', 'midpoint', 'linear'. Returns DataFrame. polars.cum_count(*columns: str, reverse: bool = False) -> Expr. Cumulative count of non-null values in column(s). Syntactic sugar for col(columns).cum_count(). Reverse operation if reverse=True. class polars.datatypes.Array(inner: PolarsDataType | PythonDataType, shape: int | tuple[int, ...] | None = None, *, width: int | None = None). Fixed length list type. Parameters: inner (value DataType), width (array length). Methods: __init__(inner[, shape, width]), base_type(), from_python(py_type), is_(other), is_decimal(), is_float(), is_integer(), is_nested(), is_numeric(), is_signed_integer(), is_temporal(), is_unsigned_integer(), to_python(). Attributes: width, inner, size, shape. GroupBy.map_groups(function: Callable[[DataFrame], DataFrame]) -> DataFrame. Apply custom function over groups as sub-DataFrame. Warning: slower than native API. Expr.rolling_skew(window_size: int, *, bias: bool = True) -> Expr. Compute rolling skew. Unstable. window_size: window size, bias: correct for statistical bias. Window includes current row and window_size-1 previous. Series.arr.join(separator: IntoExprColumn, *, ignore_nulls: bool = True) -> Series. Join string array items with separator. separator: string separator. ignore_nulls: ignore nulls (default), propagate if False. Error if inner type != String. Returns String Series. class polars.datatypes.Time. Data type for time of day. Underlying representation: 64-bit nanoseconds since midnight. Methods: __init__, base_type(), from_python(py_type), is_(other), is_decimal(), is_float(), is_integer(), is_nested(), is_numeric(), is_signed_integer(), is_temporal(), is_unsigned_integer(), to_python(). LazyFrame.with_row_count(name: str = 'row_nr', offset: int = 0) -> LazyFrame. Add row count column at index 0. Deprecated: Use with_row_index(). Warning: Negative effect on query performance. DataFrame.join_where(other: DataFrame, *predicates: Expr | Iterable[Expr], suffix: str = '_right') -> DataFrame. Perform an inner join based on (in)equality predicates. Experimental functionality. Row order not preserved. predicates: (In)Equality conditions, apply suffix if column names duplicate. suffix: For duplicate names. Example: east.join_where(west, pl.col("dur") < pl.col("time"), pl.col("rev") < pl.col("cost")). Expr.list.all() -> Expr. Evaluate if all boolean values in a list are true. Example: df.with_columns(all=pl.col("a").list.all()). DataFrame.fill_nan(value: Expr | int | float | None) -> DataFrame. Replace floating point NaN values. value: Replacement value. Note: Floating point NaNs are not missing values; use fill_null() for missing values. Example: df.fill_nan(99). Expr.str.decode(encoding: TransferEncoding, *, strict: bool = True) -> Expr. Decode string values. encoding: 'hex' or 'base64'. strict: Raise error if decoding fails, else mask with null. Returns: Expr of Binary type. Example: df.with_columns(pl.col("color").str.decode("hex").alias("decoded")). Series.list.diff(n: int = 1, null_behavior: NullBehavior = 'ignore') -> Series. Calculate first discrete difference between shifted items in sublists. n: Shift slots. null_behavior: 'ignore' or 'drop'. Expr.rolling_sum(window_size: int, weights: list[float] | None = None, *, min_samples: int | None = None, center: bool = False) -> Expr: Apply a rolling sum over the values. Parameters: window_size (length of the window), weights (multiply elementwise with the window), min_samples (min non-null values to compute result, defaults to window_size), center (labels at the center). Example: df.with_columns(pl.col("A").rolling_sum(window_size=2)). Series.cat.len_bytes() -> Series: Return the byte-length of the string representation of each value. Returns: Series of data type UInt32. See also: len_chars. Note: len_bytes is O(1), len_chars is O(n). Example: s = pl.Series(["Caf", "345", "", None], dtype=pl.Categorical); s.cat.len_bytes(). Expr.fill_nan(value: int | float | Expr | None) -> Expr: Fill floating point NaN values. Parameter: value (used to fill NaN). Warning: NaNs are not missing values; use fill_null() for missing values. See also: fill_null. Example: df.with_columns(pl.col("b").fill_nan(0)). Expr.shrink_dtype() -> Expr: Shrink numeric columns to the minimal required datatype to reduce memory pressure. Example: pl.DataFrame({"a":[1,2,3]}).select(pl.all().shrink_dtype()). Series.list.head(n: int | Expr = 5) -> Series: Slice the first n values of every sublist. Parameter: n (number of values to return). Example: s = pl.Series("a", [[1, 2, 3, 4], [10, 2, 1]]); s.list.head(2). class polars.datatypes.UInt32: 32-bit unsigned integer type. Methods: __init__, base_type, from_python, is_, is_decimal, is_float, is_integer, is_nested, is_numeric, is_signed_integer, is_temporal, is_unsigned_integer, max, min, to_python. Series.list.sort(descending: bool = False, nulls_last: bool = False, multithreaded: bool = True) -> Series Sort arrays in this column. polars.business_day_count(start: date | IntoExprColumn, end: date | IntoExprColumn, week_mask: Iterable[bool] = (True, True, True, True, True, False, False), holidays: Iterable[date] = ()) -> Expr Count business days between start and end (excluding end). Expr.var(ddof: int = 1) -> Expr Get variance. Series.rolling_quantile(quantile: float, interpolation: RollingInterpolationMethod = 'nearest', window_size: int = 2, weights: list[float] | None = None, *, min_samples: int | None = None, center: bool = False) -> Series Compute rolling quantile. Series.extend(other: Series) -> Self Extend Series with values from another Series. Modifies in-place. polars.time_range(start: time | IntoExprColumn | None = None, end: time | IntoExprColumn | None = None, interval: str | timedelta = '1h', closed: ClosedInterval = 'both', eager: bool = False) -> Series | Expr: Generate time range. interval examples: '1ns', '1us', '1ms', '1s', '1m', '1h', '1d', '1w', '1mo', '1q', '1y'. GroupBy.mean() -> DataFrame: Reduce groups to mean values after DataFrame.group_by(). LazyFrame.unique(subset: ColumnNameOrSelector | Collection[ColumnNameOrSelector] | None = None, keep: UniqueKeepStrategy = 'any', maintain_order: bool = False) -> LazyFrame: Drop duplicate rows. keep: 'first', 'last', 'any', 'none'. Fails with List column in subset. LazyGroupBy.all() -> LazyFrame. Aggregate groups into Series. Expr.arr.arg_max() -> Expr: Get index of maximum value in each sub-array. Returns UInt32/UInt64 Expr. Expr.not_() -> Expr: Negate boolean expression. Expr.list.std(ddof=1) Compute the std value of the lists in the array. ddof: Delta Degrees of Freedom (divisor is N - ddof). Returns Expr. Expr.abs() Compute absolute values. Returns Expr. Expr.alias(name) Rename expression. name: New name (str). Returns Expr. Expr.list.sample(n=None, fraction=None, with_replacement=False, shuffle=False, seed=None) Sample from list. n: Number of items. fraction: Fraction of items. Cannot use both. with_replacement: Allow sampling same value more than once. shuffle: Shuffle order. seed: Random seed. Returns Expr. LazyGroupBy.max() -> LazyFrame. Reduce groups to the maximal value. class polars.CredentialProviderAWS(*, profile_name: str | None = None, region_name: str | None = None, assume_role: AWSAssumeRoleKWArgs | None = None). AWS Credential Provider. __init__(*, profile_name: str | None = None, region_name: str | None = None, assume_role: AWSAssumeRoleKWArgs | None = None) -> None. Series.dt.total_hours() -> Series. Extract the total hours from a Duration type. DataFrame.rename(mapping: dict[str, str] | Callable[[str], str], *, strict: bool = True) -> DataFrame: Rename column names. `strict` validates that all column names exist. polars.concat_arr(exprs: IntoExpr | Iterable[IntoExpr], *more_exprs: IntoExpr) -> Expr: Horizontally concatenate columns into a single array column. class Int8. 8-bit signed integer type. Methods: __init__, base_type(), from_python(py_type), is_(other), is_decimal(), is_float(), is_integer(), is_nested(), is_numeric(), is_signed_integer(), is_temporal(), is_unsigned_integer(), max(), min(), to_python(). polars.cum_fold(acc:IntoExpr, function:Callable, exprs:Expr, include_init:bool=False) -> Expr. Cumulative horizontal fold across columns. Returns Struct column. Expr.list.count_matches(element:IntoExpr) -> Expr. Count element occurrences in list. DataFrame.with_row_count(name='row_nr', offset=0) -> DataFrame: Add a row count column. Deprecated, use with_row_index. LazyFrame.tail(n=5) -> LazyFrame: Get the last n rows. class polars.datatypes.Datetime(time_unit='us', time_zone=None): Datatype for calendar date and time. time_unit: 'us', 'ns', 'ms'; time_zone: Time zone string. Expr.str.to_datetime(format: str | None = None, time_unit: TimeUnit | None = None, time_zone: str | None = None, strict: bool = True, exact: bool = True, cache: bool = True, ambiguous: Ambiguous | Expr = 'raise') -> Expr: Convert String column to Datetime column. Parameters: format (strftime), time_unit (ns, us, ms), time_zone, strict (raise on failure), exact (require exact format), cache, ambiguous ('raise', 'earliest', 'latest', 'null'). Expr.str.concat(delimiter: str | None = None, ignore_nulls: bool = True) -> Expr: Concatenate string values. Parameters: delimiter, ignore_nulls (default True, False propagates nulls). Returns: Expr of String type. Deprecated: Use join(). Expr.unique(maintain_order: bool = False) -> Expr: Get unique values. Parameters: maintain_order (default False, requires more work). polars.read_database_uri(query: list[str] | str, uri: str, partition_on: str | None = None, partition_range: tuple[int, int] | None = None, partition_num: int | None = None, protocol: str | None = None, engine: DbReadEngine | None = None, schema_overrides: SchemaDict | None = None, execute_options: dict[str, Any] | None = None) -> DataFrame: Read SQL query results into a DataFrame from a URI. Parameters: query (SQL query), uri (connection URI), partition_on, partition_range, partition_num, protocol, engine ('connectorx', 'adbc'), schema_overrides, execute_options. Requires connectorx>=0.3.2 or pyarrow and ADBC driver. DataFrame.write_csv(file: str | Path | IO[str] | IO[bytes] | None = None, *, include_bom: bool = False, include_header: bool = True, separator: str = ',', line_terminator: str = '\n', quote_char: str = '"', batch_size: int = 1024, datetime_format: str | None = None, date_format: str | None = None, time_format: str | None = None, float_scientific: bool | None = None, float_precision: int | None = None, null_value: str | None = None, quote_style: CsvQuoteStyle | None = None, storage_options: dict[str, Any] | None = None, credential_provider: CredentialProviderFunction | Literal['auto'] | None = 'auto', retries: int = 2) -> str | None: Write DataFrame to CSV file. file is the file path. separator is the field separator. quote_style is the quoting strategy. storage_options are cloud provider options. Returns output as string if file is None. GroupBy.first() -> DataFrame: Aggregate the first values in each group. Example: df.group_by("d", maintain_order=True).first() Expr.reshape(dimensions: tuple[int, ...]) -> Expr: Reshape this Expr to a flat column or an Array column. Parameters: dimensions (tuple of dimension sizes, -1 infers dimension). Returns: Expr (original type if single dimension, Array if multiple). See also: Expr.list.explode. Example: df.select(pl.col("foo").reshape((3, 3))) Expr.name.map(function: Callable[[str], str]) -> Expr: Rename the output of an expression by mapping a function over the root name. Parameters: function (maps root name to new name). See also: keep, prefix, suffix. Notes: Undoes previous renaming. Last expression in chain only. Example: df.with_columns(pl.all().reverse().name.map(lambda c: c.rstrip("_reverse").lower())) LazyFrame.select_seq(*exprs: IntoExpr | Iterable[IntoExpr], **named_exprs: IntoExpr) -> LazyFrame: Select columns sequentially. Parameters: *exprs: columns, **named_exprs: additional columns (renamed). Expr.upper_bound() -> Expr: Calculate upper bound. Returns Series with dtype's highest value. Expr.arr.shift(n: int | IntoExprColumn = 1) -> Expr: Shift array values by n indices. n<0 shifts opposite. SQL LAG (n>0) or LEAD (n<0). class Decimal(precision: int | None = None, scale: int = 0): Decimal 128-bit type. Params: precision, scale. Methods: base_type(), from_python(), is_(), is_decimal(), is_float(), is_integer(), is_nested(), is_numeric(), is_signed_integer(), is_temporal(), is_unsigned_integer(), max(), min(), to_python(). Attrs: precision, scale. Unstable. Series.rle() -> Series: Compress Series using run-length encoding (RLE). Returns Struct Series: 'len'(UInt32), 'value'(original dtype). Expr.list.head(n: int | str | Expr = 5) -> Expr: Slice first n sublist values. Param: n: values to return. Expr.str.extract_groups(pattern: str) -> Expr Extract all capture groups for regex pattern. Returns Struct of strings. Unnamed groups are numerical strings. all(*names: str, ignore_nulls: bool=True) -> Expr Returns expression of all columns or bitwise AND operation on named columns. Config.set_tbl_column_data_type_inline(active: bool=True) -> type[Config] Set config to display data type inline with column name in tables. LazyFrame.sink_csv(path: str | Path, *, include_bom: bool = False, include_header: bool = True, separator: str = ',', line_terminator: str = '\n', quote_char: str = '"', batch_size: int = 1024, datetime_format: str | None = None, date_format: str | None = None, time_format: str | None = None, float_scientific: bool | None = None, float_precision: int | None = None, null_value: str | None = None, quote_style: CsvQuoteStyle | None = None, maintain_order: bool = True, type_coercion: bool = True, predicate_pushdown: bool = True, projection_pushdown: bool = True, simplify_expression: bool = True, slice_pushdown: bool = True, collapse_joins: bool = True, no_optimization: bool = False, storage_options: dict[str, Any] | None = None, credential_provider: CredentialProviderFunction | Literal['auto'] | None = 'auto', retries: int = 2) -> None: Evaluate query in streaming mode and write to a CSV file. Parameters: path, include_bom, include_header, separator, line_terminator, quote_char, batch_size, datetime_format, date_format, time_format, float_scientific, float_precision, null_value, quote_style, maintain_order, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, collapse_joins, no_optimization, storage_options, credential_provider, retries. Series.str.extract_many(patterns: Series | list[str], *, ascii_case_insensitive: bool = False, overlapping: bool = False) -> Series: Extract many matches using the Aho-Corasick algorithm. Parameters: patterns, ascii_case_insensitive, overlapping. Only supports matching on string literals. Example: s.str.extract_many(patterns=[...], overlapping=True) Expr.dt.add_business_days(n: int | IntoExpr, *, week_mask: Iterable[bool] = (True, True, True, True, True, False, False), holidays: Iterable[dt.date] = (), roll: Roll = 'raise') -> Expr: Offset by n business days. Parameters: n, week_mask, holidays, roll. Returns: Expr, data type is preserved. polars.select(*exprs: IntoExpr | Iterable[IntoExpr], eager: bool = True, **named_exprs: IntoExpr) -> DataFrame | LazyFrame: Run polars expressions without context. eager: DataFrame if True, LazyFrame if False. *exprs: Columns to select. **named_exprs: Rename columns. polars.struct(*exprs: IntoExpr | Iterable[IntoExpr], schema: SchemaDict | None = None, eager: bool = False, **named_exprs: IntoExpr) -> Expr | Series Collect columns into a struct column. Parameters: *exprs (positional, parsed as column names or literals), schema, eager (evaluate immediately), **named_exprs (keyword arguments, rename columns). Use pl.all() to collect all. polars.cum_reduce(function: Callable[[Series, Series], Series], exprs: Sequence[Expr | str] | Expr) -> Expr Cumulatively reduce horizontally across columns. Returns a Struct column. polars.element() -> Expr Alias for an element being evaluated in an eval expression. Example: pl.concat_list([\\"a\\", \\"b\\"]).list.eval(pl.element().rank()) polars.linear_space(start: NumericLiteral | TemporalLiteral | IntoExpr, end: NumericLiteral | TemporalLiteral | IntoExpr, num_samples: int | IntoExpr, *, closed: ClosedInterval = 'both', eager: bool = False) -> Expr | Series Create sequence of evenly-spaced points. Returns Expr or Series. Series.rolling_var(window_size: int, weights: list[float] | None = None, *, min_samples: int | None = None, center: bool = False, ddof: int = 1) -> Series: Compute rolling variance. window_size is the window length. min_samples is the minimum non-null samples. ddof is the delta degrees of freedom. Example: s.rolling_var(window_size=3) Expr.str.find_many(patterns: IntoExpr, *, ascii_case_insensitive: bool = False, overlapping: bool = False) -> Expr: Find multiple string matches using Aho-Corasick. Returns List of byte offsets. Parameters: patterns (string patterns to search), ascii_case_insensitive (enable ASCII case-insensitive matching), overlapping (allow overlapping matches). Note: Supports string literals only, not regex. DataFrame.rows(*, named: bool = False) -> list[tuple[Any, ...]] | list[dict[str, Any]]: Returns all data as a list of rows (tuples or dictionaries). Parameters: named (return dictionaries instead of tuples). Warning: Row-iteration is not optimal. Consider iter_rows for better memory usage. Note: ns-precision temporal values are truncated to s. Expr.to_physical() -> Expr: Cast to the physical representation of the logical dtype. Examples: Date to Int32, Datetime/Time/Duration to Int64, Categorical to UInt32. Warning: Physical representations are implementation details and may not be stable. Expr.index_of(element: IntoExpr) -> Expr. Get index of first occurrence. Returns None if not found. Series.bin.decode(encoding: TransferEncoding, *, strict: bool = True) -> Series. Decode binary values using encoding ('hex', 'base64'). Strict param for error handling. polars.min_horizontal(*exprs: IntoExpr | Iterable[IntoExpr]) -> Expr: Get minimum value horizontally across columns. exprs: Columns to aggregate. Expr.reinterpret(*, signed: bool = True) -> Expr: Reinterpret bits as signed/unsigned integer. Only for 64bit integers. signed: True=Int64, False=UInt64. polars.testing.assert_series_equal(left: Series, right: Series, *, check_dtypes: bool = True, check_names: bool = True, check_order: bool = True, check_exact: bool = False, rtol: float = 1e-05, atol: float = 1e-08, categorical_as_str: bool = False) -> None: Assert Series equality, raise AssertionError if different. Parameters: left, right, check_dtypes, check_names, check_order, check_exact, rtol, atol, categorical_as_str. GroupBy.max() -> DataFrame: Reduce groups to maximal value. Example: df.group_by("d", maintain_order=True).max() LazyFrame.dtypes: list[DataType]: Get column data types. Returns: List of DataType. Warning: Requires resolving the schema. polars.exclude(columns: str | PolarsDataType | Collection[str] | Collection[PolarsDataType], *more_columns: str | PolarsDataType) -> Expr Represent all columns except for the given columns. Accepts regular expression input. Expr.xor(other: Any) -> Expr. Bitwise exclusive-or operator (expr ^ other). other: Integer or boolean value, or Expr. Series.shift(n: int = 1, *, fill_value: IntoExpr | None = None) -> Series. Shift values by n indices (negative n shifts opposite). fill_value: Value to fill nulls. Expr.dt.time() -> Expr. Extract time from Datetime column. Fails on Date columns. Expr.arg_sort(*, descending: bool = False, nulls_last: bool = False) -> Expr. Get indices to sort column. descending: Sort descending. nulls_last: Place nulls last. class DataType: Base class for Polars data types. Methods: base_type() - Returns fundamental type class. from_python(py_type) - Returns Polars data type from Python type. is_(other) - Checks if DataType is the same. is_decimal() - Checks if decimal type. is_float() - Checks if float type. is_integer() - Checks if integer type. is_nested() - Checks if nested type. is_numeric() - Checks if numeric type. is_signed_integer() - Checks if signed integer type. is_temporal() - Checks if temporal type. is_unsigned_integer() - Checks if unsigned integer type. to_python() - Returns Python type from Polars data type. Expr.rle_id() -> Expr: Get a distinct integer ID for each run of identical values. ID starts at 0, increments on value change. Returns: Expr of type UInt32. See also: rle. Useful for defining new groups when a column's value changes. Example: df.with_columns(rle_id_a=pl.col("a").rle_id()) Expr.is_infinite() -> Expr: Returns boolean Series indicating infinite values. Returns: Expr of data type Boolean. Example: df.select(pl.all().is_infinite()) Series.cast(dtype: type[int | float | str | bool] | PolarsDataType, *, strict: bool = True, wrap_numerical: bool = False) -> Self: Cast between data types. Parameters: dtype - DataType to cast to. strict - If True, invalid casts raise exceptions. wrap_numerical - If True, numeric casts wrap values. Example: s.cast(pl.UInt32) polars.cum_sum_horizontal(*exprs: IntoExpr | Iterable[IntoExpr]) -> Expr: Cumulatively sum values horizontally across columns. *exprs: Column(s) to aggregate. Returns: Expr. Series.list.gather(indices: Series | list[int] | list[list[int]], *, null_on_oob: bool = False) -> Series: Take sublists by multiple indices. indices: Indices to return per sublist. null_on_oob: Behavior for out-of-bounds indices; if True, set as null, otherwise raise an error. Returns: Series. LazyFrame.interpolate() Returns LazyFrame. Interpolate intermediate values linearly. Example: lf.interpolate().collect(). Expr.dt.ordinal_day() -> Expr: Extract ordinal day from Date/Datetime (day of year, 1-366). polars.from_arrow(data: pa.Table | pa.Array | pa.ChunkedArray | pa.RecordBatch | Iterable[pa.RecordBatch | pa.Table], schema: SchemaDefinition | None = None, *, schema_overrides: SchemaDict | None = None, rechunk: bool = True) -> DataFrame | Series: Create DataFrame/Series from Arrow data. Supports schema, schema_overrides, and rechunk. polars.lit(value: Any, dtype: PolarsDataType | None = None, *, allow_object: bool = False) -> Expr. Return literal value expression. Params: value, dtype, allow_object. Returns: Expr. Config.set_trim_decimal_zeros(active: bool | None = True) -> type[Config]. Strip trailing zeros from Decimal values. Param: active: enable stripping. Returns: type[Config]. Expr.rolling_min(window_size: int, weights: list[float] | None = None, *, min_samples: int | None = None, center: bool = False) -> Expr. Apply rolling min. Params: window_size, weights, min_samples, center. Returns: Expr. Note: use rolling for multiple aggregations over same window. Series.sort(descending: bool=False, nulls_last: bool=False, multithreaded: bool=True, in_place: bool=False) -> Self. Sort Series. Params: descending, nulls_last, multithreaded, in_place. LazyFrame.drop_nans(subset: ColumnNameOrSelector | Collection[ColumnNameOrSelector] | None = None) -> LazyFrame. Drop rows with NaN values. Params: subset columns to consider. Expr.forward_fill(limit: int | None = None) -> Expr. Fill null values with last non-null value. Params: limit consecutive nulls to fill. Expr.dt.month_start() -> Expr. Roll backward to the first day of the month. Preserves time-of-day. polars.api.register_series_namespace(name:str) -> Callable[[type[NS]], type[NS]]. Decorator for registering custom Series functionality. Parameters: name:Namespace name. Returns: Callable. classmethod Config.set_ascii_tables(active:bool|None=True) -> type[Config]. Use ASCII table outlines. Parameters: active:Use ASCII tables. Returns: type[Config]. LazyGroupBy.sum() -> LazyFrame. Reduce groups to sum. Returns: LazyFrame with sum aggregations. LazyFrame.count() -> LazyFrame: Return number of non-null elements for each column. Expr.dt.century() -> Expr: Extracts the century from Date or Datetime columns. Returns Int32. Series.dt.total_milliseconds() -> Series: Extracts total milliseconds from a Duration type. Returns Int64 Series. Expr.dt.with_time_unit(time_unit: TimeUnit) -> Expr: Sets time unit of Datetime or Duration expression. Deprecated. time_unit: 'ns', 'us', 'ms'. Does not modify underlying data. DataFrame.deserialize(source: str | Path | IOBase, *, format: SerializationFormat = 'binary') -> DataFrame: Read DataFrame from file. source: file path/object. format: "binary" (default), "json". Expr.gather(indices: int | Sequence[int] | IntoExpr | Series | np.ndarray[Any, Any]) -> Expr: Take values by index. indices: UInt32 Series expression. Expr.cum_min(*, reverse: bool = False) -> Expr: Calculates cumulative minimum. Catalog.scan_table(catalog_name: str, schema_name: str, table_name: str, *, delta_table_version: int | str | datetime | None = None, delta_table_options: dict[str, Any] | None = None, storage_options: dict[str, Any] | None = None, credential_provider: CredentialProviderFunction | Literal['auto'] | None = 'auto', retries: int = 2) -> LazyFrame: Retrieves the metadata of the specified table. Expr.ewm_std(*, com: float | None = None, span: float | None = None, half_life: float | None = None, alpha: float | None = None, adjust: bool = True, bias: bool = False, min_samples: int = 1, ignore_nulls: bool = False) -> Expr: Compute exponentially-weighted moving standard deviation. Expr.str.head(n: int | IntoExprColumn) -> Expr: Returns the first n characters of each string in a String Series. n can be negative. Returns full string if n is greater than string length. Example: pl.col("s").str.head(5) Expr.sin() -> Expr: Compute the element-wise sine. Returns: Expr of Float64. Example: pl.DataFrame({"a": [0.0]}).select(pl.col("a").sin()) LazyGroupBy.last() -> LazyFrame: Aggregate the last values in the group. Example: ldf.group_by("d", maintain_order=True).last().collect() Expr.str.pad_end(length: int, fill_char: str=' ') -> Expr. Pads string end to length. Params: length, fill_char. Returns: Expr. LazyGroupBy.agg(*aggs: IntoExpr | Iterable[IntoExpr], **named_aggs: IntoExpr) -> LazyFrame. Compute aggregations for groups. Params: *aggs, **named_aggs. Returns: LazyFrame. polars.show_versions() -> None. Prints Polars and dependencies versions. Series.cut(breaks: Sequence[float], *, labels: Sequence[str] | None = None, left_closed: bool = False, include_breaks: bool = False) -> Series. Bins values into categories. Expr.list.contains(item: float | str | bool | int | date | datetime | time | IntoExprColumn) -> Expr. Checks if sublists contain the item. polars.LazyFrame.group_by(*by: IntoExpr | Iterable[IntoExpr], maintain_order: bool = False, **named_by: IntoExpr) -> LazyGroupBy. Start a group by operation. by: Column(s) to group by. maintain_order: Keep group order consistent. Slower, blocks streaming. named_by: Additional group by columns. Example: lf.group_by("a").agg(pl.col("b").sum()).collect() polars.Expr.str.extract_many(patterns: IntoExpr, *, ascii_case_insensitive: bool = False, overlapping: bool = False) -> Expr. Extract multiple matches using Aho-Corasick algorithm. patterns to search. ascii_case_insensitive: Enable ASCII case-insensitive matching. overlapping: Allow overlapping matches. Supports string literals, not regex. polars.Series.list.shift(n: int | IntoExprColumn = 1) -> Series. Shift list values by n indices. n: Number of indices to shift (positive is forward, negative is backward). Similar to SQL LAG (positive n) or LEAD (negative n). Example: s.list.shift(-2) Expr.struct.json_encode() -> Expr: Convert struct to string column with JSON values. Example: pl.DataFrame({"a": [{"a": [1, 2], "b": [45]}]}).with_columns(pl.col("a").struct.json_encode().alias("encoded")) DataFrame.unstack(*, step: int, how: UnstackDirection = 'vertical', columns: ColumnNameOrSelector | Sequence[ColumnNameOrSelector] | None = None, fill_values: list[Any] | None = None) -> DataFrame Unstack a long table to a wide form without aggregation. step: int. LazyFrame.melt(id_vars: ColumnNameOrSelector | Sequence[ColumnNameOrSelector] | None = None, value_vars: ColumnNameOrSelector | Sequence[ColumnNameOrSelector] | None = None, variable_name: str | None = None, value_name: str | None = None, *, streamable: bool = True) -> LazyFrame Unpivot a DataFrame from wide to long format. (Deprecated) DataFrame.write_ipc(file: str | Path | IO[bytes] | None, *, compression: IpcCompression = 'uncompressed', compat_level: CompatLevel | None = None, storage_options: dict[str, Any] | None = None, credential_provider: CredentialProviderFunction | Literal['auto'] | None = 'auto', retries: int = 2) -> BytesIO | None Write to Arrow IPC binary stream or Feather file. file: str | Path | IO[bytes] | None. polars.Expr.bin.decode(encoding: TransferEncoding, *, strict: bool = True) -> Expr: Decode binary values using encoding. Parameters: encoding ('hex'|'base64'), strict (bool). Returns Expr of String. polars.scan_pyarrow_dataset(source: pa.dataset.Dataset, allow_pyarrow_filter: bool = True, batch_size: int | None = None) -> LazyFrame: Scan pyarrow dataset. Unstable API. Use scan_parquet if possible.